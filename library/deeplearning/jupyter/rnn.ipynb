{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tutorial of RNN\n",
    "\n",
    "reference : https://tutorials.pytorch.kr/intermediate/char_rnn_classification_tutorial.html\n",
    "goal : predict which language a word(name) is in\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "60374d5af19fdb16"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import string\n",
    "import unicodedata"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T07:18:57.354817Z",
     "start_time": "2023-08-31T07:18:57.352526Z"
    }
   },
   "id": "5c3f5a6212028ebf"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "PROJECT_DIR = \"/Users/fivessun/workspace/fivessun\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T07:18:57.493087Z",
     "start_time": "2023-08-31T07:18:57.487880Z"
    }
   },
   "id": "55e6b20ef0ee334"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "\n",
    "def find_files(path: str) -> list[str]:\n",
    "    return glob.glob(path)\n",
    "\n",
    "\n",
    "def convert_unicode_to_ascii(s) -> str:\n",
    "    return \"\".join(\n",
    "        c for c in unicodedata.normalize(\"NFD\", s)\n",
    "        if unicodedata.category(c) != \"Mn\"\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "\n",
    "def read_lines(filename: str) -> list[str]:\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        lines = file.read().strip().split(\"\\n\")\n",
    "    return [convert_unicode_to_ascii(line) for line in lines]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T07:18:57.650007Z",
     "start_time": "2023-08-31T07:18:57.646368Z"
    }
   },
   "id": "d032d05862b7107"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Czech : 519 \n",
      "# of German : 724 \n",
      "# of Arabic : 2000 \n",
      "# of Japanese : 991 \n",
      "# of Chinese : 268 \n",
      "# of Vietnamese : 73 \n",
      "# of Russian : 9408 \n",
      "# of French : 277 \n",
      "# of Irish : 232 \n",
      "# of English : 3668 \n",
      "# of Spanish : 298 \n",
      "# of Greek : 203 \n",
      "# of Italian : 709 \n",
      "# of Portuguese : 74 \n",
      "# of Scottish : 100 \n",
      "# of Dutch : 297 \n",
      "# of Korean : 94 \n",
      "# of Polish : 139 \n"
     ]
    }
   ],
   "source": [
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "for filename in find_files(f\"{PROJECT_DIR}/library/deeplearning/sample_data/rnn/names/*.txt\"):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = read_lines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "for key, value in category_lines.items():\n",
    "    print(f\"# of {key} : {len(value)} \")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T07:18:57.834799Z",
     "start_time": "2023-08-31T07:18:57.811925Z"
    }
   },
   "id": "31c894e0686b880e"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils import data as torch_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T07:18:58.942640Z",
     "start_time": "2023-08-31T07:18:57.978849Z"
    }
   },
   "id": "d938516b77efa7b0"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class CustomDataset(torch_data.Dataset):\n",
    "    def __init__(self, x_data: list[torch.Tensor], y_data: list[torch.Tensor]):\n",
    "        self.x_data = x_data\n",
    "        self.y_data = y_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x_data[idx]\n",
    "        y = self.y_data[idx]\n",
    "        return x, y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T07:18:58.977346Z",
     "start_time": "2023-08-31T07:18:58.975598Z"
    }
   },
   "id": "60638926f95cce27"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def convert_letter_to_index(letter: str) -> int:\n",
    "    \"\"\" Convert letter to index\n",
    "    \n",
    "    e.g. a -> 0, b -> 1, c -> 2, ..., z -> 25\n",
    "    \"\"\"\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "\n",
    "def convert_letter_to_tensor(letter: str) -> torch.Tensor:\n",
    "    \"\"\" Convert letter to tensor\n",
    "    \n",
    "    e.g. a -> [1, 0, 0, ..., 0], b -> [0, 1, 0, ..., 0], c -> [0, 0, 1, ..., 0], ..., z -> [0, 0, 0, ..., 1]\n",
    "    \"\"\"\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][convert_letter_to_index(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def convert_line_to_tensor(line: str) -> torch.Tensor:\n",
    "    \"\"\" Convert line to tensor\n",
    "    \n",
    "    shape of output: (line_length, 1, n_letters) \n",
    "    e.g. \"abc\" -> [[[1, 0, 0, ..., 0]], [[0, 1, 0, ..., 0]], [[0, 0, 1, ..., 0]]]\n",
    "    \"\"\"\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][convert_letter_to_index(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def convert_category_to_index(category: str) -> torch.Tensor:\n",
    "    \"\"\" Convert category to index\n",
    "    \n",
    "    e.g. \"Korean\" -> 0, \"English\" -> 1, \"Chinese\" -> 2\n",
    "    \"\"\"\n",
    "    return torch.tensor([all_categories.index(category)])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T07:18:59.569065Z",
     "start_time": "2023-08-31T07:18:59.564326Z"
    }
   },
   "id": "c6f02f1110ce1b52"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Setting hyperparameters\n",
    "learning_rate = 0.005\n",
    "n_hidden = 128\n",
    "num_epochs = 100"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T07:19:00.385934Z",
     "start_time": "2023-08-31T07:19:00.377431Z"
    }
   },
   "id": "7530257b172fb96e"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "for category, lines in category_lines.items():\n",
    "    for line in lines:\n",
    "        x_data.append(convert_line_to_tensor(line))\n",
    "        y_data.append(convert_category_to_index(category).long())\n",
    "        # y_data.append(F.one_hot(convert_category_to_index(category), num_classes=len(all_categories)).type(torch.float32))\n",
    "\n",
    "dataset = CustomDataset(x_data, y_data)\n",
    "dataloader = torch_data.DataLoader(dataset, batch_size=None, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T07:22:19.720878Z",
     "start_time": "2023-08-31T07:22:19.075240Z"
    }
   },
   "id": "dea66d1c0fea3695"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "class BasicRNN(nn.Module):\n",
    "    \"\"\"RNN layer.\n",
    "\n",
    "    h_t = tanh(W_xh * x_t + W_hh * h_t-1)\n",
    "    y_t = tanh(W_xy * x_t + W_hy * h_t-1)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
    "        super(BasicRNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # i2h: W_xh + W_hh\n",
    "        # i2o: W_xy + W_hy\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.activation_ftn = torch.tanh\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            x: torch.Tensor,\n",
    "            h: torch.Tensor,\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"RNN forward.\n",
    "        :arg:\n",
    "            shape of x (x_t)   : (batch_size, input_size)\n",
    "            shape of h (h_t-1) : (batch_size, hidden_size)\n",
    "\n",
    "        :return:\n",
    "            shape of y (y_t)   : (batch_size, output_size)\n",
    "            shape of h (h_t)   : (batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        # Combine input and hidden state.\n",
    "        combined = torch.cat((x, h), 1)\n",
    "\n",
    "        # It is possible to use one layer by combining i2h and i2o.\n",
    "        # With only one layer, not necessarily use activation function.\n",
    "        # h = self.activation_ftn(self.i2h(combined))\n",
    "        # y = self.activation_ftn(self.i2o(combined))\n",
    "        h = self.i2h(combined)\n",
    "        y = self.i2o(combined)\n",
    "        y = self.softmax(y)  # probability distribution\n",
    "        return y, h\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T07:44:10.509886Z",
     "start_time": "2023-08-31T07:44:10.509066Z"
    }
   },
   "id": "75f678f2816615c7"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "rnn_model = BasicRNN(n_letters, n_hidden, len(all_categories))\n",
    "loss_ftn = nn.NLLLoss() # with log_softmax\n",
    "optimizer = torch.optim.SGD(rnn_model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T07:27:34.072676Z",
     "start_time": "2023-08-31T07:27:34.063439Z"
    }
   },
   "id": "f63c77a5f33b9299"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Step 1/20074 | Loss: 2.8962\n",
      "Epoch 1/100 | Step 101/20074 | Loss: 2.8702\n",
      "Epoch 1/100 | Step 201/20074 | Loss: 1.3643\n",
      "Epoch 1/100 | Step 301/20074 | Loss: 0.5807\n",
      "Epoch 1/100 | Step 401/20074 | Loss: 0.2403\n",
      "Epoch 1/100 | Step 501/20074 | Loss: 1.6644\n",
      "Epoch 1/100 | Step 601/20074 | Loss: 0.5725\n",
      "Epoch 1/100 | Step 701/20074 | Loss: 0.5542\n",
      "Epoch 1/100 | Step 801/20074 | Loss: 1.5283\n",
      "Epoch 1/100 | Step 901/20074 | Loss: 0.5122\n",
      "Epoch 1/100 | Step 1001/20074 | Loss: 0.5367\n",
      "Epoch 1/100 | Step 1101/20074 | Loss: 0.2126\n",
      "Epoch 1/100 | Step 1201/20074 | Loss: 0.3595\n",
      "Epoch 1/100 | Step 1301/20074 | Loss: 3.2814\n",
      "Epoch 1/100 | Step 1401/20074 | Loss: 0.3179\n",
      "Epoch 1/100 | Step 1501/20074 | Loss: 0.6854\n",
      "Epoch 1/100 | Step 1601/20074 | Loss: 0.3288\n",
      "Epoch 1/100 | Step 1701/20074 | Loss: 1.6080\n",
      "Epoch 1/100 | Step 1801/20074 | Loss: 0.5320\n",
      "Epoch 1/100 | Step 1901/20074 | Loss: 2.0275\n",
      "Epoch 1/100 | Step 2001/20074 | Loss: 0.1409\n",
      "Epoch 1/100 | Step 2101/20074 | Loss: 4.1048\n",
      "Epoch 1/100 | Step 2201/20074 | Loss: 2.3255\n",
      "Epoch 1/100 | Step 2301/20074 | Loss: 1.5580\n",
      "Epoch 1/100 | Step 2401/20074 | Loss: 1.2646\n",
      "Epoch 1/100 | Step 2501/20074 | Loss: 2.6109\n",
      "Epoch 1/100 | Step 2601/20074 | Loss: 1.5874\n",
      "Epoch 1/100 | Step 2701/20074 | Loss: 0.1077\n",
      "Epoch 1/100 | Step 2801/20074 | Loss: 2.1772\n",
      "Epoch 1/100 | Step 2901/20074 | Loss: 3.3477\n",
      "Epoch 1/100 | Step 3001/20074 | Loss: 3.2759\n",
      "Epoch 1/100 | Step 3101/20074 | Loss: 0.2460\n",
      "Epoch 1/100 | Step 3201/20074 | Loss: 0.1758\n",
      "Epoch 1/100 | Step 3301/20074 | Loss: 1.6696\n",
      "Epoch 1/100 | Step 3401/20074 | Loss: 0.1817\n",
      "Epoch 1/100 | Step 3501/20074 | Loss: 2.0073\n",
      "Epoch 1/100 | Step 3601/20074 | Loss: 1.0904\n",
      "Epoch 1/100 | Step 3701/20074 | Loss: 0.2455\n",
      "Epoch 1/100 | Step 3801/20074 | Loss: 2.3013\n",
      "Epoch 1/100 | Step 3901/20074 | Loss: 0.2709\n",
      "Epoch 1/100 | Step 4001/20074 | Loss: 0.3050\n",
      "Epoch 1/100 | Step 4101/20074 | Loss: 2.9383\n",
      "Epoch 1/100 | Step 4201/20074 | Loss: 4.0763\n",
      "Epoch 1/100 | Step 4301/20074 | Loss: 2.9194\n",
      "Epoch 1/100 | Step 4401/20074 | Loss: 2.9199\n",
      "Epoch 1/100 | Step 4501/20074 | Loss: 4.9290\n",
      "Epoch 1/100 | Step 4601/20074 | Loss: 0.9626\n",
      "Epoch 1/100 | Step 4701/20074 | Loss: 0.3677\n",
      "Epoch 1/100 | Step 4801/20074 | Loss: 0.0215\n",
      "Epoch 1/100 | Step 4901/20074 | Loss: 0.2591\n",
      "Epoch 1/100 | Step 5001/20074 | Loss: 2.3629\n",
      "Epoch 1/100 | Step 5101/20074 | Loss: 0.9744\n",
      "Epoch 1/100 | Step 5201/20074 | Loss: 0.1198\n",
      "Epoch 1/100 | Step 5301/20074 | Loss: 0.0716\n",
      "Epoch 1/100 | Step 5401/20074 | Loss: 3.7363\n",
      "Epoch 1/100 | Step 5501/20074 | Loss: 2.5874\n",
      "Epoch 1/100 | Step 5601/20074 | Loss: 0.3316\n",
      "Epoch 1/100 | Step 5701/20074 | Loss: 0.0315\n",
      "Epoch 1/100 | Step 5801/20074 | Loss: 0.1247\n",
      "Epoch 1/100 | Step 5901/20074 | Loss: 0.0489\n",
      "Epoch 1/100 | Step 6001/20074 | Loss: 1.7752\n",
      "Epoch 1/100 | Step 6101/20074 | Loss: 1.3089\n",
      "Epoch 1/100 | Step 6201/20074 | Loss: 1.0149\n",
      "Epoch 1/100 | Step 6301/20074 | Loss: 2.8153\n",
      "Epoch 1/100 | Step 6401/20074 | Loss: 3.6917\n",
      "Epoch 1/100 | Step 6501/20074 | Loss: 4.0395\n",
      "Epoch 1/100 | Step 6601/20074 | Loss: 3.6437\n",
      "Epoch 1/100 | Step 6701/20074 | Loss: 0.1441\n",
      "Epoch 1/100 | Step 6801/20074 | Loss: 1.1338\n",
      "Epoch 1/100 | Step 6901/20074 | Loss: 1.8805\n",
      "Epoch 1/100 | Step 7001/20074 | Loss: 0.7816\n",
      "Epoch 1/100 | Step 7101/20074 | Loss: 1.1617\n",
      "Epoch 1/100 | Step 7201/20074 | Loss: 2.8753\n",
      "Epoch 1/100 | Step 7301/20074 | Loss: 0.1784\n",
      "Epoch 1/100 | Step 7401/20074 | Loss: 3.7624\n",
      "Epoch 1/100 | Step 7501/20074 | Loss: 3.6538\n",
      "Epoch 1/100 | Step 7601/20074 | Loss: 0.0685\n",
      "Epoch 1/100 | Step 7701/20074 | Loss: 0.3429\n",
      "Epoch 1/100 | Step 7801/20074 | Loss: 1.8506\n",
      "Epoch 1/100 | Step 7901/20074 | Loss: 0.3463\n",
      "Epoch 1/100 | Step 8001/20074 | Loss: 0.1468\n",
      "Epoch 1/100 | Step 8101/20074 | Loss: 0.1191\n",
      "Epoch 1/100 | Step 8201/20074 | Loss: 1.1018\n",
      "Epoch 1/100 | Step 8301/20074 | Loss: 2.7231\n",
      "Epoch 1/100 | Step 8401/20074 | Loss: 1.2564\n",
      "Epoch 1/100 | Step 8501/20074 | Loss: 0.8191\n",
      "Epoch 1/100 | Step 8601/20074 | Loss: 3.3594\n",
      "Epoch 1/100 | Step 8701/20074 | Loss: 3.7772\n",
      "Epoch 1/100 | Step 8801/20074 | Loss: 0.1353\n",
      "Epoch 1/100 | Step 8901/20074 | Loss: 1.0641\n",
      "Epoch 1/100 | Step 9001/20074 | Loss: 0.3888\n",
      "Epoch 1/100 | Step 9101/20074 | Loss: 0.0099\n",
      "Epoch 1/100 | Step 9201/20074 | Loss: 1.2231\n",
      "Epoch 1/100 | Step 9301/20074 | Loss: 0.9373\n",
      "Epoch 1/100 | Step 9401/20074 | Loss: 1.1811\n",
      "Epoch 1/100 | Step 9501/20074 | Loss: 0.8154\n",
      "Epoch 1/100 | Step 9601/20074 | Loss: 1.1223\n",
      "Epoch 1/100 | Step 9701/20074 | Loss: 3.6064\n",
      "Epoch 1/100 | Step 9801/20074 | Loss: 0.0571\n",
      "Epoch 1/100 | Step 9901/20074 | Loss: 0.4127\n",
      "Epoch 1/100 | Step 10001/20074 | Loss: 3.2680\n",
      "Epoch 1/100 | Step 10101/20074 | Loss: 0.9027\n",
      "Epoch 1/100 | Step 10201/20074 | Loss: 0.2563\n",
      "Epoch 1/100 | Step 10301/20074 | Loss: 0.7803\n",
      "Epoch 1/100 | Step 10401/20074 | Loss: 3.9353\n",
      "Epoch 1/100 | Step 10501/20074 | Loss: 0.0385\n",
      "Epoch 1/100 | Step 10601/20074 | Loss: 0.0016\n",
      "Epoch 1/100 | Step 10701/20074 | Loss: 0.0512\n",
      "Epoch 1/100 | Step 10801/20074 | Loss: 1.0408\n",
      "Epoch 1/100 | Step 10901/20074 | Loss: 0.0217\n",
      "Epoch 1/100 | Step 11001/20074 | Loss: 0.4631\n",
      "Epoch 1/100 | Step 11101/20074 | Loss: 1.2451\n",
      "Epoch 1/100 | Step 11201/20074 | Loss: 1.9288\n",
      "Epoch 1/100 | Step 11301/20074 | Loss: 0.5175\n",
      "Epoch 1/100 | Step 11401/20074 | Loss: 1.4477\n",
      "Epoch 1/100 | Step 11501/20074 | Loss: 1.5458\n",
      "Epoch 1/100 | Step 11601/20074 | Loss: 1.1062\n",
      "Epoch 1/100 | Step 11701/20074 | Loss: 3.8247\n",
      "Epoch 1/100 | Step 11801/20074 | Loss: 0.0447\n",
      "Epoch 1/100 | Step 11901/20074 | Loss: 2.7202\n",
      "Epoch 1/100 | Step 12001/20074 | Loss: 2.8730\n",
      "Epoch 1/100 | Step 12101/20074 | Loss: 2.5339\n",
      "Epoch 1/100 | Step 12201/20074 | Loss: 3.8503\n",
      "Epoch 1/100 | Step 12301/20074 | Loss: 0.3904\n",
      "Epoch 1/100 | Step 12401/20074 | Loss: 4.0704\n",
      "Epoch 1/100 | Step 12501/20074 | Loss: 0.7016\n",
      "Epoch 1/100 | Step 12601/20074 | Loss: 0.5004\n",
      "Epoch 1/100 | Step 12701/20074 | Loss: 0.3729\n",
      "Epoch 1/100 | Step 12801/20074 | Loss: 1.6566\n",
      "Epoch 1/100 | Step 12901/20074 | Loss: 5.2271\n",
      "Epoch 1/100 | Step 13001/20074 | Loss: 2.1311\n",
      "Epoch 1/100 | Step 13101/20074 | Loss: 0.3494\n",
      "Epoch 1/100 | Step 13201/20074 | Loss: 1.1086\n",
      "Epoch 1/100 | Step 13301/20074 | Loss: 1.8622\n",
      "Epoch 1/100 | Step 13401/20074 | Loss: 0.0036\n",
      "Epoch 1/100 | Step 13501/20074 | Loss: 1.0088\n",
      "Epoch 1/100 | Step 13601/20074 | Loss: 0.1653\n",
      "Epoch 1/100 | Step 13701/20074 | Loss: 1.0700\n",
      "Epoch 1/100 | Step 13801/20074 | Loss: 0.6829\n",
      "Epoch 1/100 | Step 13901/20074 | Loss: 0.3088\n",
      "Epoch 1/100 | Step 14001/20074 | Loss: 4.1196\n",
      "Epoch 1/100 | Step 14101/20074 | Loss: 0.6491\n",
      "Epoch 1/100 | Step 14201/20074 | Loss: 3.3178\n",
      "Epoch 1/100 | Step 14301/20074 | Loss: 0.0750\n",
      "Epoch 1/100 | Step 14401/20074 | Loss: 1.4480\n",
      "Epoch 1/100 | Step 14501/20074 | Loss: 3.3271\n",
      "Epoch 1/100 | Step 14601/20074 | Loss: 1.7131\n",
      "Epoch 1/100 | Step 14701/20074 | Loss: 2.4141\n",
      "Epoch 1/100 | Step 14801/20074 | Loss: 1.4704\n",
      "Epoch 1/100 | Step 14901/20074 | Loss: 1.2120\n",
      "Epoch 1/100 | Step 15001/20074 | Loss: 0.1893\n",
      "Epoch 1/100 | Step 15101/20074 | Loss: 1.2605\n",
      "Epoch 1/100 | Step 15201/20074 | Loss: 0.0186\n",
      "Epoch 1/100 | Step 15301/20074 | Loss: 3.4181\n",
      "Epoch 1/100 | Step 15401/20074 | Loss: 0.1364\n",
      "Epoch 1/100 | Step 15501/20074 | Loss: 0.9253\n",
      "Epoch 1/100 | Step 15601/20074 | Loss: 0.0198\n",
      "Epoch 1/100 | Step 15701/20074 | Loss: 0.0104\n",
      "Epoch 1/100 | Step 15801/20074 | Loss: 0.2152\n",
      "Epoch 1/100 | Step 15901/20074 | Loss: 0.7958\n",
      "Epoch 1/100 | Step 16001/20074 | Loss: 0.0385\n",
      "Epoch 1/100 | Step 16101/20074 | Loss: 0.0406\n",
      "Epoch 1/100 | Step 16201/20074 | Loss: 0.0431\n",
      "Epoch 1/100 | Step 16301/20074 | Loss: 1.4098\n",
      "Epoch 1/100 | Step 16401/20074 | Loss: 1.6252\n",
      "Epoch 1/100 | Step 16501/20074 | Loss: 0.1521\n",
      "Epoch 1/100 | Step 16601/20074 | Loss: 1.6398\n",
      "Epoch 1/100 | Step 16701/20074 | Loss: 1.3604\n",
      "Epoch 1/100 | Step 16801/20074 | Loss: 0.0729\n",
      "Epoch 1/100 | Step 16901/20074 | Loss: 1.0746\n",
      "Epoch 1/100 | Step 17001/20074 | Loss: 1.1758\n",
      "Epoch 1/100 | Step 17101/20074 | Loss: 0.6061\n",
      "Epoch 1/100 | Step 17201/20074 | Loss: 0.1416\n",
      "Epoch 1/100 | Step 17301/20074 | Loss: 1.4103\n",
      "Epoch 1/100 | Step 17401/20074 | Loss: 0.5247\n",
      "Epoch 1/100 | Step 17501/20074 | Loss: 2.1419\n",
      "Epoch 1/100 | Step 17601/20074 | Loss: 0.8565\n",
      "Epoch 1/100 | Step 17701/20074 | Loss: 3.5186\n",
      "Epoch 1/100 | Step 17801/20074 | Loss: 0.6499\n",
      "Epoch 1/100 | Step 17901/20074 | Loss: 1.2333\n",
      "Epoch 1/100 | Step 18001/20074 | Loss: 0.6961\n",
      "Epoch 1/100 | Step 18101/20074 | Loss: 0.0243\n",
      "Epoch 1/100 | Step 18201/20074 | Loss: 1.1585\n",
      "Epoch 1/100 | Step 18301/20074 | Loss: 3.0951\n",
      "Epoch 1/100 | Step 18401/20074 | Loss: 2.9827\n",
      "Epoch 1/100 | Step 18501/20074 | Loss: 0.1043\n",
      "Epoch 1/100 | Step 18601/20074 | Loss: 0.0038\n",
      "Epoch 1/100 | Step 18701/20074 | Loss: 0.5952\n",
      "Epoch 1/100 | Step 18801/20074 | Loss: 0.7560\n",
      "Epoch 1/100 | Step 18901/20074 | Loss: 0.0384\n",
      "Epoch 1/100 | Step 19001/20074 | Loss: 0.3774\n",
      "Epoch 1/100 | Step 19101/20074 | Loss: 0.0545\n",
      "Epoch 1/100 | Step 19201/20074 | Loss: 0.3901\n",
      "Epoch 1/100 | Step 19301/20074 | Loss: 2.1262\n",
      "Epoch 1/100 | Step 19401/20074 | Loss: 0.2722\n",
      "Epoch 1/100 | Step 19501/20074 | Loss: 1.6939\n",
      "Epoch 1/100 | Step 19601/20074 | Loss: 0.0102\n",
      "Epoch 1/100 | Step 19701/20074 | Loss: 3.4379\n",
      "Epoch 1/100 | Step 19801/20074 | Loss: 1.6878\n",
      "Epoch 1/100 | Step 19901/20074 | Loss: 0.0332\n",
      "Epoch 1/100 | Step 20001/20074 | Loss: 0.7986\n",
      "Epoch 2/100 | Step 1/20074 | Loss: 0.5499\n",
      "Epoch 2/100 | Step 101/20074 | Loss: 3.5903\n",
      "Epoch 2/100 | Step 201/20074 | Loss: 0.3087\n",
      "Epoch 2/100 | Step 301/20074 | Loss: 1.1344\n",
      "Epoch 2/100 | Step 401/20074 | Loss: 1.5390\n",
      "Epoch 2/100 | Step 501/20074 | Loss: 0.0227\n",
      "Epoch 2/100 | Step 601/20074 | Loss: 1.6892\n",
      "Epoch 2/100 | Step 701/20074 | Loss: 0.2433\n",
      "Epoch 2/100 | Step 801/20074 | Loss: 0.0397\n",
      "Epoch 2/100 | Step 901/20074 | Loss: 0.3009\n",
      "Epoch 2/100 | Step 1001/20074 | Loss: 2.7499\n",
      "Epoch 2/100 | Step 1101/20074 | Loss: 0.7604\n",
      "Epoch 2/100 | Step 1201/20074 | Loss: 0.1733\n",
      "Epoch 2/100 | Step 1301/20074 | Loss: 0.0832\n",
      "Epoch 2/100 | Step 1401/20074 | Loss: 1.3825\n",
      "Epoch 2/100 | Step 1501/20074 | Loss: 0.0109\n",
      "Epoch 2/100 | Step 1601/20074 | Loss: 0.0108\n",
      "Epoch 2/100 | Step 1701/20074 | Loss: 0.2328\n",
      "Epoch 2/100 | Step 1801/20074 | Loss: 0.5376\n",
      "Epoch 2/100 | Step 1901/20074 | Loss: 0.1239\n",
      "Epoch 2/100 | Step 2001/20074 | Loss: 0.1242\n",
      "Epoch 2/100 | Step 2101/20074 | Loss: 1.7512\n",
      "Epoch 2/100 | Step 2201/20074 | Loss: 0.2056\n",
      "Epoch 2/100 | Step 2301/20074 | Loss: 0.6201\n",
      "Epoch 2/100 | Step 2401/20074 | Loss: 0.9951\n",
      "Epoch 2/100 | Step 2501/20074 | Loss: 3.8885\n",
      "Epoch 2/100 | Step 2601/20074 | Loss: 0.7802\n",
      "Epoch 2/100 | Step 2701/20074 | Loss: 0.6622\n",
      "Epoch 2/100 | Step 2801/20074 | Loss: 0.4024\n",
      "Epoch 2/100 | Step 2901/20074 | Loss: 0.0065\n",
      "Epoch 2/100 | Step 3001/20074 | Loss: 3.4106\n",
      "Epoch 2/100 | Step 3101/20074 | Loss: 3.4351\n",
      "Epoch 2/100 | Step 3201/20074 | Loss: 0.9791\n",
      "Epoch 2/100 | Step 3301/20074 | Loss: 0.1073\n",
      "Epoch 2/100 | Step 3401/20074 | Loss: 0.0473\n",
      "Epoch 2/100 | Step 3501/20074 | Loss: 2.1705\n",
      "Epoch 2/100 | Step 3601/20074 | Loss: 4.8052\n",
      "Epoch 2/100 | Step 3701/20074 | Loss: 2.2247\n",
      "Epoch 2/100 | Step 3801/20074 | Loss: 0.5873\n",
      "Epoch 2/100 | Step 3901/20074 | Loss: 0.9638\n",
      "Epoch 2/100 | Step 4001/20074 | Loss: 2.3656\n",
      "Epoch 2/100 | Step 4101/20074 | Loss: 4.7791\n",
      "Epoch 2/100 | Step 4201/20074 | Loss: 0.3406\n",
      "Epoch 2/100 | Step 4301/20074 | Loss: 0.5535\n",
      "Epoch 2/100 | Step 4401/20074 | Loss: 1.6013\n",
      "Epoch 2/100 | Step 4501/20074 | Loss: 0.1957\n",
      "Epoch 2/100 | Step 4601/20074 | Loss: 0.6948\n",
      "Epoch 2/100 | Step 4701/20074 | Loss: 0.0337\n",
      "Epoch 2/100 | Step 4801/20074 | Loss: 0.0218\n",
      "Epoch 2/100 | Step 4901/20074 | Loss: 0.0801\n",
      "Epoch 2/100 | Step 5001/20074 | Loss: 0.1444\n",
      "Epoch 2/100 | Step 5101/20074 | Loss: 2.1423\n",
      "Epoch 2/100 | Step 5201/20074 | Loss: 0.6327\n",
      "Epoch 2/100 | Step 5301/20074 | Loss: 1.0272\n",
      "Epoch 2/100 | Step 5401/20074 | Loss: 0.4342\n",
      "Epoch 2/100 | Step 5501/20074 | Loss: 0.5383\n",
      "Epoch 2/100 | Step 5601/20074 | Loss: 0.9533\n",
      "Epoch 2/100 | Step 5701/20074 | Loss: 0.2881\n",
      "Epoch 2/100 | Step 5801/20074 | Loss: 0.0199\n",
      "Epoch 2/100 | Step 5901/20074 | Loss: 0.9441\n",
      "Epoch 2/100 | Step 6001/20074 | Loss: 1.7358\n",
      "Epoch 2/100 | Step 6101/20074 | Loss: 0.0071\n",
      "Epoch 2/100 | Step 6201/20074 | Loss: 1.6984\n",
      "Epoch 2/100 | Step 6301/20074 | Loss: 2.5843\n",
      "Epoch 2/100 | Step 6401/20074 | Loss: 0.0961\n",
      "Epoch 2/100 | Step 6501/20074 | Loss: 1.6844\n",
      "Epoch 2/100 | Step 6601/20074 | Loss: 0.0114\n",
      "Epoch 2/100 | Step 6701/20074 | Loss: 2.3318\n",
      "Epoch 2/100 | Step 6801/20074 | Loss: 2.1832\n",
      "Epoch 2/100 | Step 6901/20074 | Loss: 4.2756\n",
      "Epoch 2/100 | Step 7001/20074 | Loss: 0.1536\n",
      "Epoch 2/100 | Step 7101/20074 | Loss: 0.0305\n",
      "Epoch 2/100 | Step 7201/20074 | Loss: 0.9698\n",
      "Epoch 2/100 | Step 7301/20074 | Loss: 1.1719\n",
      "Epoch 2/100 | Step 7401/20074 | Loss: 2.6638\n",
      "Epoch 2/100 | Step 7501/20074 | Loss: 2.6489\n",
      "Epoch 2/100 | Step 7601/20074 | Loss: 2.4255\n",
      "Epoch 2/100 | Step 7701/20074 | Loss: 3.0406\n",
      "Epoch 2/100 | Step 7801/20074 | Loss: 0.0167\n",
      "Epoch 2/100 | Step 7901/20074 | Loss: 0.5593\n",
      "Epoch 2/100 | Step 8001/20074 | Loss: 2.2098\n",
      "Epoch 2/100 | Step 8101/20074 | Loss: 0.1643\n",
      "Epoch 2/100 | Step 8201/20074 | Loss: 0.7244\n",
      "Epoch 2/100 | Step 8301/20074 | Loss: 0.0706\n",
      "Epoch 2/100 | Step 8401/20074 | Loss: 0.4087\n",
      "Epoch 2/100 | Step 8501/20074 | Loss: 0.0007\n",
      "Epoch 2/100 | Step 8601/20074 | Loss: 0.0055\n",
      "Epoch 2/100 | Step 8701/20074 | Loss: 0.0194\n",
      "Epoch 2/100 | Step 8801/20074 | Loss: 2.9989\n",
      "Epoch 2/100 | Step 8901/20074 | Loss: 0.0266\n",
      "Epoch 2/100 | Step 9001/20074 | Loss: 0.1004\n",
      "Epoch 2/100 | Step 9101/20074 | Loss: 0.0114\n",
      "Epoch 2/100 | Step 9201/20074 | Loss: 0.1936\n",
      "Epoch 2/100 | Step 9301/20074 | Loss: 0.7381\n",
      "Epoch 2/100 | Step 9401/20074 | Loss: 0.8127\n",
      "Epoch 2/100 | Step 9501/20074 | Loss: 0.6564\n",
      "Epoch 2/100 | Step 9601/20074 | Loss: 0.0003\n",
      "Epoch 2/100 | Step 9701/20074 | Loss: 0.3257\n",
      "Epoch 2/100 | Step 9801/20074 | Loss: 0.2340\n",
      "Epoch 2/100 | Step 9901/20074 | Loss: 0.0389\n",
      "Epoch 2/100 | Step 10001/20074 | Loss: 0.7468\n",
      "Epoch 2/100 | Step 10101/20074 | Loss: 0.4414\n",
      "Epoch 2/100 | Step 10201/20074 | Loss: 1.0464\n",
      "Epoch 2/100 | Step 10301/20074 | Loss: 2.1333\n",
      "Epoch 2/100 | Step 10401/20074 | Loss: 0.0474\n",
      "Epoch 2/100 | Step 10501/20074 | Loss: 6.5157\n",
      "Epoch 2/100 | Step 10601/20074 | Loss: 3.7567\n",
      "Epoch 2/100 | Step 10701/20074 | Loss: 0.7417\n",
      "Epoch 2/100 | Step 10801/20074 | Loss: 0.0339\n",
      "Epoch 2/100 | Step 10901/20074 | Loss: 0.1847\n",
      "Epoch 2/100 | Step 11001/20074 | Loss: 1.1740\n",
      "Epoch 2/100 | Step 11101/20074 | Loss: 0.2958\n",
      "Epoch 2/100 | Step 11201/20074 | Loss: 0.5916\n",
      "Epoch 2/100 | Step 11301/20074 | Loss: 2.5350\n",
      "Epoch 2/100 | Step 11401/20074 | Loss: 0.5948\n",
      "Epoch 2/100 | Step 11501/20074 | Loss: 1.8801\n",
      "Epoch 2/100 | Step 11601/20074 | Loss: 4.0357\n",
      "Epoch 2/100 | Step 11701/20074 | Loss: 3.4662\n",
      "Epoch 2/100 | Step 11801/20074 | Loss: 0.4170\n",
      "Epoch 2/100 | Step 11901/20074 | Loss: 3.0161\n",
      "Epoch 2/100 | Step 12001/20074 | Loss: 3.4148\n",
      "Epoch 2/100 | Step 12101/20074 | Loss: 0.7392\n",
      "Epoch 2/100 | Step 12201/20074 | Loss: 3.6693\n",
      "Epoch 2/100 | Step 12301/20074 | Loss: 0.0288\n",
      "Epoch 2/100 | Step 12401/20074 | Loss: 0.0145\n",
      "Epoch 2/100 | Step 12501/20074 | Loss: 0.4622\n",
      "Epoch 2/100 | Step 12601/20074 | Loss: 4.3539\n",
      "Epoch 2/100 | Step 12701/20074 | Loss: 0.7325\n",
      "Epoch 2/100 | Step 12801/20074 | Loss: 0.9045\n",
      "Epoch 2/100 | Step 12901/20074 | Loss: 0.0237\n",
      "Epoch 2/100 | Step 13001/20074 | Loss: 0.0103\n",
      "Epoch 2/100 | Step 13101/20074 | Loss: 0.9597\n",
      "Epoch 2/100 | Step 13201/20074 | Loss: 0.0152\n",
      "Epoch 2/100 | Step 13301/20074 | Loss: 3.4932\n",
      "Epoch 2/100 | Step 13401/20074 | Loss: 0.2726\n",
      "Epoch 2/100 | Step 13501/20074 | Loss: 0.9686\n",
      "Epoch 2/100 | Step 13601/20074 | Loss: 0.0046\n",
      "Epoch 2/100 | Step 13701/20074 | Loss: 0.5968\n",
      "Epoch 2/100 | Step 13801/20074 | Loss: 1.9013\n",
      "Epoch 2/100 | Step 13901/20074 | Loss: 0.2120\n",
      "Epoch 2/100 | Step 14001/20074 | Loss: 0.6530\n",
      "Epoch 2/100 | Step 14101/20074 | Loss: 0.0099\n",
      "Epoch 2/100 | Step 14201/20074 | Loss: 4.5108\n",
      "Epoch 2/100 | Step 14301/20074 | Loss: 1.3870\n",
      "Epoch 2/100 | Step 14401/20074 | Loss: 2.5570\n",
      "Epoch 2/100 | Step 14501/20074 | Loss: 0.3166\n",
      "Epoch 2/100 | Step 14601/20074 | Loss: 0.4079\n",
      "Epoch 2/100 | Step 14701/20074 | Loss: 0.0053\n",
      "Epoch 2/100 | Step 14801/20074 | Loss: 0.2362\n",
      "Epoch 2/100 | Step 14901/20074 | Loss: 0.0261\n",
      "Epoch 2/100 | Step 15001/20074 | Loss: 3.5352\n",
      "Epoch 2/100 | Step 15101/20074 | Loss: 0.1869\n",
      "Epoch 2/100 | Step 15201/20074 | Loss: 1.5511\n",
      "Epoch 2/100 | Step 15301/20074 | Loss: 0.9845\n",
      "Epoch 2/100 | Step 15401/20074 | Loss: 0.0118\n",
      "Epoch 2/100 | Step 15501/20074 | Loss: 3.4178\n",
      "Epoch 2/100 | Step 15601/20074 | Loss: 0.8302\n",
      "Epoch 2/100 | Step 15701/20074 | Loss: 0.5683\n",
      "Epoch 2/100 | Step 15801/20074 | Loss: 1.5110\n",
      "Epoch 2/100 | Step 15901/20074 | Loss: 0.1097\n",
      "Epoch 2/100 | Step 16001/20074 | Loss: 1.9331\n",
      "Epoch 2/100 | Step 16101/20074 | Loss: 0.6400\n",
      "Epoch 2/100 | Step 16201/20074 | Loss: 0.3036\n",
      "Epoch 2/100 | Step 16301/20074 | Loss: 0.4308\n",
      "Epoch 2/100 | Step 16401/20074 | Loss: 0.0155\n",
      "Epoch 2/100 | Step 16501/20074 | Loss: 1.1457\n",
      "Epoch 2/100 | Step 16601/20074 | Loss: 1.8608\n",
      "Epoch 2/100 | Step 16701/20074 | Loss: 0.0033\n",
      "Epoch 2/100 | Step 16801/20074 | Loss: 1.1939\n",
      "Epoch 2/100 | Step 16901/20074 | Loss: 0.1013\n",
      "Epoch 2/100 | Step 17001/20074 | Loss: 0.0074\n",
      "Epoch 2/100 | Step 17101/20074 | Loss: 2.0940\n",
      "Epoch 2/100 | Step 17201/20074 | Loss: 0.0099\n",
      "Epoch 2/100 | Step 17301/20074 | Loss: 0.0023\n",
      "Epoch 2/100 | Step 17401/20074 | Loss: 3.3861\n",
      "Epoch 2/100 | Step 17501/20074 | Loss: 0.0052\n",
      "Epoch 2/100 | Step 17601/20074 | Loss: 2.0926\n",
      "Epoch 2/100 | Step 17701/20074 | Loss: 0.0008\n",
      "Epoch 2/100 | Step 17801/20074 | Loss: 0.3715\n",
      "Epoch 2/100 | Step 17901/20074 | Loss: 2.5791\n",
      "Epoch 2/100 | Step 18001/20074 | Loss: 2.2941\n",
      "Epoch 2/100 | Step 18101/20074 | Loss: 2.0610\n",
      "Epoch 2/100 | Step 18201/20074 | Loss: 0.0426\n",
      "Epoch 2/100 | Step 18301/20074 | Loss: 0.7072\n",
      "Epoch 2/100 | Step 18401/20074 | Loss: 0.9437\n",
      "Epoch 2/100 | Step 18501/20074 | Loss: 0.1949\n",
      "Epoch 2/100 | Step 18601/20074 | Loss: 0.3065\n",
      "Epoch 2/100 | Step 18701/20074 | Loss: 0.0133\n",
      "Epoch 2/100 | Step 18801/20074 | Loss: 0.5767\n",
      "Epoch 2/100 | Step 18901/20074 | Loss: 3.2491\n",
      "Epoch 2/100 | Step 19001/20074 | Loss: 0.2034\n",
      "Epoch 2/100 | Step 19101/20074 | Loss: 0.1213\n",
      "Epoch 2/100 | Step 19201/20074 | Loss: 0.1537\n",
      "Epoch 2/100 | Step 19301/20074 | Loss: 0.0084\n",
      "Epoch 2/100 | Step 19401/20074 | Loss: 0.0578\n",
      "Epoch 2/100 | Step 19501/20074 | Loss: 0.1332\n",
      "Epoch 2/100 | Step 19601/20074 | Loss: 1.0530\n",
      "Epoch 2/100 | Step 19701/20074 | Loss: 1.4463\n",
      "Epoch 2/100 | Step 19801/20074 | Loss: 0.1109\n",
      "Epoch 2/100 | Step 19901/20074 | Loss: 0.8060\n",
      "Epoch 2/100 | Step 20001/20074 | Loss: 0.0108\n",
      "Epoch 3/100 | Step 1/20074 | Loss: 0.9520\n",
      "Epoch 3/100 | Step 101/20074 | Loss: 0.0549\n",
      "Epoch 3/100 | Step 201/20074 | Loss: 2.1107\n",
      "Epoch 3/100 | Step 301/20074 | Loss: 0.0149\n",
      "Epoch 3/100 | Step 401/20074 | Loss: 1.6904\n",
      "Epoch 3/100 | Step 501/20074 | Loss: 1.6379\n",
      "Epoch 3/100 | Step 601/20074 | Loss: 0.1396\n",
      "Epoch 3/100 | Step 701/20074 | Loss: 2.4829\n",
      "Epoch 3/100 | Step 801/20074 | Loss: 0.5489\n",
      "Epoch 3/100 | Step 901/20074 | Loss: 0.0694\n",
      "Epoch 3/100 | Step 1001/20074 | Loss: 4.4836\n",
      "Epoch 3/100 | Step 1101/20074 | Loss: 0.8791\n",
      "Epoch 3/100 | Step 1201/20074 | Loss: 2.4880\n",
      "Epoch 3/100 | Step 1301/20074 | Loss: 0.0125\n",
      "Epoch 3/100 | Step 1401/20074 | Loss: 0.0042\n",
      "Epoch 3/100 | Step 1501/20074 | Loss: 4.3489\n",
      "Epoch 3/100 | Step 1601/20074 | Loss: 1.3216\n",
      "Epoch 3/100 | Step 1701/20074 | Loss: 0.4282\n",
      "Epoch 3/100 | Step 1801/20074 | Loss: 0.1292\n",
      "Epoch 3/100 | Step 1901/20074 | Loss: 0.0881\n",
      "Epoch 3/100 | Step 2001/20074 | Loss: 1.4363\n",
      "Epoch 3/100 | Step 2101/20074 | Loss: 0.0027\n",
      "Epoch 3/100 | Step 2201/20074 | Loss: 1.0069\n",
      "Epoch 3/100 | Step 2301/20074 | Loss: 0.2623\n",
      "Epoch 3/100 | Step 2401/20074 | Loss: 0.0497\n",
      "Epoch 3/100 | Step 2501/20074 | Loss: 0.0133\n",
      "Epoch 3/100 | Step 2601/20074 | Loss: 0.0294\n",
      "Epoch 3/100 | Step 2701/20074 | Loss: 0.4078\n",
      "Epoch 3/100 | Step 2801/20074 | Loss: 0.0544\n",
      "Epoch 3/100 | Step 2901/20074 | Loss: 0.1727\n",
      "Epoch 3/100 | Step 3001/20074 | Loss: 0.0564\n",
      "Epoch 3/100 | Step 3101/20074 | Loss: 2.5446\n",
      "Epoch 3/100 | Step 3201/20074 | Loss: 0.3209\n",
      "Epoch 3/100 | Step 3301/20074 | Loss: 3.3184\n",
      "Epoch 3/100 | Step 3401/20074 | Loss: 0.0026\n",
      "Epoch 3/100 | Step 3501/20074 | Loss: 0.7223\n",
      "Epoch 3/100 | Step 3601/20074 | Loss: 0.2455\n",
      "Epoch 3/100 | Step 3701/20074 | Loss: 1.6269\n",
      "Epoch 3/100 | Step 3801/20074 | Loss: 1.1688\n",
      "Epoch 3/100 | Step 3901/20074 | Loss: 0.0212\n",
      "Epoch 3/100 | Step 4001/20074 | Loss: 0.8693\n",
      "Epoch 3/100 | Step 4101/20074 | Loss: 3.0650\n",
      "Epoch 3/100 | Step 4201/20074 | Loss: 1.1299\n",
      "Epoch 3/100 | Step 4301/20074 | Loss: 0.3916\n",
      "Epoch 3/100 | Step 4401/20074 | Loss: 0.2468\n",
      "Epoch 3/100 | Step 4501/20074 | Loss: 0.3360\n",
      "Epoch 3/100 | Step 4601/20074 | Loss: 0.6342\n",
      "Epoch 3/100 | Step 4701/20074 | Loss: 2.2316\n",
      "Epoch 3/100 | Step 4801/20074 | Loss: 1.0392\n",
      "Epoch 3/100 | Step 4901/20074 | Loss: 1.1840\n",
      "Epoch 3/100 | Step 5001/20074 | Loss: 1.0551\n",
      "Epoch 3/100 | Step 5101/20074 | Loss: 0.0987\n",
      "Epoch 3/100 | Step 5201/20074 | Loss: 1.2216\n",
      "Epoch 3/100 | Step 5301/20074 | Loss: 1.2798\n",
      "Epoch 3/100 | Step 5401/20074 | Loss: 0.9730\n",
      "Epoch 3/100 | Step 5501/20074 | Loss: 1.9249\n",
      "Epoch 3/100 | Step 5601/20074 | Loss: 0.0016\n",
      "Epoch 3/100 | Step 5701/20074 | Loss: 0.1974\n",
      "Epoch 3/100 | Step 5801/20074 | Loss: 3.6131\n",
      "Epoch 3/100 | Step 5901/20074 | Loss: 0.0534\n",
      "Epoch 3/100 | Step 6001/20074 | Loss: 1.6937\n",
      "Epoch 3/100 | Step 6101/20074 | Loss: 1.9803\n",
      "Epoch 3/100 | Step 6201/20074 | Loss: 0.0304\n",
      "Epoch 3/100 | Step 6301/20074 | Loss: 2.5307\n",
      "Epoch 3/100 | Step 6401/20074 | Loss: 0.0525\n",
      "Epoch 3/100 | Step 6501/20074 | Loss: 0.0370\n",
      "Epoch 3/100 | Step 6601/20074 | Loss: 0.3293\n",
      "Epoch 3/100 | Step 6701/20074 | Loss: 0.5443\n",
      "Epoch 3/100 | Step 6801/20074 | Loss: 0.0519\n",
      "Epoch 3/100 | Step 6901/20074 | Loss: 0.0031\n",
      "Epoch 3/100 | Step 7001/20074 | Loss: 0.4299\n",
      "Epoch 3/100 | Step 7101/20074 | Loss: 1.7425\n",
      "Epoch 3/100 | Step 7201/20074 | Loss: 0.2342\n",
      "Epoch 3/100 | Step 7301/20074 | Loss: 0.0804\n",
      "Epoch 3/100 | Step 7401/20074 | Loss: 1.6202\n",
      "Epoch 3/100 | Step 7501/20074 | Loss: 2.7637\n",
      "Epoch 3/100 | Step 7601/20074 | Loss: 0.7766\n",
      "Epoch 3/100 | Step 7701/20074 | Loss: 0.0089\n",
      "Epoch 3/100 | Step 7801/20074 | Loss: 2.1030\n",
      "Epoch 3/100 | Step 7901/20074 | Loss: 0.1558\n",
      "Epoch 3/100 | Step 8001/20074 | Loss: 3.9739\n",
      "Epoch 3/100 | Step 8101/20074 | Loss: 1.0946\n",
      "Epoch 3/100 | Step 8201/20074 | Loss: 0.1211\n",
      "Epoch 3/100 | Step 8301/20074 | Loss: 1.9437\n",
      "Epoch 3/100 | Step 8401/20074 | Loss: 1.2905\n",
      "Epoch 3/100 | Step 8501/20074 | Loss: 0.4173\n",
      "Epoch 3/100 | Step 8601/20074 | Loss: 1.1799\n",
      "Epoch 3/100 | Step 8701/20074 | Loss: 1.4793\n",
      "Epoch 3/100 | Step 8801/20074 | Loss: 1.0260\n",
      "Epoch 3/100 | Step 8901/20074 | Loss: 3.4569\n",
      "Epoch 3/100 | Step 9001/20074 | Loss: 1.0513\n",
      "Epoch 3/100 | Step 9101/20074 | Loss: 1.0218\n",
      "Epoch 3/100 | Step 9201/20074 | Loss: 0.3634\n",
      "Epoch 3/100 | Step 9301/20074 | Loss: 0.2430\n",
      "Epoch 3/100 | Step 9401/20074 | Loss: 2.0571\n",
      "Epoch 3/100 | Step 9501/20074 | Loss: 1.5251\n",
      "Epoch 3/100 | Step 9601/20074 | Loss: 0.0053\n",
      "Epoch 3/100 | Step 9701/20074 | Loss: 1.2461\n",
      "Epoch 3/100 | Step 9801/20074 | Loss: 0.0305\n",
      "Epoch 3/100 | Step 9901/20074 | Loss: 0.1230\n",
      "Epoch 3/100 | Step 10001/20074 | Loss: 2.2173\n",
      "Epoch 3/100 | Step 10101/20074 | Loss: 0.0084\n",
      "Epoch 3/100 | Step 10201/20074 | Loss: 0.0499\n",
      "Epoch 3/100 | Step 10301/20074 | Loss: 0.0412\n",
      "Epoch 3/100 | Step 10401/20074 | Loss: 0.0514\n",
      "Epoch 3/100 | Step 10501/20074 | Loss: 0.5526\n",
      "Epoch 3/100 | Step 10601/20074 | Loss: 0.1803\n",
      "Epoch 3/100 | Step 10701/20074 | Loss: 0.0048\n",
      "Epoch 3/100 | Step 10801/20074 | Loss: 0.2309\n",
      "Epoch 3/100 | Step 10901/20074 | Loss: 0.0100\n",
      "Epoch 3/100 | Step 11001/20074 | Loss: 0.5585\n",
      "Epoch 3/100 | Step 11101/20074 | Loss: 1.5051\n",
      "Epoch 3/100 | Step 11201/20074 | Loss: 0.9863\n",
      "Epoch 3/100 | Step 11301/20074 | Loss: 0.2158\n",
      "Epoch 3/100 | Step 11401/20074 | Loss: 0.0212\n",
      "Epoch 3/100 | Step 11501/20074 | Loss: 0.2812\n",
      "Epoch 3/100 | Step 11601/20074 | Loss: 0.1092\n",
      "Epoch 3/100 | Step 11701/20074 | Loss: 0.1265\n",
      "Epoch 3/100 | Step 11801/20074 | Loss: 0.0111\n",
      "Epoch 3/100 | Step 11901/20074 | Loss: 1.9713\n",
      "Epoch 3/100 | Step 12001/20074 | Loss: 0.0813\n",
      "Epoch 3/100 | Step 12101/20074 | Loss: 1.5084\n",
      "Epoch 3/100 | Step 12201/20074 | Loss: 0.0054\n",
      "Epoch 3/100 | Step 12301/20074 | Loss: 0.8870\n",
      "Epoch 3/100 | Step 12401/20074 | Loss: 0.2665\n",
      "Epoch 3/100 | Step 12501/20074 | Loss: 0.1605\n",
      "Epoch 3/100 | Step 12601/20074 | Loss: 0.7515\n",
      "Epoch 3/100 | Step 12701/20074 | Loss: 0.3331\n",
      "Epoch 3/100 | Step 12801/20074 | Loss: 2.7452\n",
      "Epoch 3/100 | Step 12901/20074 | Loss: 0.3717\n",
      "Epoch 3/100 | Step 13001/20074 | Loss: 0.1822\n",
      "Epoch 3/100 | Step 13101/20074 | Loss: 1.3890\n",
      "Epoch 3/100 | Step 13201/20074 | Loss: 1.0650\n",
      "Epoch 3/100 | Step 13301/20074 | Loss: 1.4900\n",
      "Epoch 3/100 | Step 13401/20074 | Loss: 1.4660\n",
      "Epoch 3/100 | Step 13501/20074 | Loss: 1.1466\n",
      "Epoch 3/100 | Step 13601/20074 | Loss: 0.0013\n",
      "Epoch 3/100 | Step 13701/20074 | Loss: 1.2117\n",
      "Epoch 3/100 | Step 13801/20074 | Loss: 0.9064\n",
      "Epoch 3/100 | Step 13901/20074 | Loss: 3.3649\n",
      "Epoch 3/100 | Step 14001/20074 | Loss: 0.0173\n",
      "Epoch 3/100 | Step 14101/20074 | Loss: 0.8537\n",
      "Epoch 3/100 | Step 14201/20074 | Loss: 0.2173\n",
      "Epoch 3/100 | Step 14301/20074 | Loss: 0.9090\n",
      "Epoch 3/100 | Step 14401/20074 | Loss: 0.0371\n",
      "Epoch 3/100 | Step 14501/20074 | Loss: 0.0015\n",
      "Epoch 3/100 | Step 14601/20074 | Loss: 0.1592\n",
      "Epoch 3/100 | Step 14701/20074 | Loss: 0.3610\n",
      "Epoch 3/100 | Step 14801/20074 | Loss: 0.5229\n",
      "Epoch 3/100 | Step 14901/20074 | Loss: 2.2752\n",
      "Epoch 3/100 | Step 15001/20074 | Loss: 0.2999\n",
      "Epoch 3/100 | Step 15101/20074 | Loss: 0.0136\n",
      "Epoch 3/100 | Step 15201/20074 | Loss: 0.9250\n",
      "Epoch 3/100 | Step 15301/20074 | Loss: 0.0015\n",
      "Epoch 3/100 | Step 15401/20074 | Loss: 0.1480\n",
      "Epoch 3/100 | Step 15501/20074 | Loss: 0.9123\n",
      "Epoch 3/100 | Step 15601/20074 | Loss: 1.3975\n",
      "Epoch 3/100 | Step 15701/20074 | Loss: 1.1829\n",
      "Epoch 3/100 | Step 15801/20074 | Loss: 4.3086\n",
      "Epoch 3/100 | Step 15901/20074 | Loss: 0.3908\n",
      "Epoch 3/100 | Step 16001/20074 | Loss: 4.7775\n",
      "Epoch 3/100 | Step 16101/20074 | Loss: 0.0843\n",
      "Epoch 3/100 | Step 16201/20074 | Loss: 2.3941\n",
      "Epoch 3/100 | Step 16301/20074 | Loss: 0.6620\n",
      "Epoch 3/100 | Step 16401/20074 | Loss: 1.3911\n",
      "Epoch 3/100 | Step 16501/20074 | Loss: 1.2438\n",
      "Epoch 3/100 | Step 16601/20074 | Loss: 0.0013\n",
      "Epoch 3/100 | Step 16701/20074 | Loss: 0.0009\n",
      "Epoch 3/100 | Step 16801/20074 | Loss: 2.0221\n",
      "Epoch 3/100 | Step 16901/20074 | Loss: 2.9916\n",
      "Epoch 3/100 | Step 17001/20074 | Loss: 2.9059\n",
      "Epoch 3/100 | Step 17101/20074 | Loss: 0.0830\n",
      "Epoch 3/100 | Step 17201/20074 | Loss: 0.0006\n",
      "Epoch 3/100 | Step 17301/20074 | Loss: 2.5381\n",
      "Epoch 3/100 | Step 17401/20074 | Loss: 0.0763\n",
      "Epoch 3/100 | Step 17501/20074 | Loss: 0.3526\n",
      "Epoch 3/100 | Step 17601/20074 | Loss: 0.2563\n",
      "Epoch 3/100 | Step 17701/20074 | Loss: 0.4931\n",
      "Epoch 3/100 | Step 17801/20074 | Loss: 3.8870\n",
      "Epoch 3/100 | Step 17901/20074 | Loss: 0.1671\n",
      "Epoch 3/100 | Step 18001/20074 | Loss: 4.0054\n",
      "Epoch 3/100 | Step 18101/20074 | Loss: 0.2403\n",
      "Epoch 3/100 | Step 18201/20074 | Loss: 3.7927\n",
      "Epoch 3/100 | Step 18301/20074 | Loss: 0.2890\n",
      "Epoch 3/100 | Step 18401/20074 | Loss: 1.4578\n",
      "Epoch 3/100 | Step 18501/20074 | Loss: 2.7831\n",
      "Epoch 3/100 | Step 18601/20074 | Loss: 0.0302\n",
      "Epoch 3/100 | Step 18701/20074 | Loss: 0.0500\n",
      "Epoch 3/100 | Step 18801/20074 | Loss: 0.2748\n",
      "Epoch 3/100 | Step 18901/20074 | Loss: 2.2777\n",
      "Epoch 3/100 | Step 19001/20074 | Loss: 0.0139\n",
      "Epoch 3/100 | Step 19101/20074 | Loss: 0.0083\n",
      "Epoch 3/100 | Step 19201/20074 | Loss: 1.4276\n",
      "Epoch 3/100 | Step 19301/20074 | Loss: 2.2788\n",
      "Epoch 3/100 | Step 19401/20074 | Loss: 0.0157\n",
      "Epoch 3/100 | Step 19501/20074 | Loss: 1.3891\n",
      "Epoch 3/100 | Step 19601/20074 | Loss: 2.8339\n",
      "Epoch 3/100 | Step 19701/20074 | Loss: 0.2839\n",
      "Epoch 3/100 | Step 19801/20074 | Loss: 0.1293\n",
      "Epoch 3/100 | Step 19901/20074 | Loss: 0.1093\n",
      "Epoch 3/100 | Step 20001/20074 | Loss: 0.0736\n",
      "Epoch 4/100 | Step 1/20074 | Loss: 2.4381\n",
      "Epoch 4/100 | Step 101/20074 | Loss: 0.0005\n",
      "Epoch 4/100 | Step 201/20074 | Loss: 0.0723\n",
      "Epoch 4/100 | Step 301/20074 | Loss: 0.0319\n",
      "Epoch 4/100 | Step 401/20074 | Loss: 0.0083\n",
      "Epoch 4/100 | Step 501/20074 | Loss: 2.5221\n",
      "Epoch 4/100 | Step 601/20074 | Loss: 6.6714\n",
      "Epoch 4/100 | Step 701/20074 | Loss: 0.5437\n",
      "Epoch 4/100 | Step 801/20074 | Loss: 0.1278\n",
      "Epoch 4/100 | Step 901/20074 | Loss: 0.0091\n",
      "Epoch 4/100 | Step 1001/20074 | Loss: 0.5104\n",
      "Epoch 4/100 | Step 1101/20074 | Loss: 1.0057\n",
      "Epoch 4/100 | Step 1201/20074 | Loss: 0.1321\n",
      "Epoch 4/100 | Step 1301/20074 | Loss: 0.0474\n",
      "Epoch 4/100 | Step 1401/20074 | Loss: 0.0340\n",
      "Epoch 4/100 | Step 1501/20074 | Loss: 0.3607\n",
      "Epoch 4/100 | Step 1601/20074 | Loss: 0.1757\n",
      "Epoch 4/100 | Step 1701/20074 | Loss: 0.2078\n",
      "Epoch 4/100 | Step 1801/20074 | Loss: 1.2896\n",
      "Epoch 4/100 | Step 1901/20074 | Loss: 0.0953\n",
      "Epoch 4/100 | Step 2001/20074 | Loss: 0.0041\n",
      "Epoch 4/100 | Step 2101/20074 | Loss: 0.0009\n",
      "Epoch 4/100 | Step 2201/20074 | Loss: 0.2437\n",
      "Epoch 4/100 | Step 2301/20074 | Loss: 0.1246\n",
      "Epoch 4/100 | Step 2401/20074 | Loss: 0.0266\n",
      "Epoch 4/100 | Step 2501/20074 | Loss: 0.0057\n",
      "Epoch 4/100 | Step 2601/20074 | Loss: 0.0008\n",
      "Epoch 4/100 | Step 2701/20074 | Loss: 0.0612\n",
      "Epoch 4/100 | Step 2801/20074 | Loss: 0.0122\n",
      "Epoch 4/100 | Step 2901/20074 | Loss: 0.4024\n",
      "Epoch 4/100 | Step 3001/20074 | Loss: 1.5449\n",
      "Epoch 4/100 | Step 3101/20074 | Loss: 0.0153\n",
      "Epoch 4/100 | Step 3201/20074 | Loss: 3.8788\n",
      "Epoch 4/100 | Step 3301/20074 | Loss: 1.0627\n",
      "Epoch 4/100 | Step 3401/20074 | Loss: 0.0076\n",
      "Epoch 4/100 | Step 3501/20074 | Loss: 0.0326\n",
      "Epoch 4/100 | Step 3601/20074 | Loss: 2.9467\n",
      "Epoch 4/100 | Step 3701/20074 | Loss: 0.4490\n",
      "Epoch 4/100 | Step 3801/20074 | Loss: 2.6219\n",
      "Epoch 4/100 | Step 3901/20074 | Loss: 0.4896\n",
      "Epoch 4/100 | Step 4001/20074 | Loss: 0.0226\n",
      "Epoch 4/100 | Step 4101/20074 | Loss: 4.8616\n",
      "Epoch 4/100 | Step 4201/20074 | Loss: 0.1283\n",
      "Epoch 4/100 | Step 4301/20074 | Loss: 0.0403\n",
      "Epoch 4/100 | Step 4401/20074 | Loss: 0.4436\n",
      "Epoch 4/100 | Step 4501/20074 | Loss: 0.5233\n",
      "Epoch 4/100 | Step 4601/20074 | Loss: 1.4488\n",
      "Epoch 4/100 | Step 4701/20074 | Loss: 0.8326\n",
      "Epoch 4/100 | Step 4801/20074 | Loss: 0.1817\n",
      "Epoch 4/100 | Step 4901/20074 | Loss: 1.4052\n",
      "Epoch 4/100 | Step 5001/20074 | Loss: 0.0036\n",
      "Epoch 4/100 | Step 5101/20074 | Loss: 0.0803\n",
      "Epoch 4/100 | Step 5201/20074 | Loss: 1.0153\n",
      "Epoch 4/100 | Step 5301/20074 | Loss: 0.3601\n",
      "Epoch 4/100 | Step 5401/20074 | Loss: 1.9779\n",
      "Epoch 4/100 | Step 5501/20074 | Loss: 0.2097\n",
      "Epoch 4/100 | Step 5601/20074 | Loss: 2.1977\n",
      "Epoch 4/100 | Step 5701/20074 | Loss: 0.1415\n",
      "Epoch 4/100 | Step 5801/20074 | Loss: 1.4288\n",
      "Epoch 4/100 | Step 5901/20074 | Loss: 0.7836\n",
      "Epoch 4/100 | Step 6001/20074 | Loss: 0.4190\n",
      "Epoch 4/100 | Step 6101/20074 | Loss: 0.0371\n",
      "Epoch 4/100 | Step 6201/20074 | Loss: 0.0318\n",
      "Epoch 4/100 | Step 6301/20074 | Loss: 2.5794\n",
      "Epoch 4/100 | Step 6401/20074 | Loss: 0.0057\n",
      "Epoch 4/100 | Step 6501/20074 | Loss: 0.3736\n",
      "Epoch 4/100 | Step 6601/20074 | Loss: 0.1566\n",
      "Epoch 4/100 | Step 6701/20074 | Loss: 3.4870\n",
      "Epoch 4/100 | Step 6801/20074 | Loss: 0.5261\n",
      "Epoch 4/100 | Step 6901/20074 | Loss: 1.0563\n",
      "Epoch 4/100 | Step 7001/20074 | Loss: 0.1008\n",
      "Epoch 4/100 | Step 7101/20074 | Loss: 0.4858\n",
      "Epoch 4/100 | Step 7201/20074 | Loss: 1.3352\n",
      "Epoch 4/100 | Step 7301/20074 | Loss: 0.0016\n",
      "Epoch 4/100 | Step 7401/20074 | Loss: 6.8910\n",
      "Epoch 4/100 | Step 7501/20074 | Loss: 2.2543\n",
      "Epoch 4/100 | Step 7601/20074 | Loss: 0.8323\n",
      "Epoch 4/100 | Step 7701/20074 | Loss: 1.5462\n",
      "Epoch 4/100 | Step 7801/20074 | Loss: 0.0078\n",
      "Epoch 4/100 | Step 7901/20074 | Loss: 3.7648\n",
      "Epoch 4/100 | Step 8001/20074 | Loss: 2.7532\n",
      "Epoch 4/100 | Step 8101/20074 | Loss: 0.7231\n",
      "Epoch 4/100 | Step 8201/20074 | Loss: 0.0521\n",
      "Epoch 4/100 | Step 8301/20074 | Loss: 0.0021\n",
      "Epoch 4/100 | Step 8401/20074 | Loss: 0.2922\n",
      "Epoch 4/100 | Step 8501/20074 | Loss: 2.6600\n",
      "Epoch 4/100 | Step 8601/20074 | Loss: 0.7615\n",
      "Epoch 4/100 | Step 8701/20074 | Loss: 2.1365\n",
      "Epoch 4/100 | Step 8801/20074 | Loss: 0.0016\n",
      "Epoch 4/100 | Step 8901/20074 | Loss: 0.0110\n",
      "Epoch 4/100 | Step 9001/20074 | Loss: 0.3189\n",
      "Epoch 4/100 | Step 9101/20074 | Loss: 0.2909\n",
      "Epoch 4/100 | Step 9201/20074 | Loss: 0.0452\n",
      "Epoch 4/100 | Step 9301/20074 | Loss: 1.1871\n",
      "Epoch 4/100 | Step 9401/20074 | Loss: 0.7961\n",
      "Epoch 4/100 | Step 9501/20074 | Loss: 0.0279\n",
      "Epoch 4/100 | Step 9601/20074 | Loss: 0.0059\n",
      "Epoch 4/100 | Step 9701/20074 | Loss: 0.5412\n",
      "Epoch 4/100 | Step 9801/20074 | Loss: 0.0097\n",
      "Epoch 4/100 | Step 9901/20074 | Loss: 5.4014\n",
      "Epoch 4/100 | Step 10001/20074 | Loss: 0.4345\n",
      "Epoch 4/100 | Step 10101/20074 | Loss: 0.2788\n",
      "Epoch 4/100 | Step 10201/20074 | Loss: 0.8351\n",
      "Epoch 4/100 | Step 10301/20074 | Loss: 0.0122\n",
      "Epoch 4/100 | Step 10401/20074 | Loss: 0.3149\n",
      "Epoch 4/100 | Step 10501/20074 | Loss: 1.6561\n",
      "Epoch 4/100 | Step 10601/20074 | Loss: 0.0040\n",
      "Epoch 4/100 | Step 10701/20074 | Loss: 0.0540\n",
      "Epoch 4/100 | Step 10801/20074 | Loss: 3.1983\n",
      "Epoch 4/100 | Step 10901/20074 | Loss: 0.0234\n",
      "Epoch 4/100 | Step 11001/20074 | Loss: 0.0050\n",
      "Epoch 4/100 | Step 11101/20074 | Loss: 0.1227\n",
      "Epoch 4/100 | Step 11201/20074 | Loss: 4.0535\n",
      "Epoch 4/100 | Step 11301/20074 | Loss: 3.7465\n",
      "Epoch 4/100 | Step 11401/20074 | Loss: 3.1448\n",
      "Epoch 4/100 | Step 11501/20074 | Loss: 2.2717\n",
      "Epoch 4/100 | Step 11601/20074 | Loss: 0.0700\n",
      "Epoch 4/100 | Step 11701/20074 | Loss: 0.8122\n",
      "Epoch 4/100 | Step 11801/20074 | Loss: 1.5488\n",
      "Epoch 4/100 | Step 11901/20074 | Loss: 5.2241\n",
      "Epoch 4/100 | Step 12001/20074 | Loss: 1.2690\n",
      "Epoch 4/100 | Step 12101/20074 | Loss: 0.0189\n",
      "Epoch 4/100 | Step 12201/20074 | Loss: 0.7036\n",
      "Epoch 4/100 | Step 12301/20074 | Loss: 1.6678\n",
      "Epoch 4/100 | Step 12401/20074 | Loss: 4.4895\n",
      "Epoch 4/100 | Step 12501/20074 | Loss: 0.0044\n",
      "Epoch 4/100 | Step 12601/20074 | Loss: 0.0639\n",
      "Epoch 4/100 | Step 12701/20074 | Loss: 0.2401\n",
      "Epoch 4/100 | Step 12801/20074 | Loss: 0.3938\n",
      "Epoch 4/100 | Step 12901/20074 | Loss: 1.1789\n",
      "Epoch 4/100 | Step 13001/20074 | Loss: 0.3187\n",
      "Epoch 4/100 | Step 13101/20074 | Loss: 0.0788\n",
      "Epoch 4/100 | Step 13201/20074 | Loss: 0.1209\n",
      "Epoch 4/100 | Step 13301/20074 | Loss: 0.0582\n",
      "Epoch 4/100 | Step 13401/20074 | Loss: 0.0045\n",
      "Epoch 4/100 | Step 13501/20074 | Loss: 0.0077\n",
      "Epoch 4/100 | Step 13601/20074 | Loss: 1.9700\n",
      "Epoch 4/100 | Step 13701/20074 | Loss: 0.1224\n",
      "Epoch 4/100 | Step 13801/20074 | Loss: 0.4270\n",
      "Epoch 4/100 | Step 13901/20074 | Loss: 0.1259\n",
      "Epoch 4/100 | Step 14001/20074 | Loss: 1.3029\n",
      "Epoch 4/100 | Step 14101/20074 | Loss: 1.4516\n",
      "Epoch 4/100 | Step 14201/20074 | Loss: 0.2126\n",
      "Epoch 4/100 | Step 14301/20074 | Loss: 1.3240\n",
      "Epoch 4/100 | Step 14401/20074 | Loss: 0.0026\n",
      "Epoch 4/100 | Step 14501/20074 | Loss: 2.2676\n",
      "Epoch 4/100 | Step 14601/20074 | Loss: 5.1672\n",
      "Epoch 4/100 | Step 14701/20074 | Loss: 2.2680\n",
      "Epoch 4/100 | Step 14801/20074 | Loss: 0.1127\n",
      "Epoch 4/100 | Step 14901/20074 | Loss: 2.9600\n",
      "Epoch 4/100 | Step 15001/20074 | Loss: 6.3020\n",
      "Epoch 4/100 | Step 15101/20074 | Loss: 1.3627\n",
      "Epoch 4/100 | Step 15201/20074 | Loss: 1.3998\n",
      "Epoch 4/100 | Step 15301/20074 | Loss: 0.3212\n",
      "Epoch 4/100 | Step 15401/20074 | Loss: 0.6369\n",
      "Epoch 4/100 | Step 15501/20074 | Loss: 0.0028\n",
      "Epoch 4/100 | Step 15601/20074 | Loss: 1.0162\n",
      "Epoch 4/100 | Step 15701/20074 | Loss: 1.1670\n",
      "Epoch 4/100 | Step 15801/20074 | Loss: 1.5702\n",
      "Epoch 4/100 | Step 15901/20074 | Loss: 0.0912\n",
      "Epoch 4/100 | Step 16001/20074 | Loss: 0.9700\n",
      "Epoch 4/100 | Step 16101/20074 | Loss: 0.0089\n",
      "Epoch 4/100 | Step 16201/20074 | Loss: 3.3965\n",
      "Epoch 4/100 | Step 16301/20074 | Loss: 0.3231\n",
      "Epoch 4/100 | Step 16401/20074 | Loss: 0.1898\n",
      "Epoch 4/100 | Step 16501/20074 | Loss: 0.0058\n",
      "Epoch 4/100 | Step 16601/20074 | Loss: 0.3219\n",
      "Epoch 4/100 | Step 16701/20074 | Loss: 1.4911\n",
      "Epoch 4/100 | Step 16801/20074 | Loss: 0.0139\n",
      "Epoch 4/100 | Step 16901/20074 | Loss: 3.6845\n",
      "Epoch 4/100 | Step 17001/20074 | Loss: 2.8559\n",
      "Epoch 4/100 | Step 17101/20074 | Loss: 1.2955\n",
      "Epoch 4/100 | Step 17201/20074 | Loss: 0.1810\n",
      "Epoch 4/100 | Step 17301/20074 | Loss: 0.8814\n",
      "Epoch 4/100 | Step 17401/20074 | Loss: 0.3354\n",
      "Epoch 4/100 | Step 17501/20074 | Loss: 0.2066\n",
      "Epoch 4/100 | Step 17601/20074 | Loss: 0.3270\n",
      "Epoch 4/100 | Step 17701/20074 | Loss: 0.0025\n",
      "Epoch 4/100 | Step 17801/20074 | Loss: 3.0978\n",
      "Epoch 4/100 | Step 17901/20074 | Loss: 0.0121\n",
      "Epoch 4/100 | Step 18001/20074 | Loss: 2.2932\n",
      "Epoch 4/100 | Step 18101/20074 | Loss: 0.4615\n",
      "Epoch 4/100 | Step 18201/20074 | Loss: 0.0382\n",
      "Epoch 4/100 | Step 18301/20074 | Loss: 4.1375\n",
      "Epoch 4/100 | Step 18401/20074 | Loss: 0.0306\n",
      "Epoch 4/100 | Step 18501/20074 | Loss: 0.4959\n",
      "Epoch 4/100 | Step 18601/20074 | Loss: 0.0050\n",
      "Epoch 4/100 | Step 18701/20074 | Loss: 0.0843\n",
      "Epoch 4/100 | Step 18801/20074 | Loss: 0.2402\n",
      "Epoch 4/100 | Step 18901/20074 | Loss: 5.3364\n",
      "Epoch 4/100 | Step 19001/20074 | Loss: 2.5527\n",
      "Epoch 4/100 | Step 19101/20074 | Loss: 0.0640\n",
      "Epoch 4/100 | Step 19201/20074 | Loss: 3.0304\n",
      "Epoch 4/100 | Step 19301/20074 | Loss: 0.1941\n",
      "Epoch 4/100 | Step 19401/20074 | Loss: 1.5085\n",
      "Epoch 4/100 | Step 19501/20074 | Loss: 0.5320\n",
      "Epoch 4/100 | Step 19601/20074 | Loss: 0.8733\n",
      "Epoch 4/100 | Step 19701/20074 | Loss: 0.2787\n",
      "Epoch 4/100 | Step 19801/20074 | Loss: 0.4821\n",
      "Epoch 4/100 | Step 19901/20074 | Loss: 0.1382\n",
      "Epoch 4/100 | Step 20001/20074 | Loss: 1.0209\n",
      "Epoch 5/100 | Step 1/20074 | Loss: 0.5655\n",
      "Epoch 5/100 | Step 101/20074 | Loss: 0.1771\n",
      "Epoch 5/100 | Step 201/20074 | Loss: 0.3365\n",
      "Epoch 5/100 | Step 301/20074 | Loss: 0.2102\n",
      "Epoch 5/100 | Step 401/20074 | Loss: 0.6798\n",
      "Epoch 5/100 | Step 501/20074 | Loss: 3.7418\n",
      "Epoch 5/100 | Step 601/20074 | Loss: 0.1693\n",
      "Epoch 5/100 | Step 701/20074 | Loss: 0.6116\n",
      "Epoch 5/100 | Step 801/20074 | Loss: 4.2682\n",
      "Epoch 5/100 | Step 901/20074 | Loss: 0.1866\n",
      "Epoch 5/100 | Step 1001/20074 | Loss: 1.2820\n",
      "Epoch 5/100 | Step 1101/20074 | Loss: 0.0077\n",
      "Epoch 5/100 | Step 1201/20074 | Loss: 0.7986\n",
      "Epoch 5/100 | Step 1301/20074 | Loss: 0.0241\n",
      "Epoch 5/100 | Step 1401/20074 | Loss: 0.0352\n",
      "Epoch 5/100 | Step 1501/20074 | Loss: 0.0121\n",
      "Epoch 5/100 | Step 1601/20074 | Loss: 0.7337\n",
      "Epoch 5/100 | Step 1701/20074 | Loss: 0.1238\n",
      "Epoch 5/100 | Step 1801/20074 | Loss: 5.8206\n",
      "Epoch 5/100 | Step 1901/20074 | Loss: 0.4389\n",
      "Epoch 5/100 | Step 2001/20074 | Loss: 0.1091\n",
      "Epoch 5/100 | Step 2101/20074 | Loss: 0.9685\n",
      "Epoch 5/100 | Step 2201/20074 | Loss: 0.1141\n",
      "Epoch 5/100 | Step 2301/20074 | Loss: 2.9246\n",
      "Epoch 5/100 | Step 2401/20074 | Loss: 0.0473\n",
      "Epoch 5/100 | Step 2501/20074 | Loss: 0.5559\n",
      "Epoch 5/100 | Step 2601/20074 | Loss: 0.2356\n",
      "Epoch 5/100 | Step 2701/20074 | Loss: 1.9536\n",
      "Epoch 5/100 | Step 2801/20074 | Loss: 0.0210\n",
      "Epoch 5/100 | Step 2901/20074 | Loss: 0.9648\n",
      "Epoch 5/100 | Step 3001/20074 | Loss: 0.6311\n",
      "Epoch 5/100 | Step 3101/20074 | Loss: 0.0013\n",
      "Epoch 5/100 | Step 3201/20074 | Loss: 0.0859\n",
      "Epoch 5/100 | Step 3301/20074 | Loss: 0.5899\n",
      "Epoch 5/100 | Step 3401/20074 | Loss: 0.2385\n",
      "Epoch 5/100 | Step 3501/20074 | Loss: 0.0098\n",
      "Epoch 5/100 | Step 3601/20074 | Loss: 0.0000\n",
      "Epoch 5/100 | Step 3701/20074 | Loss: 0.0985\n",
      "Epoch 5/100 | Step 3801/20074 | Loss: 0.1533\n",
      "Epoch 5/100 | Step 3901/20074 | Loss: 1.0032\n",
      "Epoch 5/100 | Step 4001/20074 | Loss: 2.8361\n",
      "Epoch 5/100 | Step 4101/20074 | Loss: 0.0631\n",
      "Epoch 5/100 | Step 4201/20074 | Loss: 2.1103\n",
      "Epoch 5/100 | Step 4301/20074 | Loss: 0.8304\n",
      "Epoch 5/100 | Step 4401/20074 | Loss: 1.5115\n",
      "Epoch 5/100 | Step 4501/20074 | Loss: 0.9296\n",
      "Epoch 5/100 | Step 4601/20074 | Loss: 1.6149\n",
      "Epoch 5/100 | Step 4701/20074 | Loss: 2.2966\n",
      "Epoch 5/100 | Step 4801/20074 | Loss: 0.1278\n",
      "Epoch 5/100 | Step 4901/20074 | Loss: 0.0057\n",
      "Epoch 5/100 | Step 5001/20074 | Loss: 1.8430\n",
      "Epoch 5/100 | Step 5101/20074 | Loss: 1.2661\n",
      "Epoch 5/100 | Step 5201/20074 | Loss: 0.2074\n",
      "Epoch 5/100 | Step 5301/20074 | Loss: 0.0042\n",
      "Epoch 5/100 | Step 5401/20074 | Loss: 0.6210\n",
      "Epoch 5/100 | Step 5501/20074 | Loss: 0.6489\n",
      "Epoch 5/100 | Step 5601/20074 | Loss: 0.6864\n",
      "Epoch 5/100 | Step 5701/20074 | Loss: 0.0061\n",
      "Epoch 5/100 | Step 5801/20074 | Loss: 0.0009\n",
      "Epoch 5/100 | Step 5901/20074 | Loss: 0.8235\n",
      "Epoch 5/100 | Step 6001/20074 | Loss: 0.0033\n",
      "Epoch 5/100 | Step 6101/20074 | Loss: 0.9544\n",
      "Epoch 5/100 | Step 6201/20074 | Loss: 0.8497\n",
      "Epoch 5/100 | Step 6301/20074 | Loss: 1.2197\n",
      "Epoch 5/100 | Step 6401/20074 | Loss: 0.5181\n",
      "Epoch 5/100 | Step 6501/20074 | Loss: 5.0043\n",
      "Epoch 5/100 | Step 6601/20074 | Loss: 0.0076\n",
      "Epoch 5/100 | Step 6701/20074 | Loss: 0.9946\n",
      "Epoch 5/100 | Step 6801/20074 | Loss: 0.8592\n",
      "Epoch 5/100 | Step 6901/20074 | Loss: 1.5988\n",
      "Epoch 5/100 | Step 7001/20074 | Loss: 0.2785\n",
      "Epoch 5/100 | Step 7101/20074 | Loss: 0.0267\n",
      "Epoch 5/100 | Step 7201/20074 | Loss: 1.2779\n",
      "Epoch 5/100 | Step 7301/20074 | Loss: 3.8240\n",
      "Epoch 5/100 | Step 7401/20074 | Loss: 0.6080\n",
      "Epoch 5/100 | Step 7501/20074 | Loss: 1.0168\n",
      "Epoch 5/100 | Step 7601/20074 | Loss: 0.0007\n",
      "Epoch 5/100 | Step 7701/20074 | Loss: 2.6473\n",
      "Epoch 5/100 | Step 7801/20074 | Loss: 0.1251\n",
      "Epoch 5/100 | Step 7901/20074 | Loss: 0.0026\n",
      "Epoch 5/100 | Step 8001/20074 | Loss: 0.7904\n",
      "Epoch 5/100 | Step 8101/20074 | Loss: 1.4345\n",
      "Epoch 5/100 | Step 8201/20074 | Loss: 0.0732\n",
      "Epoch 5/100 | Step 8301/20074 | Loss: 0.0979\n",
      "Epoch 5/100 | Step 8401/20074 | Loss: 0.0198\n",
      "Epoch 5/100 | Step 8501/20074 | Loss: 0.0856\n",
      "Epoch 5/100 | Step 8601/20074 | Loss: 1.6229\n",
      "Epoch 5/100 | Step 8701/20074 | Loss: 0.0064\n",
      "Epoch 5/100 | Step 8801/20074 | Loss: 0.2960\n",
      "Epoch 5/100 | Step 8901/20074 | Loss: 0.0956\n",
      "Epoch 5/100 | Step 9001/20074 | Loss: 1.6940\n",
      "Epoch 5/100 | Step 9101/20074 | Loss: 0.0402\n",
      "Epoch 5/100 | Step 9201/20074 | Loss: 0.0178\n",
      "Epoch 5/100 | Step 9301/20074 | Loss: 0.1333\n",
      "Epoch 5/100 | Step 9401/20074 | Loss: 1.9700\n",
      "Epoch 5/100 | Step 9501/20074 | Loss: 0.0065\n",
      "Epoch 5/100 | Step 9601/20074 | Loss: 2.0503\n",
      "Epoch 5/100 | Step 9701/20074 | Loss: 0.0145\n",
      "Epoch 5/100 | Step 9801/20074 | Loss: 0.1564\n",
      "Epoch 5/100 | Step 9901/20074 | Loss: 0.0781\n",
      "Epoch 5/100 | Step 10001/20074 | Loss: 0.5288\n",
      "Epoch 5/100 | Step 10101/20074 | Loss: 0.0845\n",
      "Epoch 5/100 | Step 10201/20074 | Loss: 0.1160\n",
      "Epoch 5/100 | Step 10301/20074 | Loss: 0.1024\n",
      "Epoch 5/100 | Step 10401/20074 | Loss: 0.4125\n",
      "Epoch 5/100 | Step 10501/20074 | Loss: 0.6634\n",
      "Epoch 5/100 | Step 10601/20074 | Loss: 0.0107\n",
      "Epoch 5/100 | Step 10701/20074 | Loss: 0.0189\n",
      "Epoch 5/100 | Step 10801/20074 | Loss: 0.0394\n",
      "Epoch 5/100 | Step 10901/20074 | Loss: 0.7154\n",
      "Epoch 5/100 | Step 11001/20074 | Loss: 3.2083\n",
      "Epoch 5/100 | Step 11101/20074 | Loss: 0.0013\n",
      "Epoch 5/100 | Step 11201/20074 | Loss: 0.6390\n",
      "Epoch 5/100 | Step 11301/20074 | Loss: 0.1985\n",
      "Epoch 5/100 | Step 11401/20074 | Loss: 3.4814\n",
      "Epoch 5/100 | Step 11501/20074 | Loss: 1.0238\n",
      "Epoch 5/100 | Step 11601/20074 | Loss: 0.4303\n",
      "Epoch 5/100 | Step 11701/20074 | Loss: 2.0811\n",
      "Epoch 5/100 | Step 11801/20074 | Loss: 0.2433\n",
      "Epoch 5/100 | Step 11901/20074 | Loss: 1.2572\n",
      "Epoch 5/100 | Step 12001/20074 | Loss: 3.2227\n",
      "Epoch 5/100 | Step 12101/20074 | Loss: 0.2846\n",
      "Epoch 5/100 | Step 12201/20074 | Loss: 0.6412\n",
      "Epoch 5/100 | Step 12301/20074 | Loss: 2.9207\n",
      "Epoch 5/100 | Step 12401/20074 | Loss: 0.4583\n",
      "Epoch 5/100 | Step 12501/20074 | Loss: 0.2967\n",
      "Epoch 5/100 | Step 12601/20074 | Loss: 3.6879\n",
      "Epoch 5/100 | Step 12701/20074 | Loss: 0.0004\n",
      "Epoch 5/100 | Step 12801/20074 | Loss: 0.4869\n",
      "Epoch 5/100 | Step 12901/20074 | Loss: 3.0005\n",
      "Epoch 5/100 | Step 13001/20074 | Loss: 1.5986\n",
      "Epoch 5/100 | Step 13101/20074 | Loss: 0.0008\n",
      "Epoch 5/100 | Step 13201/20074 | Loss: 0.0269\n",
      "Epoch 5/100 | Step 13301/20074 | Loss: 0.4126\n",
      "Epoch 5/100 | Step 13401/20074 | Loss: 0.0035\n",
      "Epoch 5/100 | Step 13501/20074 | Loss: 1.1451\n",
      "Epoch 5/100 | Step 13601/20074 | Loss: 3.1289\n",
      "Epoch 5/100 | Step 13701/20074 | Loss: 0.6608\n",
      "Epoch 5/100 | Step 13801/20074 | Loss: 2.5653\n",
      "Epoch 5/100 | Step 13901/20074 | Loss: 0.1801\n",
      "Epoch 5/100 | Step 14001/20074 | Loss: 0.9149\n",
      "Epoch 5/100 | Step 14101/20074 | Loss: 0.0610\n",
      "Epoch 5/100 | Step 14201/20074 | Loss: 0.0706\n",
      "Epoch 5/100 | Step 14301/20074 | Loss: 0.5597\n",
      "Epoch 5/100 | Step 14401/20074 | Loss: 2.6458\n",
      "Epoch 5/100 | Step 14501/20074 | Loss: 0.0580\n",
      "Epoch 5/100 | Step 14601/20074 | Loss: 0.8318\n",
      "Epoch 5/100 | Step 14701/20074 | Loss: 0.2477\n",
      "Epoch 5/100 | Step 14801/20074 | Loss: 0.1780\n",
      "Epoch 5/100 | Step 14901/20074 | Loss: 0.0945\n",
      "Epoch 5/100 | Step 15001/20074 | Loss: 0.5918\n",
      "Epoch 5/100 | Step 15101/20074 | Loss: 0.0842\n",
      "Epoch 5/100 | Step 15201/20074 | Loss: 0.0455\n",
      "Epoch 5/100 | Step 15301/20074 | Loss: 0.0376\n",
      "Epoch 5/100 | Step 15401/20074 | Loss: 0.0508\n",
      "Epoch 5/100 | Step 15501/20074 | Loss: 0.9872\n",
      "Epoch 5/100 | Step 15601/20074 | Loss: 1.7701\n",
      "Epoch 5/100 | Step 15701/20074 | Loss: 0.8726\n",
      "Epoch 5/100 | Step 15801/20074 | Loss: 1.0912\n",
      "Epoch 5/100 | Step 15901/20074 | Loss: 0.4609\n",
      "Epoch 5/100 | Step 16001/20074 | Loss: 1.0049\n",
      "Epoch 5/100 | Step 16101/20074 | Loss: 0.2469\n",
      "Epoch 5/100 | Step 16201/20074 | Loss: 0.3931\n",
      "Epoch 5/100 | Step 16301/20074 | Loss: 0.0502\n",
      "Epoch 5/100 | Step 16401/20074 | Loss: 3.6535\n",
      "Epoch 5/100 | Step 16501/20074 | Loss: 4.2890\n",
      "Epoch 5/100 | Step 16601/20074 | Loss: 1.1647\n",
      "Epoch 5/100 | Step 16701/20074 | Loss: 5.0641\n",
      "Epoch 5/100 | Step 16801/20074 | Loss: 0.3359\n",
      "Epoch 5/100 | Step 16901/20074 | Loss: 0.0066\n",
      "Epoch 5/100 | Step 17001/20074 | Loss: 1.7703\n",
      "Epoch 5/100 | Step 17101/20074 | Loss: 7.1868\n",
      "Epoch 5/100 | Step 17201/20074 | Loss: 0.1000\n",
      "Epoch 5/100 | Step 17301/20074 | Loss: 0.9855\n",
      "Epoch 5/100 | Step 17401/20074 | Loss: 1.3165\n",
      "Epoch 5/100 | Step 17501/20074 | Loss: 0.0088\n",
      "Epoch 5/100 | Step 17601/20074 | Loss: 0.3877\n",
      "Epoch 5/100 | Step 17701/20074 | Loss: 0.7917\n",
      "Epoch 5/100 | Step 17801/20074 | Loss: 1.6465\n",
      "Epoch 5/100 | Step 17901/20074 | Loss: 1.0505\n",
      "Epoch 5/100 | Step 18001/20074 | Loss: 0.0019\n",
      "Epoch 5/100 | Step 18101/20074 | Loss: 0.9673\n",
      "Epoch 5/100 | Step 18201/20074 | Loss: 0.1578\n",
      "Epoch 5/100 | Step 18301/20074 | Loss: 0.0067\n",
      "Epoch 5/100 | Step 18401/20074 | Loss: 2.0426\n",
      "Epoch 5/100 | Step 18501/20074 | Loss: 0.1469\n",
      "Epoch 5/100 | Step 18601/20074 | Loss: 0.4398\n",
      "Epoch 5/100 | Step 18701/20074 | Loss: 1.1896\n",
      "Epoch 5/100 | Step 18801/20074 | Loss: 2.9000\n",
      "Epoch 5/100 | Step 18901/20074 | Loss: 0.4058\n",
      "Epoch 5/100 | Step 19001/20074 | Loss: 2.6476\n",
      "Epoch 5/100 | Step 19101/20074 | Loss: 0.0016\n",
      "Epoch 5/100 | Step 19201/20074 | Loss: 0.0056\n",
      "Epoch 5/100 | Step 19301/20074 | Loss: 0.1780\n",
      "Epoch 5/100 | Step 19401/20074 | Loss: 1.0335\n",
      "Epoch 5/100 | Step 19501/20074 | Loss: 0.1746\n",
      "Epoch 5/100 | Step 19601/20074 | Loss: 1.3458\n",
      "Epoch 5/100 | Step 19701/20074 | Loss: 0.1798\n",
      "Epoch 5/100 | Step 19801/20074 | Loss: 2.6432\n",
      "Epoch 5/100 | Step 19901/20074 | Loss: 0.0209\n",
      "Epoch 5/100 | Step 20001/20074 | Loss: 1.8199\n",
      "Epoch 6/100 | Step 1/20074 | Loss: 0.4186\n",
      "Epoch 6/100 | Step 101/20074 | Loss: 0.1350\n",
      "Epoch 6/100 | Step 201/20074 | Loss: 0.3102\n",
      "Epoch 6/100 | Step 301/20074 | Loss: 0.7590\n",
      "Epoch 6/100 | Step 401/20074 | Loss: 1.6601\n",
      "Epoch 6/100 | Step 501/20074 | Loss: 0.1250\n",
      "Epoch 6/100 | Step 601/20074 | Loss: 0.5500\n",
      "Epoch 6/100 | Step 701/20074 | Loss: 0.8448\n",
      "Epoch 6/100 | Step 801/20074 | Loss: 1.2558\n",
      "Epoch 6/100 | Step 901/20074 | Loss: 1.1869\n",
      "Epoch 6/100 | Step 1001/20074 | Loss: 2.9953\n",
      "Epoch 6/100 | Step 1101/20074 | Loss: 2.4609\n",
      "Epoch 6/100 | Step 1201/20074 | Loss: 0.1677\n",
      "Epoch 6/100 | Step 1301/20074 | Loss: 0.1284\n",
      "Epoch 6/100 | Step 1401/20074 | Loss: 0.0150\n",
      "Epoch 6/100 | Step 1501/20074 | Loss: 0.0259\n",
      "Epoch 6/100 | Step 1601/20074 | Loss: 1.0144\n",
      "Epoch 6/100 | Step 1701/20074 | Loss: 0.0012\n",
      "Epoch 6/100 | Step 1801/20074 | Loss: 0.0183\n",
      "Epoch 6/100 | Step 1901/20074 | Loss: 0.0436\n",
      "Epoch 6/100 | Step 2001/20074 | Loss: 0.1009\n",
      "Epoch 6/100 | Step 2101/20074 | Loss: 0.0006\n",
      "Epoch 6/100 | Step 2201/20074 | Loss: 0.0010\n",
      "Epoch 6/100 | Step 2301/20074 | Loss: 0.0376\n",
      "Epoch 6/100 | Step 2401/20074 | Loss: 0.0361\n",
      "Epoch 6/100 | Step 2501/20074 | Loss: 1.1355\n",
      "Epoch 6/100 | Step 2601/20074 | Loss: 0.0028\n",
      "Epoch 6/100 | Step 2701/20074 | Loss: 0.2590\n",
      "Epoch 6/100 | Step 2801/20074 | Loss: 0.9821\n",
      "Epoch 6/100 | Step 2901/20074 | Loss: 0.3723\n",
      "Epoch 6/100 | Step 3001/20074 | Loss: 1.1552\n",
      "Epoch 6/100 | Step 3101/20074 | Loss: 0.1849\n",
      "Epoch 6/100 | Step 3201/20074 | Loss: 0.0044\n",
      "Epoch 6/100 | Step 3301/20074 | Loss: 2.5983\n",
      "Epoch 6/100 | Step 3401/20074 | Loss: 0.3451\n",
      "Epoch 6/100 | Step 3501/20074 | Loss: 0.0093\n",
      "Epoch 6/100 | Step 3601/20074 | Loss: 0.8261\n",
      "Epoch 6/100 | Step 3701/20074 | Loss: 0.6451\n",
      "Epoch 6/100 | Step 3801/20074 | Loss: 0.5124\n",
      "Epoch 6/100 | Step 3901/20074 | Loss: 0.6012\n",
      "Epoch 6/100 | Step 4001/20074 | Loss: 0.0970\n",
      "Epoch 6/100 | Step 4101/20074 | Loss: 0.2295\n",
      "Epoch 6/100 | Step 4201/20074 | Loss: 0.0781\n",
      "Epoch 6/100 | Step 4301/20074 | Loss: 2.7120\n",
      "Epoch 6/100 | Step 4401/20074 | Loss: 1.0798\n",
      "Epoch 6/100 | Step 4501/20074 | Loss: 0.0003\n",
      "Epoch 6/100 | Step 4601/20074 | Loss: 0.0377\n",
      "Epoch 6/100 | Step 4701/20074 | Loss: 4.6286\n",
      "Epoch 6/100 | Step 4801/20074 | Loss: 0.0028\n",
      "Epoch 6/100 | Step 4901/20074 | Loss: 0.0416\n",
      "Epoch 6/100 | Step 5001/20074 | Loss: 0.0442\n",
      "Epoch 6/100 | Step 5101/20074 | Loss: 0.1245\n",
      "Epoch 6/100 | Step 5201/20074 | Loss: 2.9920\n",
      "Epoch 6/100 | Step 5301/20074 | Loss: 1.1511\n",
      "Epoch 6/100 | Step 5401/20074 | Loss: 0.0604\n",
      "Epoch 6/100 | Step 5501/20074 | Loss: 2.0478\n",
      "Epoch 6/100 | Step 5601/20074 | Loss: 0.0011\n",
      "Epoch 6/100 | Step 5701/20074 | Loss: 0.0840\n",
      "Epoch 6/100 | Step 5801/20074 | Loss: 1.8341\n",
      "Epoch 6/100 | Step 5901/20074 | Loss: 3.0834\n",
      "Epoch 6/100 | Step 6001/20074 | Loss: 0.0339\n",
      "Epoch 6/100 | Step 6101/20074 | Loss: 0.3476\n",
      "Epoch 6/100 | Step 6201/20074 | Loss: 0.0073\n",
      "Epoch 6/100 | Step 6301/20074 | Loss: 0.0032\n",
      "Epoch 6/100 | Step 6401/20074 | Loss: 0.9091\n",
      "Epoch 6/100 | Step 6501/20074 | Loss: 2.7900\n",
      "Epoch 6/100 | Step 6601/20074 | Loss: 0.3384\n",
      "Epoch 6/100 | Step 6701/20074 | Loss: 0.0049\n",
      "Epoch 6/100 | Step 6801/20074 | Loss: 0.0300\n",
      "Epoch 6/100 | Step 6901/20074 | Loss: 0.0087\n",
      "Epoch 6/100 | Step 7001/20074 | Loss: 0.3819\n",
      "Epoch 6/100 | Step 7101/20074 | Loss: 0.2928\n",
      "Epoch 6/100 | Step 7201/20074 | Loss: 0.0058\n",
      "Epoch 6/100 | Step 7301/20074 | Loss: 4.0417\n",
      "Epoch 6/100 | Step 7401/20074 | Loss: 3.6192\n",
      "Epoch 6/100 | Step 7501/20074 | Loss: 0.3069\n",
      "Epoch 6/100 | Step 7601/20074 | Loss: 0.0033\n",
      "Epoch 6/100 | Step 7701/20074 | Loss: 0.0167\n",
      "Epoch 6/100 | Step 7801/20074 | Loss: 0.2996\n",
      "Epoch 6/100 | Step 7901/20074 | Loss: 0.3523\n",
      "Epoch 6/100 | Step 8001/20074 | Loss: 2.7983\n",
      "Epoch 6/100 | Step 8101/20074 | Loss: 0.3438\n",
      "Epoch 6/100 | Step 8201/20074 | Loss: 0.0018\n",
      "Epoch 6/100 | Step 8301/20074 | Loss: 0.4266\n",
      "Epoch 6/100 | Step 8401/20074 | Loss: 0.5293\n",
      "Epoch 6/100 | Step 8501/20074 | Loss: 0.2994\n",
      "Epoch 6/100 | Step 8601/20074 | Loss: 0.6308\n",
      "Epoch 6/100 | Step 8701/20074 | Loss: 0.0192\n",
      "Epoch 6/100 | Step 8801/20074 | Loss: 0.0114\n",
      "Epoch 6/100 | Step 8901/20074 | Loss: 0.4348\n",
      "Epoch 6/100 | Step 9001/20074 | Loss: 1.5033\n",
      "Epoch 6/100 | Step 9101/20074 | Loss: 0.6747\n",
      "Epoch 6/100 | Step 9201/20074 | Loss: 0.6336\n",
      "Epoch 6/100 | Step 9301/20074 | Loss: 0.4819\n",
      "Epoch 6/100 | Step 9401/20074 | Loss: 0.0013\n",
      "Epoch 6/100 | Step 9501/20074 | Loss: 0.1233\n",
      "Epoch 6/100 | Step 9601/20074 | Loss: 0.0227\n",
      "Epoch 6/100 | Step 9701/20074 | Loss: 1.8370\n",
      "Epoch 6/100 | Step 9801/20074 | Loss: 0.0037\n",
      "Epoch 6/100 | Step 9901/20074 | Loss: 0.0127\n",
      "Epoch 6/100 | Step 10001/20074 | Loss: 0.5327\n",
      "Epoch 6/100 | Step 10101/20074 | Loss: 0.0085\n",
      "Epoch 6/100 | Step 10201/20074 | Loss: 0.6266\n",
      "Epoch 6/100 | Step 10301/20074 | Loss: 0.0508\n",
      "Epoch 6/100 | Step 10401/20074 | Loss: 0.7518\n",
      "Epoch 6/100 | Step 10501/20074 | Loss: 1.0227\n",
      "Epoch 6/100 | Step 10601/20074 | Loss: 0.7530\n",
      "Epoch 6/100 | Step 10701/20074 | Loss: 2.1105\n",
      "Epoch 6/100 | Step 10801/20074 | Loss: 1.4379\n",
      "Epoch 6/100 | Step 10901/20074 | Loss: 3.3631\n",
      "Epoch 6/100 | Step 11001/20074 | Loss: 0.5132\n",
      "Epoch 6/100 | Step 11101/20074 | Loss: 0.1623\n",
      "Epoch 6/100 | Step 11201/20074 | Loss: 0.9987\n",
      "Epoch 6/100 | Step 11301/20074 | Loss: 0.0082\n",
      "Epoch 6/100 | Step 11401/20074 | Loss: 3.9776\n",
      "Epoch 6/100 | Step 11501/20074 | Loss: 0.0469\n",
      "Epoch 6/100 | Step 11601/20074 | Loss: 5.1492\n",
      "Epoch 6/100 | Step 11701/20074 | Loss: 0.0257\n",
      "Epoch 6/100 | Step 11801/20074 | Loss: 2.9730\n",
      "Epoch 6/100 | Step 11901/20074 | Loss: 0.4088\n",
      "Epoch 6/100 | Step 12001/20074 | Loss: 0.0054\n",
      "Epoch 6/100 | Step 12101/20074 | Loss: 0.0002\n",
      "Epoch 6/100 | Step 12201/20074 | Loss: 0.0036\n",
      "Epoch 6/100 | Step 12301/20074 | Loss: 0.3192\n",
      "Epoch 6/100 | Step 12401/20074 | Loss: 1.5628\n",
      "Epoch 6/100 | Step 12501/20074 | Loss: 0.8876\n",
      "Epoch 6/100 | Step 12601/20074 | Loss: 0.0001\n",
      "Epoch 6/100 | Step 12701/20074 | Loss: 0.1360\n",
      "Epoch 6/100 | Step 12801/20074 | Loss: 0.6094\n",
      "Epoch 6/100 | Step 12901/20074 | Loss: 0.0399\n",
      "Epoch 6/100 | Step 13001/20074 | Loss: 1.2349\n",
      "Epoch 6/100 | Step 13101/20074 | Loss: 0.0008\n",
      "Epoch 6/100 | Step 13201/20074 | Loss: 0.2605\n",
      "Epoch 6/100 | Step 13301/20074 | Loss: 0.9050\n",
      "Epoch 6/100 | Step 13401/20074 | Loss: 1.6041\n",
      "Epoch 6/100 | Step 13501/20074 | Loss: 1.0284\n",
      "Epoch 6/100 | Step 13601/20074 | Loss: 0.6009\n",
      "Epoch 6/100 | Step 13701/20074 | Loss: 3.9652\n",
      "Epoch 6/100 | Step 13801/20074 | Loss: 0.0012\n",
      "Epoch 6/100 | Step 13901/20074 | Loss: 4.5288\n",
      "Epoch 6/100 | Step 14001/20074 | Loss: 0.1718\n",
      "Epoch 6/100 | Step 14101/20074 | Loss: 3.5857\n",
      "Epoch 6/100 | Step 14201/20074 | Loss: 0.0028\n",
      "Epoch 6/100 | Step 14301/20074 | Loss: 0.5436\n",
      "Epoch 6/100 | Step 14401/20074 | Loss: 0.0251\n",
      "Epoch 6/100 | Step 14501/20074 | Loss: 0.1022\n",
      "Epoch 6/100 | Step 14601/20074 | Loss: 0.0646\n",
      "Epoch 6/100 | Step 14701/20074 | Loss: 0.0030\n",
      "Epoch 6/100 | Step 14801/20074 | Loss: 0.9545\n",
      "Epoch 6/100 | Step 14901/20074 | Loss: 2.3105\n",
      "Epoch 6/100 | Step 15001/20074 | Loss: 1.3247\n",
      "Epoch 6/100 | Step 15101/20074 | Loss: 3.8156\n",
      "Epoch 6/100 | Step 15201/20074 | Loss: 1.0079\n",
      "Epoch 6/100 | Step 15301/20074 | Loss: 2.5979\n",
      "Epoch 6/100 | Step 15401/20074 | Loss: 0.0018\n",
      "Epoch 6/100 | Step 15501/20074 | Loss: 0.0654\n",
      "Epoch 6/100 | Step 15601/20074 | Loss: 0.4082\n",
      "Epoch 6/100 | Step 15701/20074 | Loss: 0.0004\n",
      "Epoch 6/100 | Step 15801/20074 | Loss: 0.2271\n",
      "Epoch 6/100 | Step 15901/20074 | Loss: 0.5371\n",
      "Epoch 6/100 | Step 16001/20074 | Loss: 0.0199\n",
      "Epoch 6/100 | Step 16101/20074 | Loss: 0.2303\n",
      "Epoch 6/100 | Step 16201/20074 | Loss: 0.1475\n",
      "Epoch 6/100 | Step 16301/20074 | Loss: 0.0850\n",
      "Epoch 6/100 | Step 16401/20074 | Loss: 1.3538\n",
      "Epoch 6/100 | Step 16501/20074 | Loss: 0.0090\n",
      "Epoch 6/100 | Step 16601/20074 | Loss: 1.1490\n",
      "Epoch 6/100 | Step 16701/20074 | Loss: 0.1206\n",
      "Epoch 6/100 | Step 16801/20074 | Loss: 1.5844\n",
      "Epoch 6/100 | Step 16901/20074 | Loss: 0.9897\n",
      "Epoch 6/100 | Step 17001/20074 | Loss: 0.0091\n",
      "Epoch 6/100 | Step 17101/20074 | Loss: 0.0004\n",
      "Epoch 6/100 | Step 17201/20074 | Loss: 0.0189\n",
      "Epoch 6/100 | Step 17301/20074 | Loss: 3.3358\n",
      "Epoch 6/100 | Step 17401/20074 | Loss: 0.2754\n",
      "Epoch 6/100 | Step 17501/20074 | Loss: 1.3208\n",
      "Epoch 6/100 | Step 17601/20074 | Loss: 0.7776\n",
      "Epoch 6/100 | Step 17701/20074 | Loss: 0.1097\n",
      "Epoch 6/100 | Step 17801/20074 | Loss: 0.1505\n",
      "Epoch 6/100 | Step 17901/20074 | Loss: 0.0048\n",
      "Epoch 6/100 | Step 18001/20074 | Loss: 0.0805\n",
      "Epoch 6/100 | Step 18101/20074 | Loss: 1.0546\n",
      "Epoch 6/100 | Step 18201/20074 | Loss: 0.7364\n",
      "Epoch 6/100 | Step 18301/20074 | Loss: 0.0854\n",
      "Epoch 6/100 | Step 18401/20074 | Loss: 0.5872\n",
      "Epoch 6/100 | Step 18501/20074 | Loss: 0.0020\n",
      "Epoch 6/100 | Step 18601/20074 | Loss: 0.0428\n",
      "Epoch 6/100 | Step 18701/20074 | Loss: 1.7811\n",
      "Epoch 6/100 | Step 18801/20074 | Loss: 0.3915\n",
      "Epoch 6/100 | Step 18901/20074 | Loss: 2.6537\n",
      "Epoch 6/100 | Step 19001/20074 | Loss: 1.8594\n",
      "Epoch 6/100 | Step 19101/20074 | Loss: 0.1050\n",
      "Epoch 6/100 | Step 19201/20074 | Loss: 0.2707\n",
      "Epoch 6/100 | Step 19301/20074 | Loss: 0.4353\n",
      "Epoch 6/100 | Step 19401/20074 | Loss: 0.2991\n",
      "Epoch 6/100 | Step 19501/20074 | Loss: 3.1739\n",
      "Epoch 6/100 | Step 19601/20074 | Loss: 3.0474\n",
      "Epoch 6/100 | Step 19701/20074 | Loss: 0.0022\n",
      "Epoch 6/100 | Step 19801/20074 | Loss: 0.3942\n",
      "Epoch 6/100 | Step 19901/20074 | Loss: 0.4605\n",
      "Epoch 6/100 | Step 20001/20074 | Loss: 2.2569\n",
      "Epoch 7/100 | Step 1/20074 | Loss: 2.0696\n",
      "Epoch 7/100 | Step 101/20074 | Loss: 0.7874\n",
      "Epoch 7/100 | Step 201/20074 | Loss: 0.0025\n",
      "Epoch 7/100 | Step 301/20074 | Loss: 0.5543\n",
      "Epoch 7/100 | Step 401/20074 | Loss: 0.4649\n",
      "Epoch 7/100 | Step 501/20074 | Loss: 4.0856\n",
      "Epoch 7/100 | Step 601/20074 | Loss: 0.0229\n",
      "Epoch 7/100 | Step 701/20074 | Loss: 0.0061\n",
      "Epoch 7/100 | Step 801/20074 | Loss: 0.0017\n",
      "Epoch 7/100 | Step 901/20074 | Loss: 1.6165\n",
      "Epoch 7/100 | Step 1001/20074 | Loss: 0.6606\n",
      "Epoch 7/100 | Step 1101/20074 | Loss: 0.1149\n",
      "Epoch 7/100 | Step 1201/20074 | Loss: 0.1319\n",
      "Epoch 7/100 | Step 1301/20074 | Loss: 0.4927\n",
      "Epoch 7/100 | Step 1401/20074 | Loss: 0.0101\n",
      "Epoch 7/100 | Step 1501/20074 | Loss: 0.0025\n",
      "Epoch 7/100 | Step 1601/20074 | Loss: 0.0645\n",
      "Epoch 7/100 | Step 1701/20074 | Loss: 0.0420\n",
      "Epoch 7/100 | Step 1801/20074 | Loss: 2.0642\n",
      "Epoch 7/100 | Step 1901/20074 | Loss: 2.8796\n",
      "Epoch 7/100 | Step 2001/20074 | Loss: 1.7308\n",
      "Epoch 7/100 | Step 2101/20074 | Loss: 0.8355\n",
      "Epoch 7/100 | Step 2201/20074 | Loss: 0.8649\n",
      "Epoch 7/100 | Step 2301/20074 | Loss: 0.0027\n",
      "Epoch 7/100 | Step 2401/20074 | Loss: 0.0028\n",
      "Epoch 7/100 | Step 2501/20074 | Loss: 0.0022\n",
      "Epoch 7/100 | Step 2601/20074 | Loss: 1.2087\n",
      "Epoch 7/100 | Step 2701/20074 | Loss: 2.9560\n",
      "Epoch 7/100 | Step 2801/20074 | Loss: 0.0292\n",
      "Epoch 7/100 | Step 2901/20074 | Loss: 0.1868\n",
      "Epoch 7/100 | Step 3001/20074 | Loss: 1.7399\n",
      "Epoch 7/100 | Step 3101/20074 | Loss: 0.0510\n",
      "Epoch 7/100 | Step 3201/20074 | Loss: 0.1444\n",
      "Epoch 7/100 | Step 3301/20074 | Loss: 2.3785\n",
      "Epoch 7/100 | Step 3401/20074 | Loss: 0.0096\n",
      "Epoch 7/100 | Step 3501/20074 | Loss: 0.0290\n",
      "Epoch 7/100 | Step 3601/20074 | Loss: 2.9880\n",
      "Epoch 7/100 | Step 3701/20074 | Loss: 0.1988\n",
      "Epoch 7/100 | Step 3801/20074 | Loss: 0.9580\n",
      "Epoch 7/100 | Step 3901/20074 | Loss: 0.8473\n",
      "Epoch 7/100 | Step 4001/20074 | Loss: 0.0049\n",
      "Epoch 7/100 | Step 4101/20074 | Loss: 0.1679\n",
      "Epoch 7/100 | Step 4201/20074 | Loss: 0.6293\n",
      "Epoch 7/100 | Step 4301/20074 | Loss: 0.0003\n",
      "Epoch 7/100 | Step 4401/20074 | Loss: 0.0003\n",
      "Epoch 7/100 | Step 4501/20074 | Loss: 0.5117\n",
      "Epoch 7/100 | Step 4601/20074 | Loss: 1.8125\n",
      "Epoch 7/100 | Step 4701/20074 | Loss: 3.7461\n",
      "Epoch 7/100 | Step 4801/20074 | Loss: 0.4539\n",
      "Epoch 7/100 | Step 4901/20074 | Loss: 0.2778\n",
      "Epoch 7/100 | Step 5001/20074 | Loss: 0.7942\n",
      "Epoch 7/100 | Step 5101/20074 | Loss: 2.1708\n",
      "Epoch 7/100 | Step 5201/20074 | Loss: 0.2732\n",
      "Epoch 7/100 | Step 5301/20074 | Loss: 0.0693\n",
      "Epoch 7/100 | Step 5401/20074 | Loss: 0.4227\n",
      "Epoch 7/100 | Step 5501/20074 | Loss: 0.0192\n",
      "Epoch 7/100 | Step 5601/20074 | Loss: 2.2647\n",
      "Epoch 7/100 | Step 5701/20074 | Loss: 0.1328\n",
      "Epoch 7/100 | Step 5801/20074 | Loss: 0.3958\n",
      "Epoch 7/100 | Step 5901/20074 | Loss: 1.9729\n",
      "Epoch 7/100 | Step 6001/20074 | Loss: 0.0258\n",
      "Epoch 7/100 | Step 6101/20074 | Loss: 0.0281\n",
      "Epoch 7/100 | Step 6201/20074 | Loss: 0.1212\n",
      "Epoch 7/100 | Step 6301/20074 | Loss: 0.3349\n",
      "Epoch 7/100 | Step 6401/20074 | Loss: 0.0427\n",
      "Epoch 7/100 | Step 6501/20074 | Loss: 0.1116\n",
      "Epoch 7/100 | Step 6601/20074 | Loss: 2.7456\n",
      "Epoch 7/100 | Step 6701/20074 | Loss: 0.0110\n",
      "Epoch 7/100 | Step 6801/20074 | Loss: 0.2047\n",
      "Epoch 7/100 | Step 6901/20074 | Loss: 0.0184\n",
      "Epoch 7/100 | Step 7001/20074 | Loss: 0.3737\n",
      "Epoch 7/100 | Step 7101/20074 | Loss: 0.1843\n",
      "Epoch 7/100 | Step 7201/20074 | Loss: 0.3388\n",
      "Epoch 7/100 | Step 7301/20074 | Loss: 1.2103\n",
      "Epoch 7/100 | Step 7401/20074 | Loss: 0.3859\n",
      "Epoch 7/100 | Step 7501/20074 | Loss: 0.0586\n",
      "Epoch 7/100 | Step 7601/20074 | Loss: 0.0211\n",
      "Epoch 7/100 | Step 7701/20074 | Loss: 0.2888\n",
      "Epoch 7/100 | Step 7801/20074 | Loss: 0.0400\n",
      "Epoch 7/100 | Step 7901/20074 | Loss: 3.4349\n",
      "Epoch 7/100 | Step 8001/20074 | Loss: 0.0009\n",
      "Epoch 7/100 | Step 8101/20074 | Loss: 0.0377\n",
      "Epoch 7/100 | Step 8201/20074 | Loss: 1.4857\n",
      "Epoch 7/100 | Step 8301/20074 | Loss: 0.2671\n",
      "Epoch 7/100 | Step 8401/20074 | Loss: 0.2403\n",
      "Epoch 7/100 | Step 8501/20074 | Loss: 0.0789\n",
      "Epoch 7/100 | Step 8601/20074 | Loss: 1.4449\n",
      "Epoch 7/100 | Step 8701/20074 | Loss: 0.4716\n",
      "Epoch 7/100 | Step 8801/20074 | Loss: 0.9202\n",
      "Epoch 7/100 | Step 8901/20074 | Loss: 0.1953\n",
      "Epoch 7/100 | Step 9001/20074 | Loss: 0.0611\n",
      "Epoch 7/100 | Step 9101/20074 | Loss: 0.9124\n",
      "Epoch 7/100 | Step 9201/20074 | Loss: 2.9957\n",
      "Epoch 7/100 | Step 9301/20074 | Loss: 1.2448\n",
      "Epoch 7/100 | Step 9401/20074 | Loss: 0.0004\n",
      "Epoch 7/100 | Step 9501/20074 | Loss: 0.0466\n",
      "Epoch 7/100 | Step 9601/20074 | Loss: 0.9416\n",
      "Epoch 7/100 | Step 9701/20074 | Loss: 2.2683\n",
      "Epoch 7/100 | Step 9801/20074 | Loss: 0.0098\n",
      "Epoch 7/100 | Step 9901/20074 | Loss: 0.9103\n",
      "Epoch 7/100 | Step 10001/20074 | Loss: 0.6164\n",
      "Epoch 7/100 | Step 10101/20074 | Loss: 0.1212\n",
      "Epoch 7/100 | Step 10201/20074 | Loss: 0.9999\n",
      "Epoch 7/100 | Step 10301/20074 | Loss: 0.4562\n",
      "Epoch 7/100 | Step 10401/20074 | Loss: 0.4137\n",
      "Epoch 7/100 | Step 10501/20074 | Loss: 0.3538\n",
      "Epoch 7/100 | Step 10601/20074 | Loss: 3.4102\n",
      "Epoch 7/100 | Step 10701/20074 | Loss: 1.0148\n",
      "Epoch 7/100 | Step 10801/20074 | Loss: 6.7057\n",
      "Epoch 7/100 | Step 10901/20074 | Loss: 0.2284\n",
      "Epoch 7/100 | Step 11001/20074 | Loss: 2.1577\n",
      "Epoch 7/100 | Step 11101/20074 | Loss: 0.8397\n",
      "Epoch 7/100 | Step 11201/20074 | Loss: 0.5269\n",
      "Epoch 7/100 | Step 11301/20074 | Loss: 0.0182\n",
      "Epoch 7/100 | Step 11401/20074 | Loss: 0.2166\n",
      "Epoch 7/100 | Step 11501/20074 | Loss: 0.0054\n",
      "Epoch 7/100 | Step 11601/20074 | Loss: 0.0443\n",
      "Epoch 7/100 | Step 11701/20074 | Loss: 0.0019\n",
      "Epoch 7/100 | Step 11801/20074 | Loss: 0.0356\n",
      "Epoch 7/100 | Step 11901/20074 | Loss: 0.5408\n",
      "Epoch 7/100 | Step 12001/20074 | Loss: 3.6922\n",
      "Epoch 7/100 | Step 12101/20074 | Loss: 0.0199\n",
      "Epoch 7/100 | Step 12201/20074 | Loss: 0.4794\n",
      "Epoch 7/100 | Step 12301/20074 | Loss: 2.9337\n",
      "Epoch 7/100 | Step 12401/20074 | Loss: 0.3859\n",
      "Epoch 7/100 | Step 12501/20074 | Loss: 0.0333\n",
      "Epoch 7/100 | Step 12601/20074 | Loss: 0.0000\n",
      "Epoch 7/100 | Step 12701/20074 | Loss: 0.3752\n",
      "Epoch 7/100 | Step 12801/20074 | Loss: 0.3539\n",
      "Epoch 7/100 | Step 12901/20074 | Loss: 2.7885\n",
      "Epoch 7/100 | Step 13001/20074 | Loss: 4.1668\n",
      "Epoch 7/100 | Step 13101/20074 | Loss: 1.4514\n",
      "Epoch 7/100 | Step 13201/20074 | Loss: 0.1528\n",
      "Epoch 7/100 | Step 13301/20074 | Loss: 0.0792\n",
      "Epoch 7/100 | Step 13401/20074 | Loss: 0.5006\n",
      "Epoch 7/100 | Step 13501/20074 | Loss: 0.0549\n",
      "Epoch 7/100 | Step 13601/20074 | Loss: 0.0516\n",
      "Epoch 7/100 | Step 13701/20074 | Loss: 0.4520\n",
      "Epoch 7/100 | Step 13801/20074 | Loss: 1.8853\n",
      "Epoch 7/100 | Step 13901/20074 | Loss: 0.0005\n",
      "Epoch 7/100 | Step 14001/20074 | Loss: 5.4301\n",
      "Epoch 7/100 | Step 14101/20074 | Loss: 2.6143\n",
      "Epoch 7/100 | Step 14201/20074 | Loss: 0.0232\n",
      "Epoch 7/100 | Step 14301/20074 | Loss: 0.2964\n",
      "Epoch 7/100 | Step 14401/20074 | Loss: 0.1193\n",
      "Epoch 7/100 | Step 14501/20074 | Loss: 0.0202\n",
      "Epoch 7/100 | Step 14601/20074 | Loss: 0.0040\n",
      "Epoch 7/100 | Step 14701/20074 | Loss: 0.0206\n",
      "Epoch 7/100 | Step 14801/20074 | Loss: 1.3013\n",
      "Epoch 7/100 | Step 14901/20074 | Loss: 0.7211\n",
      "Epoch 7/100 | Step 15001/20074 | Loss: 0.3177\n",
      "Epoch 7/100 | Step 15101/20074 | Loss: 3.3221\n",
      "Epoch 7/100 | Step 15201/20074 | Loss: 0.3591\n",
      "Epoch 7/100 | Step 15301/20074 | Loss: 0.0153\n",
      "Epoch 7/100 | Step 15401/20074 | Loss: 0.6655\n",
      "Epoch 7/100 | Step 15501/20074 | Loss: 4.6606\n",
      "Epoch 7/100 | Step 15601/20074 | Loss: 0.0076\n",
      "Epoch 7/100 | Step 15701/20074 | Loss: 1.1885\n",
      "Epoch 7/100 | Step 15801/20074 | Loss: 0.5161\n",
      "Epoch 7/100 | Step 15901/20074 | Loss: 1.8428\n",
      "Epoch 7/100 | Step 16001/20074 | Loss: 0.6853\n",
      "Epoch 7/100 | Step 16101/20074 | Loss: 0.0048\n",
      "Epoch 7/100 | Step 16201/20074 | Loss: 0.0444\n",
      "Epoch 7/100 | Step 16301/20074 | Loss: 0.0008\n",
      "Epoch 7/100 | Step 16401/20074 | Loss: 0.7962\n",
      "Epoch 7/100 | Step 16501/20074 | Loss: 0.0426\n",
      "Epoch 7/100 | Step 16601/20074 | Loss: 0.1668\n",
      "Epoch 7/100 | Step 16701/20074 | Loss: 0.0006\n",
      "Epoch 7/100 | Step 16801/20074 | Loss: 1.4809\n",
      "Epoch 7/100 | Step 16901/20074 | Loss: 1.3280\n",
      "Epoch 7/100 | Step 17001/20074 | Loss: 0.2869\n",
      "Epoch 7/100 | Step 17101/20074 | Loss: 0.0015\n",
      "Epoch 7/100 | Step 17201/20074 | Loss: 1.3915\n",
      "Epoch 7/100 | Step 17301/20074 | Loss: 3.7195\n",
      "Epoch 7/100 | Step 17401/20074 | Loss: 0.0207\n",
      "Epoch 7/100 | Step 17501/20074 | Loss: 0.4914\n",
      "Epoch 7/100 | Step 17601/20074 | Loss: 1.0505\n",
      "Epoch 7/100 | Step 17701/20074 | Loss: 1.2121\n",
      "Epoch 7/100 | Step 17801/20074 | Loss: 2.6235\n",
      "Epoch 7/100 | Step 17901/20074 | Loss: 0.1123\n",
      "Epoch 7/100 | Step 18001/20074 | Loss: 0.4628\n",
      "Epoch 7/100 | Step 18101/20074 | Loss: 0.5232\n",
      "Epoch 7/100 | Step 18201/20074 | Loss: 0.1911\n",
      "Epoch 7/100 | Step 18301/20074 | Loss: 1.0640\n",
      "Epoch 7/100 | Step 18401/20074 | Loss: 0.0069\n",
      "Epoch 7/100 | Step 18501/20074 | Loss: 1.7415\n",
      "Epoch 7/100 | Step 18601/20074 | Loss: 0.0010\n",
      "Epoch 7/100 | Step 18701/20074 | Loss: 0.3200\n",
      "Epoch 7/100 | Step 18801/20074 | Loss: 0.0012\n",
      "Epoch 7/100 | Step 18901/20074 | Loss: 0.6025\n",
      "Epoch 7/100 | Step 19001/20074 | Loss: 0.0023\n",
      "Epoch 7/100 | Step 19101/20074 | Loss: 0.4878\n",
      "Epoch 7/100 | Step 19201/20074 | Loss: 0.0907\n",
      "Epoch 7/100 | Step 19301/20074 | Loss: 1.2729\n",
      "Epoch 7/100 | Step 19401/20074 | Loss: 1.8837\n",
      "Epoch 7/100 | Step 19501/20074 | Loss: 0.5184\n",
      "Epoch 7/100 | Step 19601/20074 | Loss: 0.2774\n",
      "Epoch 7/100 | Step 19701/20074 | Loss: 0.4550\n",
      "Epoch 7/100 | Step 19801/20074 | Loss: 0.1136\n",
      "Epoch 7/100 | Step 19901/20074 | Loss: 0.0013\n",
      "Epoch 7/100 | Step 20001/20074 | Loss: 0.0441\n",
      "Epoch 8/100 | Step 1/20074 | Loss: 0.0015\n",
      "Epoch 8/100 | Step 101/20074 | Loss: 0.6436\n",
      "Epoch 8/100 | Step 201/20074 | Loss: 0.4676\n",
      "Epoch 8/100 | Step 301/20074 | Loss: 1.6117\n",
      "Epoch 8/100 | Step 401/20074 | Loss: 0.2058\n",
      "Epoch 8/100 | Step 501/20074 | Loss: 0.0233\n",
      "Epoch 8/100 | Step 601/20074 | Loss: 0.7161\n",
      "Epoch 8/100 | Step 701/20074 | Loss: 0.1431\n",
      "Epoch 8/100 | Step 801/20074 | Loss: 2.0543\n",
      "Epoch 8/100 | Step 901/20074 | Loss: 0.0030\n",
      "Epoch 8/100 | Step 1001/20074 | Loss: 0.2300\n",
      "Epoch 8/100 | Step 1101/20074 | Loss: 2.7695\n",
      "Epoch 8/100 | Step 1201/20074 | Loss: 0.5194\n",
      "Epoch 8/100 | Step 1301/20074 | Loss: 0.2101\n",
      "Epoch 8/100 | Step 1401/20074 | Loss: 4.9579\n",
      "Epoch 8/100 | Step 1501/20074 | Loss: 0.0471\n",
      "Epoch 8/100 | Step 1601/20074 | Loss: 1.7295\n",
      "Epoch 8/100 | Step 1701/20074 | Loss: 0.0109\n",
      "Epoch 8/100 | Step 1801/20074 | Loss: 0.1687\n",
      "Epoch 8/100 | Step 1901/20074 | Loss: 0.0097\n",
      "Epoch 8/100 | Step 2001/20074 | Loss: 0.0760\n",
      "Epoch 8/100 | Step 2101/20074 | Loss: 0.3344\n",
      "Epoch 8/100 | Step 2201/20074 | Loss: 1.5313\n",
      "Epoch 8/100 | Step 2301/20074 | Loss: 2.0239\n",
      "Epoch 8/100 | Step 2401/20074 | Loss: 0.5679\n",
      "Epoch 8/100 | Step 2501/20074 | Loss: 0.0475\n",
      "Epoch 8/100 | Step 2601/20074 | Loss: 0.7916\n",
      "Epoch 8/100 | Step 2701/20074 | Loss: 1.8422\n",
      "Epoch 8/100 | Step 2801/20074 | Loss: 0.1038\n",
      "Epoch 8/100 | Step 2901/20074 | Loss: 3.2521\n",
      "Epoch 8/100 | Step 3001/20074 | Loss: 0.9189\n",
      "Epoch 8/100 | Step 3101/20074 | Loss: 0.1362\n",
      "Epoch 8/100 | Step 3201/20074 | Loss: 0.0391\n",
      "Epoch 8/100 | Step 3301/20074 | Loss: 0.3092\n",
      "Epoch 8/100 | Step 3401/20074 | Loss: 0.0379\n",
      "Epoch 8/100 | Step 3501/20074 | Loss: 1.7300\n",
      "Epoch 8/100 | Step 3601/20074 | Loss: 0.4361\n",
      "Epoch 8/100 | Step 3701/20074 | Loss: 0.3075\n",
      "Epoch 8/100 | Step 3801/20074 | Loss: 0.0202\n",
      "Epoch 8/100 | Step 3901/20074 | Loss: 1.5226\n",
      "Epoch 8/100 | Step 4001/20074 | Loss: 0.2326\n",
      "Epoch 8/100 | Step 4101/20074 | Loss: 0.3357\n",
      "Epoch 8/100 | Step 4201/20074 | Loss: 4.3827\n",
      "Epoch 8/100 | Step 4301/20074 | Loss: 0.0815\n",
      "Epoch 8/100 | Step 4401/20074 | Loss: 0.0010\n",
      "Epoch 8/100 | Step 4501/20074 | Loss: 0.0215\n",
      "Epoch 8/100 | Step 4601/20074 | Loss: 1.6600\n",
      "Epoch 8/100 | Step 4701/20074 | Loss: 0.0006\n",
      "Epoch 8/100 | Step 4801/20074 | Loss: 0.3156\n",
      "Epoch 8/100 | Step 4901/20074 | Loss: 1.2279\n",
      "Epoch 8/100 | Step 5001/20074 | Loss: 0.8201\n",
      "Epoch 8/100 | Step 5101/20074 | Loss: 0.4663\n",
      "Epoch 8/100 | Step 5201/20074 | Loss: 0.0070\n",
      "Epoch 8/100 | Step 5301/20074 | Loss: 0.0004\n",
      "Epoch 8/100 | Step 5401/20074 | Loss: 1.5025\n",
      "Epoch 8/100 | Step 5501/20074 | Loss: 0.0957\n",
      "Epoch 8/100 | Step 5601/20074 | Loss: 0.6475\n",
      "Epoch 8/100 | Step 5701/20074 | Loss: 1.0496\n",
      "Epoch 8/100 | Step 5801/20074 | Loss: 0.0013\n",
      "Epoch 8/100 | Step 5901/20074 | Loss: 2.3838\n",
      "Epoch 8/100 | Step 6001/20074 | Loss: 0.6415\n",
      "Epoch 8/100 | Step 6101/20074 | Loss: 0.0011\n",
      "Epoch 8/100 | Step 6201/20074 | Loss: 0.0913\n",
      "Epoch 8/100 | Step 6301/20074 | Loss: 4.2602\n",
      "Epoch 8/100 | Step 6401/20074 | Loss: 0.0485\n",
      "Epoch 8/100 | Step 6501/20074 | Loss: 0.7701\n",
      "Epoch 8/100 | Step 6601/20074 | Loss: 3.2383\n",
      "Epoch 8/100 | Step 6701/20074 | Loss: 0.1359\n",
      "Epoch 8/100 | Step 6801/20074 | Loss: 0.1131\n",
      "Epoch 8/100 | Step 6901/20074 | Loss: 2.0874\n",
      "Epoch 8/100 | Step 7001/20074 | Loss: 0.7022\n",
      "Epoch 8/100 | Step 7101/20074 | Loss: 0.9663\n",
      "Epoch 8/100 | Step 7201/20074 | Loss: 0.0023\n",
      "Epoch 8/100 | Step 7301/20074 | Loss: 0.2609\n",
      "Epoch 8/100 | Step 7401/20074 | Loss: 0.2746\n",
      "Epoch 8/100 | Step 7501/20074 | Loss: 2.5329\n",
      "Epoch 8/100 | Step 7601/20074 | Loss: 0.1894\n",
      "Epoch 8/100 | Step 7701/20074 | Loss: 0.0058\n",
      "Epoch 8/100 | Step 7801/20074 | Loss: 0.0030\n",
      "Epoch 8/100 | Step 7901/20074 | Loss: 0.6386\n",
      "Epoch 8/100 | Step 8001/20074 | Loss: 2.1860\n",
      "Epoch 8/100 | Step 8101/20074 | Loss: 0.0055\n",
      "Epoch 8/100 | Step 8201/20074 | Loss: 0.2715\n",
      "Epoch 8/100 | Step 8301/20074 | Loss: 5.2860\n",
      "Epoch 8/100 | Step 8401/20074 | Loss: 0.0822\n",
      "Epoch 8/100 | Step 8501/20074 | Loss: 0.3758\n",
      "Epoch 8/100 | Step 8601/20074 | Loss: 0.0074\n",
      "Epoch 8/100 | Step 8701/20074 | Loss: 1.8917\n",
      "Epoch 8/100 | Step 8801/20074 | Loss: 0.4109\n",
      "Epoch 8/100 | Step 8901/20074 | Loss: 0.0188\n",
      "Epoch 8/100 | Step 9001/20074 | Loss: 0.0911\n",
      "Epoch 8/100 | Step 9101/20074 | Loss: 0.0225\n",
      "Epoch 8/100 | Step 9201/20074 | Loss: 0.1063\n",
      "Epoch 8/100 | Step 9301/20074 | Loss: 0.5169\n",
      "Epoch 8/100 | Step 9401/20074 | Loss: 0.1297\n",
      "Epoch 8/100 | Step 9501/20074 | Loss: 3.9347\n",
      "Epoch 8/100 | Step 9601/20074 | Loss: 2.5771\n",
      "Epoch 8/100 | Step 9701/20074 | Loss: 0.0077\n",
      "Epoch 8/100 | Step 9801/20074 | Loss: 0.7576\n",
      "Epoch 8/100 | Step 9901/20074 | Loss: 1.0229\n",
      "Epoch 8/100 | Step 10001/20074 | Loss: 1.1853\n",
      "Epoch 8/100 | Step 10101/20074 | Loss: 2.5179\n",
      "Epoch 8/100 | Step 10201/20074 | Loss: 0.1701\n",
      "Epoch 8/100 | Step 10301/20074 | Loss: 0.3772\n",
      "Epoch 8/100 | Step 10401/20074 | Loss: 4.6240\n",
      "Epoch 8/100 | Step 10501/20074 | Loss: 0.0002\n",
      "Epoch 8/100 | Step 10601/20074 | Loss: 0.6771\n",
      "Epoch 8/100 | Step 10701/20074 | Loss: 2.4583\n",
      "Epoch 8/100 | Step 10801/20074 | Loss: 2.3508\n",
      "Epoch 8/100 | Step 10901/20074 | Loss: 0.5086\n",
      "Epoch 8/100 | Step 11001/20074 | Loss: 3.3879\n",
      "Epoch 8/100 | Step 11101/20074 | Loss: 0.0270\n",
      "Epoch 8/100 | Step 11201/20074 | Loss: 0.8069\n",
      "Epoch 8/100 | Step 11301/20074 | Loss: 0.0091\n",
      "Epoch 8/100 | Step 11401/20074 | Loss: 0.4738\n",
      "Epoch 8/100 | Step 11501/20074 | Loss: 0.0016\n",
      "Epoch 8/100 | Step 11601/20074 | Loss: 0.0029\n",
      "Epoch 8/100 | Step 11701/20074 | Loss: 0.0792\n",
      "Epoch 8/100 | Step 11801/20074 | Loss: 2.0917\n",
      "Epoch 8/100 | Step 11901/20074 | Loss: 2.4642\n",
      "Epoch 8/100 | Step 12001/20074 | Loss: 0.3323\n",
      "Epoch 8/100 | Step 12101/20074 | Loss: 1.8666\n",
      "Epoch 8/100 | Step 12201/20074 | Loss: 1.7122\n",
      "Epoch 8/100 | Step 12301/20074 | Loss: 0.2607\n",
      "Epoch 8/100 | Step 12401/20074 | Loss: 1.1890\n",
      "Epoch 8/100 | Step 12501/20074 | Loss: 0.7308\n",
      "Epoch 8/100 | Step 12601/20074 | Loss: 0.0045\n",
      "Epoch 8/100 | Step 12701/20074 | Loss: 0.0009\n",
      "Epoch 8/100 | Step 12801/20074 | Loss: 0.0012\n",
      "Epoch 8/100 | Step 12901/20074 | Loss: 0.0091\n",
      "Epoch 8/100 | Step 13001/20074 | Loss: 1.3138\n",
      "Epoch 8/100 | Step 13101/20074 | Loss: 0.5845\n",
      "Epoch 8/100 | Step 13201/20074 | Loss: 0.5153\n",
      "Epoch 8/100 | Step 13301/20074 | Loss: 0.1133\n",
      "Epoch 8/100 | Step 13401/20074 | Loss: 2.4005\n",
      "Epoch 8/100 | Step 13501/20074 | Loss: 0.1060\n",
      "Epoch 8/100 | Step 13601/20074 | Loss: 1.3981\n",
      "Epoch 8/100 | Step 13701/20074 | Loss: 0.0033\n",
      "Epoch 8/100 | Step 13801/20074 | Loss: 1.0079\n",
      "Epoch 8/100 | Step 13901/20074 | Loss: 0.4864\n",
      "Epoch 8/100 | Step 14001/20074 | Loss: 0.3652\n",
      "Epoch 8/100 | Step 14101/20074 | Loss: 0.0100\n",
      "Epoch 8/100 | Step 14201/20074 | Loss: 0.2352\n",
      "Epoch 8/100 | Step 14301/20074 | Loss: 1.0510\n",
      "Epoch 8/100 | Step 14401/20074 | Loss: 0.0002\n",
      "Epoch 8/100 | Step 14501/20074 | Loss: 0.8841\n",
      "Epoch 8/100 | Step 14601/20074 | Loss: 0.0002\n",
      "Epoch 8/100 | Step 14701/20074 | Loss: 0.0023\n",
      "Epoch 8/100 | Step 14801/20074 | Loss: 0.2842\n",
      "Epoch 8/100 | Step 14901/20074 | Loss: 0.1002\n",
      "Epoch 8/100 | Step 15001/20074 | Loss: 0.0885\n",
      "Epoch 8/100 | Step 15101/20074 | Loss: 0.1883\n",
      "Epoch 8/100 | Step 15201/20074 | Loss: 0.0556\n",
      "Epoch 8/100 | Step 15301/20074 | Loss: 0.0028\n",
      "Epoch 8/100 | Step 15401/20074 | Loss: 0.2076\n",
      "Epoch 8/100 | Step 15501/20074 | Loss: 3.2813\n",
      "Epoch 8/100 | Step 15601/20074 | Loss: 0.0042\n",
      "Epoch 8/100 | Step 15701/20074 | Loss: 0.0804\n",
      "Epoch 8/100 | Step 15801/20074 | Loss: 0.0040\n",
      "Epoch 8/100 | Step 15901/20074 | Loss: 0.5363\n",
      "Epoch 8/100 | Step 16001/20074 | Loss: 0.1342\n",
      "Epoch 8/100 | Step 16101/20074 | Loss: 3.8587\n",
      "Epoch 8/100 | Step 16201/20074 | Loss: 0.0044\n",
      "Epoch 8/100 | Step 16301/20074 | Loss: 0.0042\n",
      "Epoch 8/100 | Step 16401/20074 | Loss: 0.4745\n",
      "Epoch 8/100 | Step 16501/20074 | Loss: 0.0010\n",
      "Epoch 8/100 | Step 16601/20074 | Loss: 0.0560\n",
      "Epoch 8/100 | Step 16701/20074 | Loss: 1.1342\n",
      "Epoch 8/100 | Step 16801/20074 | Loss: 1.3340\n",
      "Epoch 8/100 | Step 16901/20074 | Loss: 0.7153\n",
      "Epoch 8/100 | Step 17001/20074 | Loss: 0.8405\n",
      "Epoch 8/100 | Step 17101/20074 | Loss: 0.0090\n",
      "Epoch 8/100 | Step 17201/20074 | Loss: 1.0190\n",
      "Epoch 8/100 | Step 17301/20074 | Loss: 0.0005\n",
      "Epoch 8/100 | Step 17401/20074 | Loss: 0.0066\n",
      "Epoch 8/100 | Step 17501/20074 | Loss: 1.3503\n",
      "Epoch 8/100 | Step 17601/20074 | Loss: 0.0020\n",
      "Epoch 8/100 | Step 17701/20074 | Loss: 0.9182\n",
      "Epoch 8/100 | Step 17801/20074 | Loss: 0.3048\n",
      "Epoch 8/100 | Step 17901/20074 | Loss: 0.9427\n",
      "Epoch 8/100 | Step 18001/20074 | Loss: 0.1532\n",
      "Epoch 8/100 | Step 18101/20074 | Loss: 0.1697\n",
      "Epoch 8/100 | Step 18201/20074 | Loss: 0.0144\n",
      "Epoch 8/100 | Step 18301/20074 | Loss: 0.0079\n",
      "Epoch 8/100 | Step 18401/20074 | Loss: 1.0949\n",
      "Epoch 8/100 | Step 18501/20074 | Loss: 3.5235\n",
      "Epoch 8/100 | Step 18601/20074 | Loss: 3.8067\n",
      "Epoch 8/100 | Step 18701/20074 | Loss: 0.0004\n",
      "Epoch 8/100 | Step 18801/20074 | Loss: 0.6129\n",
      "Epoch 8/100 | Step 18901/20074 | Loss: 0.0002\n",
      "Epoch 8/100 | Step 19001/20074 | Loss: 0.0016\n",
      "Epoch 8/100 | Step 19101/20074 | Loss: 0.9873\n",
      "Epoch 8/100 | Step 19201/20074 | Loss: 4.2622\n",
      "Epoch 8/100 | Step 19301/20074 | Loss: 0.3680\n",
      "Epoch 8/100 | Step 19401/20074 | Loss: 0.4659\n",
      "Epoch 8/100 | Step 19501/20074 | Loss: 3.8240\n",
      "Epoch 8/100 | Step 19601/20074 | Loss: 0.5507\n",
      "Epoch 8/100 | Step 19701/20074 | Loss: 0.0750\n",
      "Epoch 8/100 | Step 19801/20074 | Loss: 0.4900\n",
      "Epoch 8/100 | Step 19901/20074 | Loss: 0.0117\n",
      "Epoch 8/100 | Step 20001/20074 | Loss: 0.0678\n",
      "Epoch 9/100 | Step 1/20074 | Loss: 0.0002\n",
      "Epoch 9/100 | Step 101/20074 | Loss: 0.0012\n",
      "Epoch 9/100 | Step 201/20074 | Loss: 0.0039\n",
      "Epoch 9/100 | Step 301/20074 | Loss: 1.3734\n",
      "Epoch 9/100 | Step 401/20074 | Loss: 0.1358\n",
      "Epoch 9/100 | Step 501/20074 | Loss: 0.3296\n",
      "Epoch 9/100 | Step 601/20074 | Loss: 0.0036\n",
      "Epoch 9/100 | Step 701/20074 | Loss: 0.8308\n",
      "Epoch 9/100 | Step 801/20074 | Loss: 0.0248\n",
      "Epoch 9/100 | Step 901/20074 | Loss: 3.7563\n",
      "Epoch 9/100 | Step 1001/20074 | Loss: 0.0023\n",
      "Epoch 9/100 | Step 1101/20074 | Loss: 0.0491\n",
      "Epoch 9/100 | Step 1201/20074 | Loss: 2.8600\n",
      "Epoch 9/100 | Step 1301/20074 | Loss: 3.4185\n",
      "Epoch 9/100 | Step 1401/20074 | Loss: 0.0095\n",
      "Epoch 9/100 | Step 1501/20074 | Loss: 1.0796\n",
      "Epoch 9/100 | Step 1601/20074 | Loss: 1.1238\n",
      "Epoch 9/100 | Step 1701/20074 | Loss: 0.0800\n",
      "Epoch 9/100 | Step 1801/20074 | Loss: 0.2030\n",
      "Epoch 9/100 | Step 1901/20074 | Loss: 0.0464\n",
      "Epoch 9/100 | Step 2001/20074 | Loss: 3.6710\n",
      "Epoch 9/100 | Step 2101/20074 | Loss: 0.2305\n",
      "Epoch 9/100 | Step 2201/20074 | Loss: 1.2036\n",
      "Epoch 9/100 | Step 2301/20074 | Loss: 2.6861\n",
      "Epoch 9/100 | Step 2401/20074 | Loss: 0.0785\n",
      "Epoch 9/100 | Step 2501/20074 | Loss: 1.8876\n",
      "Epoch 9/100 | Step 2601/20074 | Loss: 0.6314\n",
      "Epoch 9/100 | Step 2701/20074 | Loss: 1.1958\n",
      "Epoch 9/100 | Step 2801/20074 | Loss: 0.3200\n",
      "Epoch 9/100 | Step 2901/20074 | Loss: 0.2949\n",
      "Epoch 9/100 | Step 3001/20074 | Loss: 1.3349\n",
      "Epoch 9/100 | Step 3101/20074 | Loss: 0.3241\n",
      "Epoch 9/100 | Step 3201/20074 | Loss: 1.8083\n",
      "Epoch 9/100 | Step 3301/20074 | Loss: 0.0021\n",
      "Epoch 9/100 | Step 3401/20074 | Loss: 1.8463\n",
      "Epoch 9/100 | Step 3501/20074 | Loss: 0.4493\n",
      "Epoch 9/100 | Step 3601/20074 | Loss: 3.0576\n",
      "Epoch 9/100 | Step 3701/20074 | Loss: 0.6828\n",
      "Epoch 9/100 | Step 3801/20074 | Loss: 0.1172\n",
      "Epoch 9/100 | Step 3901/20074 | Loss: 0.1844\n",
      "Epoch 9/100 | Step 4001/20074 | Loss: 0.0022\n",
      "Epoch 9/100 | Step 4101/20074 | Loss: 1.1769\n",
      "Epoch 9/100 | Step 4201/20074 | Loss: 0.3776\n",
      "Epoch 9/100 | Step 4301/20074 | Loss: 0.0033\n",
      "Epoch 9/100 | Step 4401/20074 | Loss: 0.0927\n",
      "Epoch 9/100 | Step 4501/20074 | Loss: 0.0071\n",
      "Epoch 9/100 | Step 4601/20074 | Loss: 0.1031\n",
      "Epoch 9/100 | Step 4701/20074 | Loss: 0.9489\n",
      "Epoch 9/100 | Step 4801/20074 | Loss: 0.0697\n",
      "Epoch 9/100 | Step 4901/20074 | Loss: 4.3268\n",
      "Epoch 9/100 | Step 5001/20074 | Loss: 0.0105\n",
      "Epoch 9/100 | Step 5101/20074 | Loss: 0.3878\n",
      "Epoch 9/100 | Step 5201/20074 | Loss: 3.8714\n",
      "Epoch 9/100 | Step 5301/20074 | Loss: 1.1012\n",
      "Epoch 9/100 | Step 5401/20074 | Loss: 1.9924\n",
      "Epoch 9/100 | Step 5501/20074 | Loss: 1.7422\n",
      "Epoch 9/100 | Step 5601/20074 | Loss: 0.0168\n",
      "Epoch 9/100 | Step 5701/20074 | Loss: 0.0382\n",
      "Epoch 9/100 | Step 5801/20074 | Loss: 0.8406\n",
      "Epoch 9/100 | Step 5901/20074 | Loss: 0.7161\n",
      "Epoch 9/100 | Step 6001/20074 | Loss: 0.5465\n",
      "Epoch 9/100 | Step 6101/20074 | Loss: 0.0118\n",
      "Epoch 9/100 | Step 6201/20074 | Loss: 0.0025\n",
      "Epoch 9/100 | Step 6301/20074 | Loss: 0.0062\n",
      "Epoch 9/100 | Step 6401/20074 | Loss: 0.0138\n",
      "Epoch 9/100 | Step 6501/20074 | Loss: 0.0108\n",
      "Epoch 9/100 | Step 6601/20074 | Loss: 0.0713\n",
      "Epoch 9/100 | Step 6701/20074 | Loss: 0.0002\n",
      "Epoch 9/100 | Step 6801/20074 | Loss: 0.7630\n",
      "Epoch 9/100 | Step 6901/20074 | Loss: 2.2681\n",
      "Epoch 9/100 | Step 7001/20074 | Loss: 4.3010\n",
      "Epoch 9/100 | Step 7101/20074 | Loss: 0.0822\n",
      "Epoch 9/100 | Step 7201/20074 | Loss: 0.7805\n",
      "Epoch 9/100 | Step 7301/20074 | Loss: 3.9874\n",
      "Epoch 9/100 | Step 7401/20074 | Loss: 0.0099\n",
      "Epoch 9/100 | Step 7501/20074 | Loss: 1.7736\n",
      "Epoch 9/100 | Step 7601/20074 | Loss: 0.1644\n",
      "Epoch 9/100 | Step 7701/20074 | Loss: 0.1190\n",
      "Epoch 9/100 | Step 7801/20074 | Loss: 0.6801\n",
      "Epoch 9/100 | Step 7901/20074 | Loss: 0.0004\n",
      "Epoch 9/100 | Step 8001/20074 | Loss: 0.1075\n",
      "Epoch 9/100 | Step 8101/20074 | Loss: 0.0167\n",
      "Epoch 9/100 | Step 8201/20074 | Loss: 0.0018\n",
      "Epoch 9/100 | Step 8301/20074 | Loss: 1.9669\n",
      "Epoch 9/100 | Step 8401/20074 | Loss: 0.1023\n",
      "Epoch 9/100 | Step 8501/20074 | Loss: 0.1377\n",
      "Epoch 9/100 | Step 8601/20074 | Loss: 0.4936\n",
      "Epoch 9/100 | Step 8701/20074 | Loss: 0.2584\n",
      "Epoch 9/100 | Step 8801/20074 | Loss: 0.5140\n",
      "Epoch 9/100 | Step 8901/20074 | Loss: 0.0085\n",
      "Epoch 9/100 | Step 9001/20074 | Loss: 2.3544\n",
      "Epoch 9/100 | Step 9101/20074 | Loss: 0.4921\n",
      "Epoch 9/100 | Step 9201/20074 | Loss: 0.0101\n",
      "Epoch 9/100 | Step 9301/20074 | Loss: 0.0003\n",
      "Epoch 9/100 | Step 9401/20074 | Loss: 3.5598\n",
      "Epoch 9/100 | Step 9501/20074 | Loss: 1.6418\n",
      "Epoch 9/100 | Step 9601/20074 | Loss: 0.0033\n",
      "Epoch 9/100 | Step 9701/20074 | Loss: 1.7278\n",
      "Epoch 9/100 | Step 9801/20074 | Loss: 1.2544\n",
      "Epoch 9/100 | Step 9901/20074 | Loss: 0.0135\n",
      "Epoch 9/100 | Step 10001/20074 | Loss: 2.5725\n",
      "Epoch 9/100 | Step 10101/20074 | Loss: 0.1534\n",
      "Epoch 9/100 | Step 10201/20074 | Loss: 0.9663\n",
      "Epoch 9/100 | Step 10301/20074 | Loss: 0.5996\n",
      "Epoch 9/100 | Step 10401/20074 | Loss: 0.0077\n",
      "Epoch 9/100 | Step 10501/20074 | Loss: 1.8618\n",
      "Epoch 9/100 | Step 10601/20074 | Loss: 0.1835\n",
      "Epoch 9/100 | Step 10701/20074 | Loss: 0.2312\n",
      "Epoch 9/100 | Step 10801/20074 | Loss: 1.1934\n",
      "Epoch 9/100 | Step 10901/20074 | Loss: 0.0004\n",
      "Epoch 9/100 | Step 11001/20074 | Loss: 0.5581\n",
      "Epoch 9/100 | Step 11101/20074 | Loss: 1.2635\n",
      "Epoch 9/100 | Step 11201/20074 | Loss: 0.0008\n",
      "Epoch 9/100 | Step 11301/20074 | Loss: 0.0094\n",
      "Epoch 9/100 | Step 11401/20074 | Loss: 0.1885\n",
      "Epoch 9/100 | Step 11501/20074 | Loss: 0.0112\n",
      "Epoch 9/100 | Step 11601/20074 | Loss: 0.5615\n",
      "Epoch 9/100 | Step 11701/20074 | Loss: 0.0116\n",
      "Epoch 9/100 | Step 11801/20074 | Loss: 0.0211\n",
      "Epoch 9/100 | Step 11901/20074 | Loss: 0.0910\n",
      "Epoch 9/100 | Step 12001/20074 | Loss: 0.0087\n",
      "Epoch 9/100 | Step 12101/20074 | Loss: 2.8959\n",
      "Epoch 9/100 | Step 12201/20074 | Loss: 1.2580\n",
      "Epoch 9/100 | Step 12301/20074 | Loss: 0.4894\n",
      "Epoch 9/100 | Step 12401/20074 | Loss: 0.1235\n",
      "Epoch 9/100 | Step 12501/20074 | Loss: 0.0083\n",
      "Epoch 9/100 | Step 12601/20074 | Loss: 0.7301\n",
      "Epoch 9/100 | Step 12701/20074 | Loss: 0.2040\n",
      "Epoch 9/100 | Step 12801/20074 | Loss: 0.1049\n",
      "Epoch 9/100 | Step 12901/20074 | Loss: 0.0248\n",
      "Epoch 9/100 | Step 13001/20074 | Loss: 0.0034\n",
      "Epoch 9/100 | Step 13101/20074 | Loss: 0.8014\n",
      "Epoch 9/100 | Step 13201/20074 | Loss: 0.0068\n",
      "Epoch 9/100 | Step 13301/20074 | Loss: 0.2316\n",
      "Epoch 9/100 | Step 13401/20074 | Loss: 0.0001\n",
      "Epoch 9/100 | Step 13501/20074 | Loss: 1.0960\n",
      "Epoch 9/100 | Step 13601/20074 | Loss: 0.0194\n",
      "Epoch 9/100 | Step 13701/20074 | Loss: 0.1961\n",
      "Epoch 9/100 | Step 13801/20074 | Loss: 0.0725\n",
      "Epoch 9/100 | Step 13901/20074 | Loss: 0.2514\n",
      "Epoch 9/100 | Step 14001/20074 | Loss: 0.0115\n",
      "Epoch 9/100 | Step 14101/20074 | Loss: 1.0170\n",
      "Epoch 9/100 | Step 14201/20074 | Loss: 0.0417\n",
      "Epoch 9/100 | Step 14301/20074 | Loss: 0.5862\n",
      "Epoch 9/100 | Step 14401/20074 | Loss: 0.0709\n",
      "Epoch 9/100 | Step 14501/20074 | Loss: 0.1121\n",
      "Epoch 9/100 | Step 14601/20074 | Loss: 0.1160\n",
      "Epoch 9/100 | Step 14701/20074 | Loss: 2.0491\n",
      "Epoch 9/100 | Step 14801/20074 | Loss: 0.2541\n",
      "Epoch 9/100 | Step 14901/20074 | Loss: 1.9861\n",
      "Epoch 9/100 | Step 15001/20074 | Loss: 0.4023\n",
      "Epoch 9/100 | Step 15101/20074 | Loss: 0.0936\n",
      "Epoch 9/100 | Step 15201/20074 | Loss: 0.0078\n",
      "Epoch 9/100 | Step 15301/20074 | Loss: 0.8287\n",
      "Epoch 9/100 | Step 15401/20074 | Loss: 0.2622\n",
      "Epoch 9/100 | Step 15501/20074 | Loss: 3.7572\n",
      "Epoch 9/100 | Step 15601/20074 | Loss: 0.0066\n",
      "Epoch 9/100 | Step 15701/20074 | Loss: 1.7738\n",
      "Epoch 9/100 | Step 15801/20074 | Loss: 0.2185\n",
      "Epoch 9/100 | Step 15901/20074 | Loss: 0.1155\n",
      "Epoch 9/100 | Step 16001/20074 | Loss: 0.3478\n",
      "Epoch 9/100 | Step 16101/20074 | Loss: 3.4682\n",
      "Epoch 9/100 | Step 16201/20074 | Loss: 0.3530\n",
      "Epoch 9/100 | Step 16301/20074 | Loss: 0.0047\n",
      "Epoch 9/100 | Step 16401/20074 | Loss: 2.5039\n",
      "Epoch 9/100 | Step 16501/20074 | Loss: 0.1086\n",
      "Epoch 9/100 | Step 16601/20074 | Loss: 0.0699\n",
      "Epoch 9/100 | Step 16701/20074 | Loss: 3.8782\n",
      "Epoch 9/100 | Step 16801/20074 | Loss: 2.2502\n",
      "Epoch 9/100 | Step 16901/20074 | Loss: 0.5875\n",
      "Epoch 9/100 | Step 17001/20074 | Loss: 0.0037\n",
      "Epoch 9/100 | Step 17101/20074 | Loss: 0.0003\n",
      "Epoch 9/100 | Step 17201/20074 | Loss: 0.4960\n",
      "Epoch 9/100 | Step 17301/20074 | Loss: 0.0032\n",
      "Epoch 9/100 | Step 17401/20074 | Loss: 3.0803\n",
      "Epoch 9/100 | Step 17501/20074 | Loss: 1.6668\n",
      "Epoch 9/100 | Step 17601/20074 | Loss: 4.8153\n",
      "Epoch 9/100 | Step 17701/20074 | Loss: 0.0093\n",
      "Epoch 9/100 | Step 17801/20074 | Loss: 0.1883\n",
      "Epoch 9/100 | Step 17901/20074 | Loss: 0.7716\n",
      "Epoch 9/100 | Step 18001/20074 | Loss: 0.1266\n",
      "Epoch 9/100 | Step 18101/20074 | Loss: 0.0267\n",
      "Epoch 9/100 | Step 18201/20074 | Loss: 1.2845\n",
      "Epoch 9/100 | Step 18301/20074 | Loss: 0.1664\n",
      "Epoch 9/100 | Step 18401/20074 | Loss: 0.0018\n",
      "Epoch 9/100 | Step 18501/20074 | Loss: 1.0312\n",
      "Epoch 9/100 | Step 18601/20074 | Loss: 0.0624\n",
      "Epoch 9/100 | Step 18701/20074 | Loss: 2.7431\n",
      "Epoch 9/100 | Step 18801/20074 | Loss: 0.0031\n",
      "Epoch 9/100 | Step 18901/20074 | Loss: 0.2287\n",
      "Epoch 9/100 | Step 19001/20074 | Loss: 0.0001\n",
      "Epoch 9/100 | Step 19101/20074 | Loss: 1.9071\n",
      "Epoch 9/100 | Step 19201/20074 | Loss: 0.0220\n",
      "Epoch 9/100 | Step 19301/20074 | Loss: 0.0011\n",
      "Epoch 9/100 | Step 19401/20074 | Loss: 1.3995\n",
      "Epoch 9/100 | Step 19501/20074 | Loss: 1.6090\n",
      "Epoch 9/100 | Step 19601/20074 | Loss: 2.0396\n",
      "Epoch 9/100 | Step 19701/20074 | Loss: 0.0834\n",
      "Epoch 9/100 | Step 19801/20074 | Loss: 0.0071\n",
      "Epoch 9/100 | Step 19901/20074 | Loss: 0.0128\n",
      "Epoch 9/100 | Step 20001/20074 | Loss: 3.9003\n",
      "Epoch 10/100 | Step 1/20074 | Loss: 0.0019\n",
      "Epoch 10/100 | Step 101/20074 | Loss: 1.2730\n",
      "Epoch 10/100 | Step 201/20074 | Loss: 0.4933\n",
      "Epoch 10/100 | Step 301/20074 | Loss: 0.1310\n",
      "Epoch 10/100 | Step 401/20074 | Loss: 0.0831\n",
      "Epoch 10/100 | Step 501/20074 | Loss: 1.6699\n",
      "Epoch 10/100 | Step 601/20074 | Loss: 0.5924\n",
      "Epoch 10/100 | Step 701/20074 | Loss: 0.0805\n",
      "Epoch 10/100 | Step 801/20074 | Loss: 0.0140\n",
      "Epoch 10/100 | Step 901/20074 | Loss: 0.0112\n",
      "Epoch 10/100 | Step 1001/20074 | Loss: 0.1080\n",
      "Epoch 10/100 | Step 1101/20074 | Loss: 1.0381\n",
      "Epoch 10/100 | Step 1201/20074 | Loss: 0.0581\n",
      "Epoch 10/100 | Step 1301/20074 | Loss: 0.0188\n",
      "Epoch 10/100 | Step 1401/20074 | Loss: 2.3474\n",
      "Epoch 10/100 | Step 1501/20074 | Loss: 0.2822\n",
      "Epoch 10/100 | Step 1601/20074 | Loss: 2.1312\n",
      "Epoch 10/100 | Step 1701/20074 | Loss: 0.1894\n",
      "Epoch 10/100 | Step 1801/20074 | Loss: 0.0974\n",
      "Epoch 10/100 | Step 1901/20074 | Loss: 0.0013\n",
      "Epoch 10/100 | Step 2001/20074 | Loss: 0.8007\n",
      "Epoch 10/100 | Step 2101/20074 | Loss: 0.2393\n",
      "Epoch 10/100 | Step 2201/20074 | Loss: 0.0013\n",
      "Epoch 10/100 | Step 2301/20074 | Loss: 0.3131\n",
      "Epoch 10/100 | Step 2401/20074 | Loss: 0.0019\n",
      "Epoch 10/100 | Step 2501/20074 | Loss: 0.0042\n",
      "Epoch 10/100 | Step 2601/20074 | Loss: 4.6834\n",
      "Epoch 10/100 | Step 2701/20074 | Loss: 2.0764\n",
      "Epoch 10/100 | Step 2801/20074 | Loss: 0.4520\n",
      "Epoch 10/100 | Step 2901/20074 | Loss: 0.2560\n",
      "Epoch 10/100 | Step 3001/20074 | Loss: 0.0503\n",
      "Epoch 10/100 | Step 3101/20074 | Loss: 0.1018\n",
      "Epoch 10/100 | Step 3201/20074 | Loss: 0.3582\n",
      "Epoch 10/100 | Step 3301/20074 | Loss: 0.0013\n",
      "Epoch 10/100 | Step 3401/20074 | Loss: 0.0195\n",
      "Epoch 10/100 | Step 3501/20074 | Loss: 0.0058\n",
      "Epoch 10/100 | Step 3601/20074 | Loss: 0.3456\n",
      "Epoch 10/100 | Step 3701/20074 | Loss: 2.3254\n",
      "Epoch 10/100 | Step 3801/20074 | Loss: 0.6009\n",
      "Epoch 10/100 | Step 3901/20074 | Loss: 1.0414\n",
      "Epoch 10/100 | Step 4001/20074 | Loss: 1.4919\n",
      "Epoch 10/100 | Step 4101/20074 | Loss: 0.0023\n",
      "Epoch 10/100 | Step 4201/20074 | Loss: 2.0633\n",
      "Epoch 10/100 | Step 4301/20074 | Loss: 1.4806\n",
      "Epoch 10/100 | Step 4401/20074 | Loss: 0.2089\n",
      "Epoch 10/100 | Step 4501/20074 | Loss: 0.2169\n",
      "Epoch 10/100 | Step 4601/20074 | Loss: 0.0644\n",
      "Epoch 10/100 | Step 4701/20074 | Loss: 0.1525\n",
      "Epoch 10/100 | Step 4801/20074 | Loss: 2.4470\n",
      "Epoch 10/100 | Step 4901/20074 | Loss: 0.7019\n",
      "Epoch 10/100 | Step 5001/20074 | Loss: 0.6551\n",
      "Epoch 10/100 | Step 5101/20074 | Loss: 0.0002\n",
      "Epoch 10/100 | Step 5201/20074 | Loss: 0.0168\n",
      "Epoch 10/100 | Step 5301/20074 | Loss: 3.6252\n",
      "Epoch 10/100 | Step 5401/20074 | Loss: 1.5754\n",
      "Epoch 10/100 | Step 5501/20074 | Loss: 0.3825\n",
      "Epoch 10/100 | Step 5601/20074 | Loss: 0.4282\n",
      "Epoch 10/100 | Step 5701/20074 | Loss: 0.0288\n",
      "Epoch 10/100 | Step 5801/20074 | Loss: 0.0062\n",
      "Epoch 10/100 | Step 5901/20074 | Loss: 0.0002\n",
      "Epoch 10/100 | Step 6001/20074 | Loss: 0.0012\n",
      "Epoch 10/100 | Step 6101/20074 | Loss: 0.0065\n",
      "Epoch 10/100 | Step 6201/20074 | Loss: 1.3997\n",
      "Epoch 10/100 | Step 6301/20074 | Loss: 0.4284\n",
      "Epoch 10/100 | Step 6401/20074 | Loss: 2.4399\n",
      "Epoch 10/100 | Step 6501/20074 | Loss: 0.0179\n",
      "Epoch 10/100 | Step 6601/20074 | Loss: 2.9941\n",
      "Epoch 10/100 | Step 6701/20074 | Loss: 0.6861\n",
      "Epoch 10/100 | Step 6801/20074 | Loss: 0.8997\n",
      "Epoch 10/100 | Step 6901/20074 | Loss: 0.4649\n",
      "Epoch 10/100 | Step 7001/20074 | Loss: 0.0427\n",
      "Epoch 10/100 | Step 7101/20074 | Loss: 0.0748\n",
      "Epoch 10/100 | Step 7201/20074 | Loss: 0.0335\n",
      "Epoch 10/100 | Step 7301/20074 | Loss: 0.0397\n",
      "Epoch 10/100 | Step 7401/20074 | Loss: 0.1319\n",
      "Epoch 10/100 | Step 7501/20074 | Loss: 0.1575\n",
      "Epoch 10/100 | Step 7601/20074 | Loss: 0.0448\n",
      "Epoch 10/100 | Step 7701/20074 | Loss: 0.0284\n",
      "Epoch 10/100 | Step 7801/20074 | Loss: 0.2705\n",
      "Epoch 10/100 | Step 7901/20074 | Loss: 1.8668\n",
      "Epoch 10/100 | Step 8001/20074 | Loss: 0.5424\n",
      "Epoch 10/100 | Step 8101/20074 | Loss: 0.0138\n",
      "Epoch 10/100 | Step 8201/20074 | Loss: 1.1248\n",
      "Epoch 10/100 | Step 8301/20074 | Loss: 0.0194\n",
      "Epoch 10/100 | Step 8401/20074 | Loss: 0.1116\n",
      "Epoch 10/100 | Step 8501/20074 | Loss: 0.0047\n",
      "Epoch 10/100 | Step 8601/20074 | Loss: 1.1233\n",
      "Epoch 10/100 | Step 8701/20074 | Loss: 0.8895\n",
      "Epoch 10/100 | Step 8801/20074 | Loss: 0.6886\n",
      "Epoch 10/100 | Step 8901/20074 | Loss: 1.3482\n",
      "Epoch 10/100 | Step 9001/20074 | Loss: 0.0042\n",
      "Epoch 10/100 | Step 9101/20074 | Loss: 2.8916\n",
      "Epoch 10/100 | Step 9201/20074 | Loss: 0.0078\n",
      "Epoch 10/100 | Step 9301/20074 | Loss: 0.4042\n",
      "Epoch 10/100 | Step 9401/20074 | Loss: 2.1883\n",
      "Epoch 10/100 | Step 9501/20074 | Loss: 1.3455\n",
      "Epoch 10/100 | Step 9601/20074 | Loss: 1.3971\n",
      "Epoch 10/100 | Step 9701/20074 | Loss: 0.2129\n",
      "Epoch 10/100 | Step 9801/20074 | Loss: 0.5561\n",
      "Epoch 10/100 | Step 9901/20074 | Loss: 3.1795\n",
      "Epoch 10/100 | Step 10001/20074 | Loss: 1.1114\n",
      "Epoch 10/100 | Step 10101/20074 | Loss: 1.1964\n",
      "Epoch 10/100 | Step 10201/20074 | Loss: 0.3765\n",
      "Epoch 10/100 | Step 10301/20074 | Loss: 0.1823\n",
      "Epoch 10/100 | Step 10401/20074 | Loss: 1.0354\n",
      "Epoch 10/100 | Step 10501/20074 | Loss: 0.6823\n",
      "Epoch 10/100 | Step 10601/20074 | Loss: 1.6327\n",
      "Epoch 10/100 | Step 10701/20074 | Loss: 0.0968\n",
      "Epoch 10/100 | Step 10801/20074 | Loss: 0.4777\n",
      "Epoch 10/100 | Step 10901/20074 | Loss: 1.5667\n",
      "Epoch 10/100 | Step 11001/20074 | Loss: 0.6740\n",
      "Epoch 10/100 | Step 11101/20074 | Loss: 1.3520\n",
      "Epoch 10/100 | Step 11201/20074 | Loss: 0.0021\n",
      "Epoch 10/100 | Step 11301/20074 | Loss: 0.0007\n",
      "Epoch 10/100 | Step 11401/20074 | Loss: 1.6448\n",
      "Epoch 10/100 | Step 11501/20074 | Loss: 0.2690\n",
      "Epoch 10/100 | Step 11601/20074 | Loss: 0.6868\n",
      "Epoch 10/100 | Step 11701/20074 | Loss: 0.0754\n",
      "Epoch 10/100 | Step 11801/20074 | Loss: 0.0010\n",
      "Epoch 10/100 | Step 11901/20074 | Loss: 1.7171\n",
      "Epoch 10/100 | Step 12001/20074 | Loss: 0.0740\n",
      "Epoch 10/100 | Step 12101/20074 | Loss: 0.3901\n",
      "Epoch 10/100 | Step 12201/20074 | Loss: 0.0307\n",
      "Epoch 10/100 | Step 12301/20074 | Loss: 3.2394\n",
      "Epoch 10/100 | Step 12401/20074 | Loss: 0.3976\n",
      "Epoch 10/100 | Step 12501/20074 | Loss: 4.2469\n",
      "Epoch 10/100 | Step 12601/20074 | Loss: 5.0370\n",
      "Epoch 10/100 | Step 12701/20074 | Loss: 1.7612\n",
      "Epoch 10/100 | Step 12801/20074 | Loss: 0.1140\n",
      "Epoch 10/100 | Step 12901/20074 | Loss: 0.8702\n",
      "Epoch 10/100 | Step 13001/20074 | Loss: 1.5022\n",
      "Epoch 10/100 | Step 13101/20074 | Loss: 2.3547\n",
      "Epoch 10/100 | Step 13201/20074 | Loss: 0.8211\n",
      "Epoch 10/100 | Step 13301/20074 | Loss: 0.2885\n",
      "Epoch 10/100 | Step 13401/20074 | Loss: 2.6054\n",
      "Epoch 10/100 | Step 13501/20074 | Loss: 3.4857\n",
      "Epoch 10/100 | Step 13601/20074 | Loss: 2.4240\n",
      "Epoch 10/100 | Step 13701/20074 | Loss: 0.0691\n",
      "Epoch 10/100 | Step 13801/20074 | Loss: 0.3358\n",
      "Epoch 10/100 | Step 13901/20074 | Loss: 0.0017\n",
      "Epoch 10/100 | Step 14001/20074 | Loss: 0.0010\n",
      "Epoch 10/100 | Step 14101/20074 | Loss: 0.6331\n",
      "Epoch 10/100 | Step 14201/20074 | Loss: 0.0006\n",
      "Epoch 10/100 | Step 14301/20074 | Loss: 1.1140\n",
      "Epoch 10/100 | Step 14401/20074 | Loss: 0.0012\n",
      "Epoch 10/100 | Step 14501/20074 | Loss: 2.8928\n",
      "Epoch 10/100 | Step 14601/20074 | Loss: 3.0437\n",
      "Epoch 10/100 | Step 14701/20074 | Loss: 2.1431\n",
      "Epoch 10/100 | Step 14801/20074 | Loss: 0.6132\n",
      "Epoch 10/100 | Step 14901/20074 | Loss: 0.0547\n",
      "Epoch 10/100 | Step 15001/20074 | Loss: 0.5847\n",
      "Epoch 10/100 | Step 15101/20074 | Loss: 0.0052\n",
      "Epoch 10/100 | Step 15201/20074 | Loss: 0.1386\n",
      "Epoch 10/100 | Step 15301/20074 | Loss: 0.1442\n",
      "Epoch 10/100 | Step 15401/20074 | Loss: 1.8865\n",
      "Epoch 10/100 | Step 15501/20074 | Loss: 1.0175\n",
      "Epoch 10/100 | Step 15601/20074 | Loss: 0.5961\n",
      "Epoch 10/100 | Step 15701/20074 | Loss: 0.1592\n",
      "Epoch 10/100 | Step 15801/20074 | Loss: 0.3130\n",
      "Epoch 10/100 | Step 15901/20074 | Loss: 0.1071\n",
      "Epoch 10/100 | Step 16001/20074 | Loss: 0.0925\n",
      "Epoch 10/100 | Step 16101/20074 | Loss: 1.5226\n",
      "Epoch 10/100 | Step 16201/20074 | Loss: 2.3884\n",
      "Epoch 10/100 | Step 16301/20074 | Loss: 0.0132\n",
      "Epoch 10/100 | Step 16401/20074 | Loss: 1.9203\n",
      "Epoch 10/100 | Step 16501/20074 | Loss: 0.3181\n",
      "Epoch 10/100 | Step 16601/20074 | Loss: 0.0017\n",
      "Epoch 10/100 | Step 16701/20074 | Loss: 0.5963\n",
      "Epoch 10/100 | Step 16801/20074 | Loss: 0.0120\n",
      "Epoch 10/100 | Step 16901/20074 | Loss: 0.4121\n",
      "Epoch 10/100 | Step 17001/20074 | Loss: 0.0007\n",
      "Epoch 10/100 | Step 17101/20074 | Loss: 0.2734\n",
      "Epoch 10/100 | Step 17201/20074 | Loss: 0.0000\n",
      "Epoch 10/100 | Step 17301/20074 | Loss: 0.0342\n",
      "Epoch 10/100 | Step 17401/20074 | Loss: 0.0891\n",
      "Epoch 10/100 | Step 17501/20074 | Loss: 0.1584\n",
      "Epoch 10/100 | Step 17601/20074 | Loss: 0.0442\n",
      "Epoch 10/100 | Step 17701/20074 | Loss: 0.0979\n",
      "Epoch 10/100 | Step 17801/20074 | Loss: 0.0004\n",
      "Epoch 10/100 | Step 17901/20074 | Loss: 0.8989\n",
      "Epoch 10/100 | Step 18001/20074 | Loss: 0.0175\n",
      "Epoch 10/100 | Step 18101/20074 | Loss: 0.7431\n",
      "Epoch 10/100 | Step 18201/20074 | Loss: 0.7126\n",
      "Epoch 10/100 | Step 18301/20074 | Loss: 0.2853\n",
      "Epoch 10/100 | Step 18401/20074 | Loss: 4.1840\n",
      "Epoch 10/100 | Step 18501/20074 | Loss: 0.3245\n",
      "Epoch 10/100 | Step 18601/20074 | Loss: 1.2013\n",
      "Epoch 10/100 | Step 18701/20074 | Loss: 0.0967\n",
      "Epoch 10/100 | Step 18801/20074 | Loss: 0.0206\n",
      "Epoch 10/100 | Step 18901/20074 | Loss: 1.8163\n",
      "Epoch 10/100 | Step 19001/20074 | Loss: 0.0094\n",
      "Epoch 10/100 | Step 19101/20074 | Loss: 0.7405\n",
      "Epoch 10/100 | Step 19201/20074 | Loss: 0.0005\n",
      "Epoch 10/100 | Step 19301/20074 | Loss: 0.4765\n",
      "Epoch 10/100 | Step 19401/20074 | Loss: 0.0587\n",
      "Epoch 10/100 | Step 19501/20074 | Loss: 0.0076\n",
      "Epoch 10/100 | Step 19601/20074 | Loss: 0.0924\n",
      "Epoch 10/100 | Step 19701/20074 | Loss: 0.0031\n",
      "Epoch 10/100 | Step 19801/20074 | Loss: 0.1511\n",
      "Epoch 10/100 | Step 19901/20074 | Loss: 1.0042\n",
      "Epoch 10/100 | Step 20001/20074 | Loss: 1.3987\n",
      "Epoch 11/100 | Step 1/20074 | Loss: 3.4757\n",
      "Epoch 11/100 | Step 101/20074 | Loss: 0.6978\n",
      "Epoch 11/100 | Step 201/20074 | Loss: 0.0010\n",
      "Epoch 11/100 | Step 301/20074 | Loss: 0.0008\n",
      "Epoch 11/100 | Step 401/20074 | Loss: 0.0008\n",
      "Epoch 11/100 | Step 501/20074 | Loss: 0.8361\n",
      "Epoch 11/100 | Step 601/20074 | Loss: 0.8124\n",
      "Epoch 11/100 | Step 701/20074 | Loss: 2.2546\n",
      "Epoch 11/100 | Step 801/20074 | Loss: 0.6166\n",
      "Epoch 11/100 | Step 901/20074 | Loss: 0.8423\n",
      "Epoch 11/100 | Step 1001/20074 | Loss: 3.4903\n",
      "Epoch 11/100 | Step 1101/20074 | Loss: 0.0739\n",
      "Epoch 11/100 | Step 1201/20074 | Loss: 1.2099\n",
      "Epoch 11/100 | Step 1301/20074 | Loss: 0.0017\n",
      "Epoch 11/100 | Step 1401/20074 | Loss: 0.3495\n",
      "Epoch 11/100 | Step 1501/20074 | Loss: 0.0012\n",
      "Epoch 11/100 | Step 1601/20074 | Loss: 0.5530\n",
      "Epoch 11/100 | Step 1701/20074 | Loss: 1.8975\n",
      "Epoch 11/100 | Step 1801/20074 | Loss: 0.2311\n",
      "Epoch 11/100 | Step 1901/20074 | Loss: 0.0006\n",
      "Epoch 11/100 | Step 2001/20074 | Loss: 0.0015\n",
      "Epoch 11/100 | Step 2101/20074 | Loss: 1.6010\n",
      "Epoch 11/100 | Step 2201/20074 | Loss: 1.2964\n",
      "Epoch 11/100 | Step 2301/20074 | Loss: 0.0044\n",
      "Epoch 11/100 | Step 2401/20074 | Loss: 0.0013\n",
      "Epoch 11/100 | Step 2501/20074 | Loss: 1.4198\n",
      "Epoch 11/100 | Step 2601/20074 | Loss: 0.0150\n",
      "Epoch 11/100 | Step 2701/20074 | Loss: 0.0535\n",
      "Epoch 11/100 | Step 2801/20074 | Loss: 0.8579\n",
      "Epoch 11/100 | Step 2901/20074 | Loss: 1.5783\n",
      "Epoch 11/100 | Step 3001/20074 | Loss: 3.6586\n",
      "Epoch 11/100 | Step 3101/20074 | Loss: 0.1273\n",
      "Epoch 11/100 | Step 3201/20074 | Loss: 0.0402\n",
      "Epoch 11/100 | Step 3301/20074 | Loss: 0.0029\n",
      "Epoch 11/100 | Step 3401/20074 | Loss: 0.2666\n",
      "Epoch 11/100 | Step 3501/20074 | Loss: 0.4120\n",
      "Epoch 11/100 | Step 3601/20074 | Loss: 2.1755\n",
      "Epoch 11/100 | Step 3701/20074 | Loss: 0.0915\n",
      "Epoch 11/100 | Step 3801/20074 | Loss: 0.0020\n",
      "Epoch 11/100 | Step 3901/20074 | Loss: 0.8363\n",
      "Epoch 11/100 | Step 4001/20074 | Loss: 0.2778\n",
      "Epoch 11/100 | Step 4101/20074 | Loss: 2.2353\n",
      "Epoch 11/100 | Step 4201/20074 | Loss: 2.9559\n",
      "Epoch 11/100 | Step 4301/20074 | Loss: 0.0009\n",
      "Epoch 11/100 | Step 4401/20074 | Loss: 1.4462\n",
      "Epoch 11/100 | Step 4501/20074 | Loss: 4.7831\n",
      "Epoch 11/100 | Step 4601/20074 | Loss: 1.7407\n",
      "Epoch 11/100 | Step 4701/20074 | Loss: 0.5968\n",
      "Epoch 11/100 | Step 4801/20074 | Loss: 0.0104\n",
      "Epoch 11/100 | Step 4901/20074 | Loss: 0.0025\n",
      "Epoch 11/100 | Step 5001/20074 | Loss: 0.0029\n",
      "Epoch 11/100 | Step 5101/20074 | Loss: 0.0069\n",
      "Epoch 11/100 | Step 5201/20074 | Loss: 0.0506\n",
      "Epoch 11/100 | Step 5301/20074 | Loss: 1.2446\n",
      "Epoch 11/100 | Step 5401/20074 | Loss: 2.3177\n",
      "Epoch 11/100 | Step 5501/20074 | Loss: 0.7493\n",
      "Epoch 11/100 | Step 5601/20074 | Loss: 0.7428\n",
      "Epoch 11/100 | Step 5701/20074 | Loss: 0.0627\n",
      "Epoch 11/100 | Step 5801/20074 | Loss: 0.0265\n",
      "Epoch 11/100 | Step 5901/20074 | Loss: 0.0019\n",
      "Epoch 11/100 | Step 6001/20074 | Loss: 0.6124\n",
      "Epoch 11/100 | Step 6101/20074 | Loss: 0.0007\n",
      "Epoch 11/100 | Step 6201/20074 | Loss: 0.1584\n",
      "Epoch 11/100 | Step 6301/20074 | Loss: 0.4802\n",
      "Epoch 11/100 | Step 6401/20074 | Loss: 0.4503\n",
      "Epoch 11/100 | Step 6501/20074 | Loss: 0.0561\n",
      "Epoch 11/100 | Step 6601/20074 | Loss: 2.0732\n",
      "Epoch 11/100 | Step 6701/20074 | Loss: 0.3119\n",
      "Epoch 11/100 | Step 6801/20074 | Loss: 0.8765\n",
      "Epoch 11/100 | Step 6901/20074 | Loss: 3.2197\n",
      "Epoch 11/100 | Step 7001/20074 | Loss: 0.0104\n",
      "Epoch 11/100 | Step 7101/20074 | Loss: 0.0096\n",
      "Epoch 11/100 | Step 7201/20074 | Loss: 2.0119\n",
      "Epoch 11/100 | Step 7301/20074 | Loss: 0.0791\n",
      "Epoch 11/100 | Step 7401/20074 | Loss: 0.0005\n",
      "Epoch 11/100 | Step 7501/20074 | Loss: 0.0684\n",
      "Epoch 11/100 | Step 7601/20074 | Loss: 0.3989\n",
      "Epoch 11/100 | Step 7701/20074 | Loss: 0.9771\n",
      "Epoch 11/100 | Step 7801/20074 | Loss: 0.6901\n",
      "Epoch 11/100 | Step 7901/20074 | Loss: 0.0056\n",
      "Epoch 11/100 | Step 8001/20074 | Loss: 1.0097\n",
      "Epoch 11/100 | Step 8101/20074 | Loss: 1.1489\n",
      "Epoch 11/100 | Step 8201/20074 | Loss: 2.4353\n",
      "Epoch 11/100 | Step 8301/20074 | Loss: 4.0379\n",
      "Epoch 11/100 | Step 8401/20074 | Loss: 1.3968\n",
      "Epoch 11/100 | Step 8501/20074 | Loss: 0.0001\n",
      "Epoch 11/100 | Step 8601/20074 | Loss: 1.1611\n",
      "Epoch 11/100 | Step 8701/20074 | Loss: 0.0313\n",
      "Epoch 11/100 | Step 8801/20074 | Loss: 0.4816\n",
      "Epoch 11/100 | Step 8901/20074 | Loss: 0.8039\n",
      "Epoch 11/100 | Step 9001/20074 | Loss: 0.0002\n",
      "Epoch 11/100 | Step 9101/20074 | Loss: 0.0335\n",
      "Epoch 11/100 | Step 9201/20074 | Loss: 0.0537\n",
      "Epoch 11/100 | Step 9301/20074 | Loss: 0.5808\n",
      "Epoch 11/100 | Step 9401/20074 | Loss: 2.0076\n",
      "Epoch 11/100 | Step 9501/20074 | Loss: 0.6832\n",
      "Epoch 11/100 | Step 9601/20074 | Loss: 0.1321\n",
      "Epoch 11/100 | Step 9701/20074 | Loss: 0.7636\n",
      "Epoch 11/100 | Step 9801/20074 | Loss: 2.7868\n",
      "Epoch 11/100 | Step 9901/20074 | Loss: 0.2728\n",
      "Epoch 11/100 | Step 10001/20074 | Loss: 3.5088\n",
      "Epoch 11/100 | Step 10101/20074 | Loss: 1.0628\n",
      "Epoch 11/100 | Step 10201/20074 | Loss: 0.0471\n",
      "Epoch 11/100 | Step 10301/20074 | Loss: 0.0081\n",
      "Epoch 11/100 | Step 10401/20074 | Loss: 0.0051\n",
      "Epoch 11/100 | Step 10501/20074 | Loss: 0.0298\n",
      "Epoch 11/100 | Step 10601/20074 | Loss: 1.8501\n",
      "Epoch 11/100 | Step 10701/20074 | Loss: 1.1307\n",
      "Epoch 11/100 | Step 10801/20074 | Loss: 0.0019\n",
      "Epoch 11/100 | Step 10901/20074 | Loss: 0.0007\n",
      "Epoch 11/100 | Step 11001/20074 | Loss: 0.4392\n",
      "Epoch 11/100 | Step 11101/20074 | Loss: 0.3580\n",
      "Epoch 11/100 | Step 11201/20074 | Loss: 0.7449\n",
      "Epoch 11/100 | Step 11301/20074 | Loss: 0.0089\n",
      "Epoch 11/100 | Step 11401/20074 | Loss: 0.2406\n",
      "Epoch 11/100 | Step 11501/20074 | Loss: 0.7379\n",
      "Epoch 11/100 | Step 11601/20074 | Loss: 0.0094\n",
      "Epoch 11/100 | Step 11701/20074 | Loss: 0.0003\n",
      "Epoch 11/100 | Step 11801/20074 | Loss: 0.6451\n",
      "Epoch 11/100 | Step 11901/20074 | Loss: 1.1939\n",
      "Epoch 11/100 | Step 12001/20074 | Loss: 0.0128\n",
      "Epoch 11/100 | Step 12101/20074 | Loss: 4.9863\n",
      "Epoch 11/100 | Step 12201/20074 | Loss: 0.9804\n",
      "Epoch 11/100 | Step 12301/20074 | Loss: 0.0249\n",
      "Epoch 11/100 | Step 12401/20074 | Loss: 0.0321\n",
      "Epoch 11/100 | Step 12501/20074 | Loss: 0.0325\n",
      "Epoch 11/100 | Step 12601/20074 | Loss: 0.0001\n",
      "Epoch 11/100 | Step 12701/20074 | Loss: 0.0545\n",
      "Epoch 11/100 | Step 12801/20074 | Loss: 0.3212\n",
      "Epoch 11/100 | Step 12901/20074 | Loss: 0.0182\n",
      "Epoch 11/100 | Step 13001/20074 | Loss: 0.5218\n",
      "Epoch 11/100 | Step 13101/20074 | Loss: 0.1489\n",
      "Epoch 11/100 | Step 13201/20074 | Loss: 0.0045\n",
      "Epoch 11/100 | Step 13301/20074 | Loss: 0.0568\n",
      "Epoch 11/100 | Step 13401/20074 | Loss: 0.0598\n",
      "Epoch 11/100 | Step 13501/20074 | Loss: 0.0016\n",
      "Epoch 11/100 | Step 13601/20074 | Loss: 0.7281\n",
      "Epoch 11/100 | Step 13701/20074 | Loss: 2.3144\n",
      "Epoch 11/100 | Step 13801/20074 | Loss: 0.0059\n",
      "Epoch 11/100 | Step 13901/20074 | Loss: 0.2958\n",
      "Epoch 11/100 | Step 14001/20074 | Loss: 0.7917\n",
      "Epoch 11/100 | Step 14101/20074 | Loss: 0.0913\n",
      "Epoch 11/100 | Step 14201/20074 | Loss: 1.7705\n",
      "Epoch 11/100 | Step 14301/20074 | Loss: 0.1789\n",
      "Epoch 11/100 | Step 14401/20074 | Loss: 0.8968\n",
      "Epoch 11/100 | Step 14501/20074 | Loss: 0.1216\n",
      "Epoch 11/100 | Step 14601/20074 | Loss: 0.4310\n",
      "Epoch 11/100 | Step 14701/20074 | Loss: 0.0177\n",
      "Epoch 11/100 | Step 14801/20074 | Loss: 0.0022\n",
      "Epoch 11/100 | Step 14901/20074 | Loss: 0.3397\n",
      "Epoch 11/100 | Step 15001/20074 | Loss: 0.9452\n",
      "Epoch 11/100 | Step 15101/20074 | Loss: 0.0035\n",
      "Epoch 11/100 | Step 15201/20074 | Loss: 0.3509\n",
      "Epoch 11/100 | Step 15301/20074 | Loss: 5.3611\n",
      "Epoch 11/100 | Step 15401/20074 | Loss: 0.0094\n",
      "Epoch 11/100 | Step 15501/20074 | Loss: 0.6131\n",
      "Epoch 11/100 | Step 15601/20074 | Loss: 1.6289\n",
      "Epoch 11/100 | Step 15701/20074 | Loss: 0.0183\n",
      "Epoch 11/100 | Step 15801/20074 | Loss: 0.0006\n",
      "Epoch 11/100 | Step 15901/20074 | Loss: 2.7219\n",
      "Epoch 11/100 | Step 16001/20074 | Loss: 0.0519\n",
      "Epoch 11/100 | Step 16101/20074 | Loss: 0.0010\n",
      "Epoch 11/100 | Step 16201/20074 | Loss: 0.7932\n",
      "Epoch 11/100 | Step 16301/20074 | Loss: 1.1008\n",
      "Epoch 11/100 | Step 16401/20074 | Loss: 0.5234\n",
      "Epoch 11/100 | Step 16501/20074 | Loss: 0.3541\n",
      "Epoch 11/100 | Step 16601/20074 | Loss: 0.1457\n",
      "Epoch 11/100 | Step 16701/20074 | Loss: 0.3785\n",
      "Epoch 11/100 | Step 16801/20074 | Loss: 1.4439\n",
      "Epoch 11/100 | Step 16901/20074 | Loss: 0.2081\n",
      "Epoch 11/100 | Step 17001/20074 | Loss: 0.4631\n",
      "Epoch 11/100 | Step 17101/20074 | Loss: 2.5387\n",
      "Epoch 11/100 | Step 17201/20074 | Loss: 0.4573\n",
      "Epoch 11/100 | Step 17301/20074 | Loss: 0.0844\n",
      "Epoch 11/100 | Step 17401/20074 | Loss: 0.0044\n",
      "Epoch 11/100 | Step 17501/20074 | Loss: 0.2591\n",
      "Epoch 11/100 | Step 17601/20074 | Loss: 1.7900\n",
      "Epoch 11/100 | Step 17701/20074 | Loss: 0.0003\n",
      "Epoch 11/100 | Step 17801/20074 | Loss: 0.8672\n",
      "Epoch 11/100 | Step 17901/20074 | Loss: 0.5830\n",
      "Epoch 11/100 | Step 18001/20074 | Loss: 0.2853\n",
      "Epoch 11/100 | Step 18101/20074 | Loss: 0.0182\n",
      "Epoch 11/100 | Step 18201/20074 | Loss: 0.0013\n",
      "Epoch 11/100 | Step 18301/20074 | Loss: 0.0005\n",
      "Epoch 11/100 | Step 18401/20074 | Loss: 0.0001\n",
      "Epoch 11/100 | Step 18501/20074 | Loss: 0.0659\n",
      "Epoch 11/100 | Step 18601/20074 | Loss: 0.3641\n",
      "Epoch 11/100 | Step 18701/20074 | Loss: 4.6325\n",
      "Epoch 11/100 | Step 18801/20074 | Loss: 0.2740\n",
      "Epoch 11/100 | Step 18901/20074 | Loss: 0.2089\n",
      "Epoch 11/100 | Step 19001/20074 | Loss: 3.6525\n",
      "Epoch 11/100 | Step 19101/20074 | Loss: 0.2063\n",
      "Epoch 11/100 | Step 19201/20074 | Loss: 0.0068\n",
      "Epoch 11/100 | Step 19301/20074 | Loss: 0.4946\n",
      "Epoch 11/100 | Step 19401/20074 | Loss: 2.7227\n",
      "Epoch 11/100 | Step 19501/20074 | Loss: 0.2880\n",
      "Epoch 11/100 | Step 19601/20074 | Loss: 0.5763\n",
      "Epoch 11/100 | Step 19701/20074 | Loss: 1.0974\n",
      "Epoch 11/100 | Step 19801/20074 | Loss: 0.2165\n",
      "Epoch 11/100 | Step 19901/20074 | Loss: 1.8801\n",
      "Epoch 11/100 | Step 20001/20074 | Loss: 0.6175\n",
      "Epoch 12/100 | Step 1/20074 | Loss: 1.5108\n",
      "Epoch 12/100 | Step 101/20074 | Loss: 3.3220\n",
      "Epoch 12/100 | Step 201/20074 | Loss: 0.0240\n",
      "Epoch 12/100 | Step 301/20074 | Loss: 0.0106\n",
      "Epoch 12/100 | Step 401/20074 | Loss: 3.8671\n",
      "Epoch 12/100 | Step 501/20074 | Loss: 0.0570\n",
      "Epoch 12/100 | Step 601/20074 | Loss: 2.4274\n",
      "Epoch 12/100 | Step 701/20074 | Loss: 1.5212\n",
      "Epoch 12/100 | Step 801/20074 | Loss: 3.9436\n",
      "Epoch 12/100 | Step 901/20074 | Loss: 3.0590\n",
      "Epoch 12/100 | Step 1001/20074 | Loss: 0.1604\n",
      "Epoch 12/100 | Step 1101/20074 | Loss: 0.1113\n",
      "Epoch 12/100 | Step 1201/20074 | Loss: 0.5972\n",
      "Epoch 12/100 | Step 1301/20074 | Loss: 1.0236\n",
      "Epoch 12/100 | Step 1401/20074 | Loss: 0.4983\n",
      "Epoch 12/100 | Step 1501/20074 | Loss: 0.0357\n",
      "Epoch 12/100 | Step 1601/20074 | Loss: 0.2008\n",
      "Epoch 12/100 | Step 1701/20074 | Loss: 0.8884\n",
      "Epoch 12/100 | Step 1801/20074 | Loss: 3.5308\n",
      "Epoch 12/100 | Step 1901/20074 | Loss: 0.0720\n",
      "Epoch 12/100 | Step 2001/20074 | Loss: 3.0784\n",
      "Epoch 12/100 | Step 2101/20074 | Loss: 3.6810\n",
      "Epoch 12/100 | Step 2201/20074 | Loss: 0.0034\n",
      "Epoch 12/100 | Step 2301/20074 | Loss: 0.4869\n",
      "Epoch 12/100 | Step 2401/20074 | Loss: 0.1979\n",
      "Epoch 12/100 | Step 2501/20074 | Loss: 4.1635\n",
      "Epoch 12/100 | Step 2601/20074 | Loss: 0.0253\n",
      "Epoch 12/100 | Step 2701/20074 | Loss: 3.7619\n",
      "Epoch 12/100 | Step 2801/20074 | Loss: 4.4082\n",
      "Epoch 12/100 | Step 2901/20074 | Loss: 0.0870\n",
      "Epoch 12/100 | Step 3001/20074 | Loss: 0.1931\n",
      "Epoch 12/100 | Step 3101/20074 | Loss: 0.5735\n",
      "Epoch 12/100 | Step 3201/20074 | Loss: 0.0003\n",
      "Epoch 12/100 | Step 3301/20074 | Loss: 0.0492\n",
      "Epoch 12/100 | Step 3401/20074 | Loss: 0.0016\n",
      "Epoch 12/100 | Step 3501/20074 | Loss: 1.7553\n",
      "Epoch 12/100 | Step 3601/20074 | Loss: 4.6708\n",
      "Epoch 12/100 | Step 3701/20074 | Loss: 1.3394\n",
      "Epoch 12/100 | Step 3801/20074 | Loss: 3.1577\n",
      "Epoch 12/100 | Step 3901/20074 | Loss: 3.3910\n",
      "Epoch 12/100 | Step 4001/20074 | Loss: 0.0035\n",
      "Epoch 12/100 | Step 4101/20074 | Loss: 0.0058\n",
      "Epoch 12/100 | Step 4201/20074 | Loss: 0.0117\n",
      "Epoch 12/100 | Step 4301/20074 | Loss: 0.0018\n",
      "Epoch 12/100 | Step 4401/20074 | Loss: 0.2785\n",
      "Epoch 12/100 | Step 4501/20074 | Loss: 0.0086\n",
      "Epoch 12/100 | Step 4601/20074 | Loss: 0.0733\n",
      "Epoch 12/100 | Step 4701/20074 | Loss: 0.0059\n",
      "Epoch 12/100 | Step 4801/20074 | Loss: 0.1839\n",
      "Epoch 12/100 | Step 4901/20074 | Loss: 2.5987\n",
      "Epoch 12/100 | Step 5001/20074 | Loss: 0.0614\n",
      "Epoch 12/100 | Step 5101/20074 | Loss: 0.0023\n",
      "Epoch 12/100 | Step 5201/20074 | Loss: 1.1480\n",
      "Epoch 12/100 | Step 5301/20074 | Loss: 0.1642\n",
      "Epoch 12/100 | Step 5401/20074 | Loss: 0.0024\n",
      "Epoch 12/100 | Step 5501/20074 | Loss: 1.8802\n",
      "Epoch 12/100 | Step 5601/20074 | Loss: 0.6404\n",
      "Epoch 12/100 | Step 5701/20074 | Loss: 0.0004\n",
      "Epoch 12/100 | Step 5801/20074 | Loss: 1.6076\n",
      "Epoch 12/100 | Step 5901/20074 | Loss: 1.9576\n",
      "Epoch 12/100 | Step 6001/20074 | Loss: 1.2016\n",
      "Epoch 12/100 | Step 6101/20074 | Loss: 0.0307\n",
      "Epoch 12/100 | Step 6201/20074 | Loss: 3.4719\n",
      "Epoch 12/100 | Step 6301/20074 | Loss: 2.7501\n",
      "Epoch 12/100 | Step 6401/20074 | Loss: 0.2991\n",
      "Epoch 12/100 | Step 6501/20074 | Loss: 0.7878\n",
      "Epoch 12/100 | Step 6601/20074 | Loss: 0.1484\n",
      "Epoch 12/100 | Step 6701/20074 | Loss: 0.2761\n",
      "Epoch 12/100 | Step 6801/20074 | Loss: 0.0946\n",
      "Epoch 12/100 | Step 6901/20074 | Loss: 0.4258\n",
      "Epoch 12/100 | Step 7001/20074 | Loss: 2.1834\n",
      "Epoch 12/100 | Step 7101/20074 | Loss: 0.0003\n",
      "Epoch 12/100 | Step 7201/20074 | Loss: 0.9115\n",
      "Epoch 12/100 | Step 7301/20074 | Loss: 0.0022\n",
      "Epoch 12/100 | Step 7401/20074 | Loss: 3.2982\n",
      "Epoch 12/100 | Step 7501/20074 | Loss: 0.0003\n",
      "Epoch 12/100 | Step 7601/20074 | Loss: 0.0521\n",
      "Epoch 12/100 | Step 7701/20074 | Loss: 0.0024\n",
      "Epoch 12/100 | Step 7801/20074 | Loss: 0.1267\n",
      "Epoch 12/100 | Step 7901/20074 | Loss: 1.6186\n",
      "Epoch 12/100 | Step 8001/20074 | Loss: 0.0018\n",
      "Epoch 12/100 | Step 8101/20074 | Loss: 1.4843\n",
      "Epoch 12/100 | Step 8201/20074 | Loss: 0.0037\n",
      "Epoch 12/100 | Step 8301/20074 | Loss: 0.4126\n",
      "Epoch 12/100 | Step 8401/20074 | Loss: 1.4401\n",
      "Epoch 12/100 | Step 8501/20074 | Loss: 0.0817\n",
      "Epoch 12/100 | Step 8601/20074 | Loss: 3.1830\n",
      "Epoch 12/100 | Step 8701/20074 | Loss: 0.0023\n",
      "Epoch 12/100 | Step 8801/20074 | Loss: 0.0135\n",
      "Epoch 12/100 | Step 8901/20074 | Loss: 0.0010\n",
      "Epoch 12/100 | Step 9001/20074 | Loss: 0.0003\n",
      "Epoch 12/100 | Step 9101/20074 | Loss: 0.0157\n",
      "Epoch 12/100 | Step 9201/20074 | Loss: 0.5946\n",
      "Epoch 12/100 | Step 9301/20074 | Loss: 0.0370\n",
      "Epoch 12/100 | Step 9401/20074 | Loss: 0.2146\n",
      "Epoch 12/100 | Step 9501/20074 | Loss: 2.7381\n",
      "Epoch 12/100 | Step 9601/20074 | Loss: 1.0969\n",
      "Epoch 12/100 | Step 9701/20074 | Loss: 3.4328\n",
      "Epoch 12/100 | Step 9801/20074 | Loss: 2.2341\n",
      "Epoch 12/100 | Step 9901/20074 | Loss: 0.1925\n",
      "Epoch 12/100 | Step 10001/20074 | Loss: 0.2685\n",
      "Epoch 12/100 | Step 10101/20074 | Loss: 0.8944\n",
      "Epoch 12/100 | Step 10201/20074 | Loss: 0.4508\n",
      "Epoch 12/100 | Step 10301/20074 | Loss: 0.0615\n",
      "Epoch 12/100 | Step 10401/20074 | Loss: 0.0014\n",
      "Epoch 12/100 | Step 10501/20074 | Loss: 0.1194\n",
      "Epoch 12/100 | Step 10601/20074 | Loss: 0.7323\n",
      "Epoch 12/100 | Step 10701/20074 | Loss: 2.3082\n",
      "Epoch 12/100 | Step 10801/20074 | Loss: 1.6047\n",
      "Epoch 12/100 | Step 10901/20074 | Loss: 0.0727\n",
      "Epoch 12/100 | Step 11001/20074 | Loss: 3.1382\n",
      "Epoch 12/100 | Step 11101/20074 | Loss: 0.0452\n",
      "Epoch 12/100 | Step 11201/20074 | Loss: 1.8266\n",
      "Epoch 12/100 | Step 11301/20074 | Loss: 1.1864\n",
      "Epoch 12/100 | Step 11401/20074 | Loss: 0.1394\n",
      "Epoch 12/100 | Step 11501/20074 | Loss: 0.0012\n",
      "Epoch 12/100 | Step 11601/20074 | Loss: 0.7491\n",
      "Epoch 12/100 | Step 11701/20074 | Loss: 2.8712\n",
      "Epoch 12/100 | Step 11801/20074 | Loss: 0.2911\n",
      "Epoch 12/100 | Step 11901/20074 | Loss: 0.1995\n",
      "Epoch 12/100 | Step 12001/20074 | Loss: 2.3015\n",
      "Epoch 12/100 | Step 12101/20074 | Loss: 0.6578\n",
      "Epoch 12/100 | Step 12201/20074 | Loss: 3.2400\n",
      "Epoch 12/100 | Step 12301/20074 | Loss: 0.4072\n",
      "Epoch 12/100 | Step 12401/20074 | Loss: 0.0072\n",
      "Epoch 12/100 | Step 12501/20074 | Loss: 0.0551\n",
      "Epoch 12/100 | Step 12601/20074 | Loss: 0.2219\n",
      "Epoch 12/100 | Step 12701/20074 | Loss: 0.4723\n",
      "Epoch 12/100 | Step 12801/20074 | Loss: 1.3340\n",
      "Epoch 12/100 | Step 12901/20074 | Loss: 0.5471\n",
      "Epoch 12/100 | Step 13001/20074 | Loss: 0.0108\n",
      "Epoch 12/100 | Step 13101/20074 | Loss: 1.2820\n",
      "Epoch 12/100 | Step 13201/20074 | Loss: 0.0396\n",
      "Epoch 12/100 | Step 13301/20074 | Loss: 3.3164\n",
      "Epoch 12/100 | Step 13401/20074 | Loss: 1.0690\n",
      "Epoch 12/100 | Step 13501/20074 | Loss: 0.0021\n",
      "Epoch 12/100 | Step 13601/20074 | Loss: 0.0015\n",
      "Epoch 12/100 | Step 13701/20074 | Loss: 0.0206\n",
      "Epoch 12/100 | Step 13801/20074 | Loss: 1.9161\n",
      "Epoch 12/100 | Step 13901/20074 | Loss: 0.0214\n",
      "Epoch 12/100 | Step 14001/20074 | Loss: 0.2153\n",
      "Epoch 12/100 | Step 14101/20074 | Loss: 1.0776\n",
      "Epoch 12/100 | Step 14201/20074 | Loss: 0.3389\n",
      "Epoch 12/100 | Step 14301/20074 | Loss: 0.9479\n",
      "Epoch 12/100 | Step 14401/20074 | Loss: 0.0060\n",
      "Epoch 12/100 | Step 14501/20074 | Loss: 0.0031\n",
      "Epoch 12/100 | Step 14601/20074 | Loss: 0.0058\n",
      "Epoch 12/100 | Step 14701/20074 | Loss: 0.0266\n",
      "Epoch 12/100 | Step 14801/20074 | Loss: 0.2691\n",
      "Epoch 12/100 | Step 14901/20074 | Loss: 1.5430\n",
      "Epoch 12/100 | Step 15001/20074 | Loss: 1.4279\n",
      "Epoch 12/100 | Step 15101/20074 | Loss: 0.0016\n",
      "Epoch 12/100 | Step 15201/20074 | Loss: 1.3908\n",
      "Epoch 12/100 | Step 15301/20074 | Loss: 0.0077\n",
      "Epoch 12/100 | Step 15401/20074 | Loss: 0.2381\n",
      "Epoch 12/100 | Step 15501/20074 | Loss: 0.0204\n",
      "Epoch 12/100 | Step 15601/20074 | Loss: 0.0079\n",
      "Epoch 12/100 | Step 15701/20074 | Loss: 0.0013\n",
      "Epoch 12/100 | Step 15801/20074 | Loss: 0.0858\n",
      "Epoch 12/100 | Step 15901/20074 | Loss: 0.0032\n",
      "Epoch 12/100 | Step 16001/20074 | Loss: 0.0822\n",
      "Epoch 12/100 | Step 16101/20074 | Loss: 0.2704\n",
      "Epoch 12/100 | Step 16201/20074 | Loss: 0.4064\n",
      "Epoch 12/100 | Step 16301/20074 | Loss: 0.6021\n",
      "Epoch 12/100 | Step 16401/20074 | Loss: 0.0055\n",
      "Epoch 12/100 | Step 16501/20074 | Loss: 0.0096\n",
      "Epoch 12/100 | Step 16601/20074 | Loss: 0.5036\n",
      "Epoch 12/100 | Step 16701/20074 | Loss: 0.0154\n",
      "Epoch 12/100 | Step 16801/20074 | Loss: 1.6706\n",
      "Epoch 12/100 | Step 16901/20074 | Loss: 1.2889\n",
      "Epoch 12/100 | Step 17001/20074 | Loss: 0.0001\n",
      "Epoch 12/100 | Step 17101/20074 | Loss: 0.0005\n",
      "Epoch 12/100 | Step 17201/20074 | Loss: 0.0140\n",
      "Epoch 12/100 | Step 17301/20074 | Loss: 0.0007\n",
      "Epoch 12/100 | Step 17401/20074 | Loss: 1.1479\n",
      "Epoch 12/100 | Step 17501/20074 | Loss: 0.0380\n",
      "Epoch 12/100 | Step 17601/20074 | Loss: 1.2791\n",
      "Epoch 12/100 | Step 17701/20074 | Loss: 3.5202\n",
      "Epoch 12/100 | Step 17801/20074 | Loss: 0.3179\n",
      "Epoch 12/100 | Step 17901/20074 | Loss: 0.7076\n",
      "Epoch 12/100 | Step 18001/20074 | Loss: 0.1625\n",
      "Epoch 12/100 | Step 18101/20074 | Loss: 0.0015\n",
      "Epoch 12/100 | Step 18201/20074 | Loss: 0.3056\n",
      "Epoch 12/100 | Step 18301/20074 | Loss: 0.1519\n",
      "Epoch 12/100 | Step 18401/20074 | Loss: 0.3906\n",
      "Epoch 12/100 | Step 18501/20074 | Loss: 0.0005\n",
      "Epoch 12/100 | Step 18601/20074 | Loss: 3.9992\n",
      "Epoch 12/100 | Step 18701/20074 | Loss: 0.2332\n",
      "Epoch 12/100 | Step 18801/20074 | Loss: 0.0006\n",
      "Epoch 12/100 | Step 18901/20074 | Loss: 0.1884\n",
      "Epoch 12/100 | Step 19001/20074 | Loss: 1.5517\n",
      "Epoch 12/100 | Step 19101/20074 | Loss: 3.8332\n",
      "Epoch 12/100 | Step 19201/20074 | Loss: 0.0007\n",
      "Epoch 12/100 | Step 19301/20074 | Loss: 0.0006\n",
      "Epoch 12/100 | Step 19401/20074 | Loss: 2.3487\n",
      "Epoch 12/100 | Step 19501/20074 | Loss: 0.5661\n",
      "Epoch 12/100 | Step 19601/20074 | Loss: 0.0579\n",
      "Epoch 12/100 | Step 19701/20074 | Loss: 1.0749\n",
      "Epoch 12/100 | Step 19801/20074 | Loss: 0.2740\n",
      "Epoch 12/100 | Step 19901/20074 | Loss: 0.2071\n",
      "Epoch 12/100 | Step 20001/20074 | Loss: 0.2216\n",
      "Epoch 13/100 | Step 1/20074 | Loss: 0.4920\n",
      "Epoch 13/100 | Step 101/20074 | Loss: 0.0028\n",
      "Epoch 13/100 | Step 201/20074 | Loss: 0.5283\n",
      "Epoch 13/100 | Step 301/20074 | Loss: 0.0951\n",
      "Epoch 13/100 | Step 401/20074 | Loss: 0.1910\n",
      "Epoch 13/100 | Step 501/20074 | Loss: 5.7787\n",
      "Epoch 13/100 | Step 601/20074 | Loss: 0.1252\n",
      "Epoch 13/100 | Step 701/20074 | Loss: 0.2127\n",
      "Epoch 13/100 | Step 801/20074 | Loss: 0.1472\n",
      "Epoch 13/100 | Step 901/20074 | Loss: 0.1291\n",
      "Epoch 13/100 | Step 1001/20074 | Loss: 0.6502\n",
      "Epoch 13/100 | Step 1101/20074 | Loss: 0.0568\n",
      "Epoch 13/100 | Step 1201/20074 | Loss: 3.8855\n",
      "Epoch 13/100 | Step 1301/20074 | Loss: 0.0062\n",
      "Epoch 13/100 | Step 1401/20074 | Loss: 0.0004\n",
      "Epoch 13/100 | Step 1501/20074 | Loss: 0.2310\n",
      "Epoch 13/100 | Step 1601/20074 | Loss: 0.0080\n",
      "Epoch 13/100 | Step 1701/20074 | Loss: 0.0040\n",
      "Epoch 13/100 | Step 1801/20074 | Loss: 2.4456\n",
      "Epoch 13/100 | Step 1901/20074 | Loss: 0.7965\n",
      "Epoch 13/100 | Step 2001/20074 | Loss: 0.0674\n",
      "Epoch 13/100 | Step 2101/20074 | Loss: 0.1780\n",
      "Epoch 13/100 | Step 2201/20074 | Loss: 0.1777\n",
      "Epoch 13/100 | Step 2301/20074 | Loss: 0.0074\n",
      "Epoch 13/100 | Step 2401/20074 | Loss: 0.9918\n",
      "Epoch 13/100 | Step 2501/20074 | Loss: 0.9932\n",
      "Epoch 13/100 | Step 2601/20074 | Loss: 0.5771\n",
      "Epoch 13/100 | Step 2701/20074 | Loss: 0.0017\n",
      "Epoch 13/100 | Step 2801/20074 | Loss: 0.1757\n",
      "Epoch 13/100 | Step 2901/20074 | Loss: 4.9697\n",
      "Epoch 13/100 | Step 3001/20074 | Loss: 0.0002\n",
      "Epoch 13/100 | Step 3101/20074 | Loss: 1.2489\n",
      "Epoch 13/100 | Step 3201/20074 | Loss: 0.0564\n",
      "Epoch 13/100 | Step 3301/20074 | Loss: 0.9688\n",
      "Epoch 13/100 | Step 3401/20074 | Loss: 2.7143\n",
      "Epoch 13/100 | Step 3501/20074 | Loss: 0.5474\n",
      "Epoch 13/100 | Step 3601/20074 | Loss: 1.5319\n",
      "Epoch 13/100 | Step 3701/20074 | Loss: 0.1634\n",
      "Epoch 13/100 | Step 3801/20074 | Loss: 0.7130\n",
      "Epoch 13/100 | Step 3901/20074 | Loss: 3.8421\n",
      "Epoch 13/100 | Step 4001/20074 | Loss: 3.4040\n",
      "Epoch 13/100 | Step 4101/20074 | Loss: 2.5336\n",
      "Epoch 13/100 | Step 4201/20074 | Loss: 0.0478\n",
      "Epoch 13/100 | Step 4301/20074 | Loss: 2.0357\n",
      "Epoch 13/100 | Step 4401/20074 | Loss: 2.4753\n",
      "Epoch 13/100 | Step 4501/20074 | Loss: 0.3226\n",
      "Epoch 13/100 | Step 4601/20074 | Loss: 0.7338\n",
      "Epoch 13/100 | Step 4701/20074 | Loss: 0.1903\n",
      "Epoch 13/100 | Step 4801/20074 | Loss: 0.0167\n",
      "Epoch 13/100 | Step 4901/20074 | Loss: 2.0345\n",
      "Epoch 13/100 | Step 5001/20074 | Loss: 0.1660\n",
      "Epoch 13/100 | Step 5101/20074 | Loss: 0.5660\n",
      "Epoch 13/100 | Step 5201/20074 | Loss: 2.4806\n",
      "Epoch 13/100 | Step 5301/20074 | Loss: 0.0013\n",
      "Epoch 13/100 | Step 5401/20074 | Loss: 0.9204\n",
      "Epoch 13/100 | Step 5501/20074 | Loss: 0.8296\n",
      "Epoch 13/100 | Step 5601/20074 | Loss: 0.5885\n",
      "Epoch 13/100 | Step 5701/20074 | Loss: 0.0196\n",
      "Epoch 13/100 | Step 5801/20074 | Loss: 4.1386\n",
      "Epoch 13/100 | Step 5901/20074 | Loss: 0.7440\n",
      "Epoch 13/100 | Step 6001/20074 | Loss: 0.3145\n",
      "Epoch 13/100 | Step 6101/20074 | Loss: 0.0032\n",
      "Epoch 13/100 | Step 6201/20074 | Loss: 0.6341\n",
      "Epoch 13/100 | Step 6301/20074 | Loss: 0.0879\n",
      "Epoch 13/100 | Step 6401/20074 | Loss: 0.0011\n",
      "Epoch 13/100 | Step 6501/20074 | Loss: 3.6050\n",
      "Epoch 13/100 | Step 6601/20074 | Loss: 2.5884\n",
      "Epoch 13/100 | Step 6701/20074 | Loss: 0.0292\n",
      "Epoch 13/100 | Step 6801/20074 | Loss: 0.2119\n",
      "Epoch 13/100 | Step 6901/20074 | Loss: 2.3218\n",
      "Epoch 13/100 | Step 7001/20074 | Loss: 0.1672\n",
      "Epoch 13/100 | Step 7101/20074 | Loss: 2.4826\n",
      "Epoch 13/100 | Step 7201/20074 | Loss: 1.5117\n",
      "Epoch 13/100 | Step 7301/20074 | Loss: 0.3463\n",
      "Epoch 13/100 | Step 7401/20074 | Loss: 0.9786\n",
      "Epoch 13/100 | Step 7501/20074 | Loss: 1.2207\n",
      "Epoch 13/100 | Step 7601/20074 | Loss: 0.3341\n",
      "Epoch 13/100 | Step 7701/20074 | Loss: 0.5695\n",
      "Epoch 13/100 | Step 7801/20074 | Loss: 4.0756\n",
      "Epoch 13/100 | Step 7901/20074 | Loss: 0.0556\n",
      "Epoch 13/100 | Step 8001/20074 | Loss: 0.1449\n",
      "Epoch 13/100 | Step 8101/20074 | Loss: 0.1010\n",
      "Epoch 13/100 | Step 8201/20074 | Loss: 0.0044\n",
      "Epoch 13/100 | Step 8301/20074 | Loss: 3.4092\n",
      "Epoch 13/100 | Step 8401/20074 | Loss: 0.2567\n",
      "Epoch 13/100 | Step 8501/20074 | Loss: 4.6282\n",
      "Epoch 13/100 | Step 8601/20074 | Loss: 0.0011\n",
      "Epoch 13/100 | Step 8701/20074 | Loss: 0.0015\n",
      "Epoch 13/100 | Step 8801/20074 | Loss: 0.0472\n",
      "Epoch 13/100 | Step 8901/20074 | Loss: 0.9516\n",
      "Epoch 13/100 | Step 9001/20074 | Loss: 1.1076\n",
      "Epoch 13/100 | Step 9101/20074 | Loss: 0.1871\n",
      "Epoch 13/100 | Step 9201/20074 | Loss: 0.0007\n",
      "Epoch 13/100 | Step 9301/20074 | Loss: 0.0077\n",
      "Epoch 13/100 | Step 9401/20074 | Loss: 4.1478\n",
      "Epoch 13/100 | Step 9501/20074 | Loss: 0.0008\n",
      "Epoch 13/100 | Step 9601/20074 | Loss: 0.1445\n",
      "Epoch 13/100 | Step 9701/20074 | Loss: 0.5228\n",
      "Epoch 13/100 | Step 9801/20074 | Loss: 0.0001\n",
      "Epoch 13/100 | Step 9901/20074 | Loss: 0.5143\n",
      "Epoch 13/100 | Step 10001/20074 | Loss: 0.0309\n",
      "Epoch 13/100 | Step 10101/20074 | Loss: 0.0038\n",
      "Epoch 13/100 | Step 10201/20074 | Loss: 3.9788\n",
      "Epoch 13/100 | Step 10301/20074 | Loss: 0.0784\n",
      "Epoch 13/100 | Step 10401/20074 | Loss: 0.0004\n",
      "Epoch 13/100 | Step 10501/20074 | Loss: 0.6242\n",
      "Epoch 13/100 | Step 10601/20074 | Loss: 3.8628\n",
      "Epoch 13/100 | Step 10701/20074 | Loss: 0.4259\n",
      "Epoch 13/100 | Step 10801/20074 | Loss: 0.2022\n",
      "Epoch 13/100 | Step 10901/20074 | Loss: 0.0036\n",
      "Epoch 13/100 | Step 11001/20074 | Loss: 0.3038\n",
      "Epoch 13/100 | Step 11101/20074 | Loss: 2.6522\n",
      "Epoch 13/100 | Step 11201/20074 | Loss: 0.0117\n",
      "Epoch 13/100 | Step 11301/20074 | Loss: 0.0154\n",
      "Epoch 13/100 | Step 11401/20074 | Loss: 0.0018\n",
      "Epoch 13/100 | Step 11501/20074 | Loss: 0.2749\n",
      "Epoch 13/100 | Step 11601/20074 | Loss: 0.4684\n",
      "Epoch 13/100 | Step 11701/20074 | Loss: 3.6463\n",
      "Epoch 13/100 | Step 11801/20074 | Loss: 0.1302\n",
      "Epoch 13/100 | Step 11901/20074 | Loss: 0.0645\n",
      "Epoch 13/100 | Step 12001/20074 | Loss: 0.0001\n",
      "Epoch 13/100 | Step 12101/20074 | Loss: 0.0022\n",
      "Epoch 13/100 | Step 12201/20074 | Loss: 0.0064\n",
      "Epoch 13/100 | Step 12301/20074 | Loss: 0.1496\n",
      "Epoch 13/100 | Step 12401/20074 | Loss: 0.0266\n",
      "Epoch 13/100 | Step 12501/20074 | Loss: 1.1301\n",
      "Epoch 13/100 | Step 12601/20074 | Loss: 0.2306\n",
      "Epoch 13/100 | Step 12701/20074 | Loss: 0.1381\n",
      "Epoch 13/100 | Step 12801/20074 | Loss: 2.5299\n",
      "Epoch 13/100 | Step 12901/20074 | Loss: 0.1163\n",
      "Epoch 13/100 | Step 13001/20074 | Loss: 0.0005\n",
      "Epoch 13/100 | Step 13101/20074 | Loss: 2.0167\n",
      "Epoch 13/100 | Step 13201/20074 | Loss: 0.5478\n",
      "Epoch 13/100 | Step 13301/20074 | Loss: 4.3268\n",
      "Epoch 13/100 | Step 13401/20074 | Loss: 2.5792\n",
      "Epoch 13/100 | Step 13501/20074 | Loss: 1.6748\n",
      "Epoch 13/100 | Step 13601/20074 | Loss: 0.7153\n",
      "Epoch 13/100 | Step 13701/20074 | Loss: 0.0113\n",
      "Epoch 13/100 | Step 13801/20074 | Loss: 1.3171\n",
      "Epoch 13/100 | Step 13901/20074 | Loss: 0.0091\n",
      "Epoch 13/100 | Step 14001/20074 | Loss: 0.0003\n",
      "Epoch 13/100 | Step 14101/20074 | Loss: 1.7786\n",
      "Epoch 13/100 | Step 14201/20074 | Loss: 0.3539\n",
      "Epoch 13/100 | Step 14301/20074 | Loss: 0.5868\n",
      "Epoch 13/100 | Step 14401/20074 | Loss: 0.0107\n",
      "Epoch 13/100 | Step 14501/20074 | Loss: 0.7718\n",
      "Epoch 13/100 | Step 14601/20074 | Loss: 0.7656\n",
      "Epoch 13/100 | Step 14701/20074 | Loss: 0.2330\n",
      "Epoch 13/100 | Step 14801/20074 | Loss: 0.3875\n",
      "Epoch 13/100 | Step 14901/20074 | Loss: 0.2017\n",
      "Epoch 13/100 | Step 15001/20074 | Loss: 0.0199\n",
      "Epoch 13/100 | Step 15101/20074 | Loss: 0.1006\n",
      "Epoch 13/100 | Step 15201/20074 | Loss: 0.7091\n",
      "Epoch 13/100 | Step 15301/20074 | Loss: 4.2884\n",
      "Epoch 13/100 | Step 15401/20074 | Loss: 1.6852\n",
      "Epoch 13/100 | Step 15501/20074 | Loss: 0.0727\n",
      "Epoch 13/100 | Step 15601/20074 | Loss: 0.0827\n",
      "Epoch 13/100 | Step 15701/20074 | Loss: 0.0058\n",
      "Epoch 13/100 | Step 15801/20074 | Loss: 0.1874\n",
      "Epoch 13/100 | Step 15901/20074 | Loss: 0.1931\n",
      "Epoch 13/100 | Step 16001/20074 | Loss: 0.2143\n",
      "Epoch 13/100 | Step 16101/20074 | Loss: 0.2597\n",
      "Epoch 13/100 | Step 16201/20074 | Loss: 0.3646\n",
      "Epoch 13/100 | Step 16301/20074 | Loss: 0.4844\n",
      "Epoch 13/100 | Step 16401/20074 | Loss: 0.1368\n",
      "Epoch 13/100 | Step 16501/20074 | Loss: 0.0008\n",
      "Epoch 13/100 | Step 16601/20074 | Loss: 0.2308\n",
      "Epoch 13/100 | Step 16701/20074 | Loss: 0.0002\n",
      "Epoch 13/100 | Step 16801/20074 | Loss: 1.2365\n",
      "Epoch 13/100 | Step 16901/20074 | Loss: 0.0048\n",
      "Epoch 13/100 | Step 17001/20074 | Loss: 0.2297\n",
      "Epoch 13/100 | Step 17101/20074 | Loss: 0.0119\n",
      "Epoch 13/100 | Step 17201/20074 | Loss: 1.4263\n",
      "Epoch 13/100 | Step 17301/20074 | Loss: 2.9456\n",
      "Epoch 13/100 | Step 17401/20074 | Loss: 0.0024\n",
      "Epoch 13/100 | Step 17501/20074 | Loss: 0.3973\n",
      "Epoch 13/100 | Step 17601/20074 | Loss: 0.1991\n",
      "Epoch 13/100 | Step 17701/20074 | Loss: 0.4331\n",
      "Epoch 13/100 | Step 17801/20074 | Loss: 3.7002\n",
      "Epoch 13/100 | Step 17901/20074 | Loss: 4.0547\n",
      "Epoch 13/100 | Step 18001/20074 | Loss: 0.3748\n",
      "Epoch 13/100 | Step 18101/20074 | Loss: 0.0276\n",
      "Epoch 13/100 | Step 18201/20074 | Loss: 0.5507\n",
      "Epoch 13/100 | Step 18301/20074 | Loss: 0.0290\n",
      "Epoch 13/100 | Step 18401/20074 | Loss: 4.0267\n",
      "Epoch 13/100 | Step 18501/20074 | Loss: 1.4139\n",
      "Epoch 13/100 | Step 18601/20074 | Loss: 0.0004\n",
      "Epoch 13/100 | Step 18701/20074 | Loss: 0.0013\n",
      "Epoch 13/100 | Step 18801/20074 | Loss: 2.0320\n",
      "Epoch 13/100 | Step 18901/20074 | Loss: 0.1347\n",
      "Epoch 13/100 | Step 19001/20074 | Loss: 0.0055\n",
      "Epoch 13/100 | Step 19101/20074 | Loss: 0.0764\n",
      "Epoch 13/100 | Step 19201/20074 | Loss: 0.5397\n",
      "Epoch 13/100 | Step 19301/20074 | Loss: 0.6772\n",
      "Epoch 13/100 | Step 19401/20074 | Loss: 0.0053\n",
      "Epoch 13/100 | Step 19501/20074 | Loss: 0.0022\n",
      "Epoch 13/100 | Step 19601/20074 | Loss: 1.0984\n",
      "Epoch 13/100 | Step 19701/20074 | Loss: 0.0475\n",
      "Epoch 13/100 | Step 19801/20074 | Loss: 3.1941\n",
      "Epoch 13/100 | Step 19901/20074 | Loss: 1.3448\n",
      "Epoch 13/100 | Step 20001/20074 | Loss: 1.8862\n",
      "Epoch 14/100 | Step 1/20074 | Loss: 4.6011\n",
      "Epoch 14/100 | Step 101/20074 | Loss: 0.7105\n",
      "Epoch 14/100 | Step 201/20074 | Loss: 6.9126\n",
      "Epoch 14/100 | Step 301/20074 | Loss: 0.7226\n",
      "Epoch 14/100 | Step 401/20074 | Loss: 0.0079\n",
      "Epoch 14/100 | Step 501/20074 | Loss: 3.4461\n",
      "Epoch 14/100 | Step 601/20074 | Loss: 0.0016\n",
      "Epoch 14/100 | Step 701/20074 | Loss: 0.4833\n",
      "Epoch 14/100 | Step 801/20074 | Loss: 0.8079\n",
      "Epoch 14/100 | Step 901/20074 | Loss: 0.0417\n",
      "Epoch 14/100 | Step 1001/20074 | Loss: 0.0093\n",
      "Epoch 14/100 | Step 1101/20074 | Loss: 0.3705\n",
      "Epoch 14/100 | Step 1201/20074 | Loss: 0.3782\n",
      "Epoch 14/100 | Step 1301/20074 | Loss: 0.2788\n",
      "Epoch 14/100 | Step 1401/20074 | Loss: 0.0541\n",
      "Epoch 14/100 | Step 1501/20074 | Loss: 0.0554\n",
      "Epoch 14/100 | Step 1601/20074 | Loss: 3.8112\n",
      "Epoch 14/100 | Step 1701/20074 | Loss: 4.8631\n",
      "Epoch 14/100 | Step 1801/20074 | Loss: 0.2203\n",
      "Epoch 14/100 | Step 1901/20074 | Loss: 0.9248\n",
      "Epoch 14/100 | Step 2001/20074 | Loss: 0.0485\n",
      "Epoch 14/100 | Step 2101/20074 | Loss: 2.7901\n",
      "Epoch 14/100 | Step 2201/20074 | Loss: 0.5610\n",
      "Epoch 14/100 | Step 2301/20074 | Loss: 0.4093\n",
      "Epoch 14/100 | Step 2401/20074 | Loss: 5.3416\n",
      "Epoch 14/100 | Step 2501/20074 | Loss: 2.8147\n",
      "Epoch 14/100 | Step 2601/20074 | Loss: 0.0799\n",
      "Epoch 14/100 | Step 2701/20074 | Loss: 0.0953\n",
      "Epoch 14/100 | Step 2801/20074 | Loss: 0.9915\n",
      "Epoch 14/100 | Step 2901/20074 | Loss: 0.9249\n",
      "Epoch 14/100 | Step 3001/20074 | Loss: 0.0350\n",
      "Epoch 14/100 | Step 3101/20074 | Loss: 1.5409\n",
      "Epoch 14/100 | Step 3201/20074 | Loss: 0.1984\n",
      "Epoch 14/100 | Step 3301/20074 | Loss: 1.7315\n",
      "Epoch 14/100 | Step 3401/20074 | Loss: 3.2079\n",
      "Epoch 14/100 | Step 3501/20074 | Loss: 0.0377\n",
      "Epoch 14/100 | Step 3601/20074 | Loss: 0.6007\n",
      "Epoch 14/100 | Step 3701/20074 | Loss: 3.1643\n",
      "Epoch 14/100 | Step 3801/20074 | Loss: 0.6851\n",
      "Epoch 14/100 | Step 3901/20074 | Loss: 0.0023\n",
      "Epoch 14/100 | Step 4001/20074 | Loss: 0.3690\n",
      "Epoch 14/100 | Step 4101/20074 | Loss: 4.4695\n",
      "Epoch 14/100 | Step 4201/20074 | Loss: 1.9175\n",
      "Epoch 14/100 | Step 4301/20074 | Loss: 0.3619\n",
      "Epoch 14/100 | Step 4401/20074 | Loss: 0.4033\n",
      "Epoch 14/100 | Step 4501/20074 | Loss: 2.1658\n",
      "Epoch 14/100 | Step 4601/20074 | Loss: 1.7648\n",
      "Epoch 14/100 | Step 4701/20074 | Loss: 0.0044\n",
      "Epoch 14/100 | Step 4801/20074 | Loss: 0.1952\n",
      "Epoch 14/100 | Step 4901/20074 | Loss: 3.6689\n",
      "Epoch 14/100 | Step 5001/20074 | Loss: 0.0104\n",
      "Epoch 14/100 | Step 5101/20074 | Loss: 0.0019\n",
      "Epoch 14/100 | Step 5201/20074 | Loss: 0.9020\n",
      "Epoch 14/100 | Step 5301/20074 | Loss: 0.0509\n",
      "Epoch 14/100 | Step 5401/20074 | Loss: 0.0000\n",
      "Epoch 14/100 | Step 5501/20074 | Loss: 0.3966\n",
      "Epoch 14/100 | Step 5601/20074 | Loss: 0.7509\n",
      "Epoch 14/100 | Step 5701/20074 | Loss: 2.1060\n",
      "Epoch 14/100 | Step 5801/20074 | Loss: 0.0012\n",
      "Epoch 14/100 | Step 5901/20074 | Loss: 0.0885\n",
      "Epoch 14/100 | Step 6001/20074 | Loss: 0.3706\n",
      "Epoch 14/100 | Step 6101/20074 | Loss: 5.7457\n",
      "Epoch 14/100 | Step 6201/20074 | Loss: 0.0056\n",
      "Epoch 14/100 | Step 6301/20074 | Loss: 0.1332\n",
      "Epoch 14/100 | Step 6401/20074 | Loss: 0.0121\n",
      "Epoch 14/100 | Step 6501/20074 | Loss: 0.0440\n",
      "Epoch 14/100 | Step 6601/20074 | Loss: 0.6410\n",
      "Epoch 14/100 | Step 6701/20074 | Loss: 5.3846\n",
      "Epoch 14/100 | Step 6801/20074 | Loss: 0.9722\n",
      "Epoch 14/100 | Step 6901/20074 | Loss: 0.0023\n",
      "Epoch 14/100 | Step 7001/20074 | Loss: 1.7685\n",
      "Epoch 14/100 | Step 7101/20074 | Loss: 0.0019\n",
      "Epoch 14/100 | Step 7201/20074 | Loss: 0.0131\n",
      "Epoch 14/100 | Step 7301/20074 | Loss: 2.4236\n",
      "Epoch 14/100 | Step 7401/20074 | Loss: 0.0588\n",
      "Epoch 14/100 | Step 7501/20074 | Loss: 1.1729\n",
      "Epoch 14/100 | Step 7601/20074 | Loss: 0.0275\n",
      "Epoch 14/100 | Step 7701/20074 | Loss: 0.2493\n",
      "Epoch 14/100 | Step 7801/20074 | Loss: 0.0056\n",
      "Epoch 14/100 | Step 7901/20074 | Loss: 0.5241\n",
      "Epoch 14/100 | Step 8001/20074 | Loss: 1.1555\n",
      "Epoch 14/100 | Step 8101/20074 | Loss: 0.0254\n",
      "Epoch 14/100 | Step 8201/20074 | Loss: 0.3942\n",
      "Epoch 14/100 | Step 8301/20074 | Loss: 0.1894\n",
      "Epoch 14/100 | Step 8401/20074 | Loss: 0.2956\n",
      "Epoch 14/100 | Step 8501/20074 | Loss: 2.1385\n",
      "Epoch 14/100 | Step 8601/20074 | Loss: 0.0073\n",
      "Epoch 14/100 | Step 8701/20074 | Loss: 0.0099\n",
      "Epoch 14/100 | Step 8801/20074 | Loss: 0.2261\n",
      "Epoch 14/100 | Step 8901/20074 | Loss: 2.5492\n",
      "Epoch 14/100 | Step 9001/20074 | Loss: 1.1214\n",
      "Epoch 14/100 | Step 9101/20074 | Loss: 4.5484\n",
      "Epoch 14/100 | Step 9201/20074 | Loss: 0.5890\n",
      "Epoch 14/100 | Step 9301/20074 | Loss: 0.3996\n",
      "Epoch 14/100 | Step 9401/20074 | Loss: 0.0019\n",
      "Epoch 14/100 | Step 9501/20074 | Loss: 0.9640\n",
      "Epoch 14/100 | Step 9601/20074 | Loss: 0.0009\n",
      "Epoch 14/100 | Step 9701/20074 | Loss: 4.6211\n",
      "Epoch 14/100 | Step 9801/20074 | Loss: 1.8307\n",
      "Epoch 14/100 | Step 9901/20074 | Loss: 2.7186\n",
      "Epoch 14/100 | Step 10001/20074 | Loss: 0.4393\n",
      "Epoch 14/100 | Step 10101/20074 | Loss: 0.6834\n",
      "Epoch 14/100 | Step 10201/20074 | Loss: 0.5103\n",
      "Epoch 14/100 | Step 10301/20074 | Loss: 0.6875\n",
      "Epoch 14/100 | Step 10401/20074 | Loss: 0.0078\n",
      "Epoch 14/100 | Step 10501/20074 | Loss: 2.5150\n",
      "Epoch 14/100 | Step 10601/20074 | Loss: 1.7633\n",
      "Epoch 14/100 | Step 10701/20074 | Loss: 0.0022\n",
      "Epoch 14/100 | Step 10801/20074 | Loss: 0.0011\n",
      "Epoch 14/100 | Step 10901/20074 | Loss: 0.0229\n",
      "Epoch 14/100 | Step 11001/20074 | Loss: 2.8926\n",
      "Epoch 14/100 | Step 11101/20074 | Loss: 0.4721\n",
      "Epoch 14/100 | Step 11201/20074 | Loss: 0.0853\n",
      "Epoch 14/100 | Step 11301/20074 | Loss: 0.4483\n",
      "Epoch 14/100 | Step 11401/20074 | Loss: 0.1718\n",
      "Epoch 14/100 | Step 11501/20074 | Loss: 0.0588\n",
      "Epoch 14/100 | Step 11601/20074 | Loss: 0.1900\n",
      "Epoch 14/100 | Step 11701/20074 | Loss: 0.9031\n",
      "Epoch 14/100 | Step 11801/20074 | Loss: 0.0010\n",
      "Epoch 14/100 | Step 11901/20074 | Loss: 0.2465\n",
      "Epoch 14/100 | Step 12001/20074 | Loss: 0.0056\n",
      "Epoch 14/100 | Step 12101/20074 | Loss: 0.1404\n",
      "Epoch 14/100 | Step 12201/20074 | Loss: 0.0165\n",
      "Epoch 14/100 | Step 12301/20074 | Loss: 0.0489\n",
      "Epoch 14/100 | Step 12401/20074 | Loss: 0.0032\n",
      "Epoch 14/100 | Step 12501/20074 | Loss: 0.0662\n",
      "Epoch 14/100 | Step 12601/20074 | Loss: 0.2806\n",
      "Epoch 14/100 | Step 12701/20074 | Loss: 0.3544\n",
      "Epoch 14/100 | Step 12801/20074 | Loss: 2.2776\n",
      "Epoch 14/100 | Step 12901/20074 | Loss: 0.1037\n",
      "Epoch 14/100 | Step 13001/20074 | Loss: 0.0002\n",
      "Epoch 14/100 | Step 13101/20074 | Loss: 0.0020\n",
      "Epoch 14/100 | Step 13201/20074 | Loss: 0.0734\n",
      "Epoch 14/100 | Step 13301/20074 | Loss: 0.6008\n",
      "Epoch 14/100 | Step 13401/20074 | Loss: 0.2299\n",
      "Epoch 14/100 | Step 13501/20074 | Loss: 1.1195\n",
      "Epoch 14/100 | Step 13601/20074 | Loss: 1.7715\n",
      "Epoch 14/100 | Step 13701/20074 | Loss: 2.1671\n",
      "Epoch 14/100 | Step 13801/20074 | Loss: 3.3960\n",
      "Epoch 14/100 | Step 13901/20074 | Loss: 0.0011\n",
      "Epoch 14/100 | Step 14001/20074 | Loss: 0.0815\n",
      "Epoch 14/100 | Step 14101/20074 | Loss: 3.3913\n",
      "Epoch 14/100 | Step 14201/20074 | Loss: 0.9210\n",
      "Epoch 14/100 | Step 14301/20074 | Loss: 0.5229\n",
      "Epoch 14/100 | Step 14401/20074 | Loss: 0.2521\n",
      "Epoch 14/100 | Step 14501/20074 | Loss: 1.4196\n",
      "Epoch 14/100 | Step 14601/20074 | Loss: 1.9707\n",
      "Epoch 14/100 | Step 14701/20074 | Loss: 0.1259\n",
      "Epoch 14/100 | Step 14801/20074 | Loss: 0.5576\n",
      "Epoch 14/100 | Step 14901/20074 | Loss: 0.0548\n",
      "Epoch 14/100 | Step 15001/20074 | Loss: 0.0949\n",
      "Epoch 14/100 | Step 15101/20074 | Loss: 2.6478\n",
      "Epoch 14/100 | Step 15201/20074 | Loss: 4.8437\n",
      "Epoch 14/100 | Step 15301/20074 | Loss: 0.0296\n",
      "Epoch 14/100 | Step 15401/20074 | Loss: 3.5784\n",
      "Epoch 14/100 | Step 15501/20074 | Loss: 3.9557\n",
      "Epoch 14/100 | Step 15601/20074 | Loss: 2.3312\n",
      "Epoch 14/100 | Step 15701/20074 | Loss: 0.1177\n",
      "Epoch 14/100 | Step 15801/20074 | Loss: 0.0116\n",
      "Epoch 14/100 | Step 15901/20074 | Loss: 0.4927\n",
      "Epoch 14/100 | Step 16001/20074 | Loss: 0.1613\n",
      "Epoch 14/100 | Step 16101/20074 | Loss: 1.1024\n",
      "Epoch 14/100 | Step 16201/20074 | Loss: 0.0005\n",
      "Epoch 14/100 | Step 16301/20074 | Loss: 0.0285\n",
      "Epoch 14/100 | Step 16401/20074 | Loss: 0.3978\n",
      "Epoch 14/100 | Step 16501/20074 | Loss: 1.4531\n",
      "Epoch 14/100 | Step 16601/20074 | Loss: 0.4258\n",
      "Epoch 14/100 | Step 16701/20074 | Loss: 1.1267\n",
      "Epoch 14/100 | Step 16801/20074 | Loss: 1.8188\n",
      "Epoch 14/100 | Step 16901/20074 | Loss: 0.2479\n",
      "Epoch 14/100 | Step 17001/20074 | Loss: 0.0599\n",
      "Epoch 14/100 | Step 17101/20074 | Loss: 0.0284\n",
      "Epoch 14/100 | Step 17201/20074 | Loss: 3.5499\n",
      "Epoch 14/100 | Step 17301/20074 | Loss: 1.0950\n",
      "Epoch 14/100 | Step 17401/20074 | Loss: 0.0036\n",
      "Epoch 14/100 | Step 17501/20074 | Loss: 0.3478\n",
      "Epoch 14/100 | Step 17601/20074 | Loss: 0.0896\n",
      "Epoch 14/100 | Step 17701/20074 | Loss: 0.4617\n",
      "Epoch 14/100 | Step 17801/20074 | Loss: 0.2391\n",
      "Epoch 14/100 | Step 17901/20074 | Loss: 0.3869\n",
      "Epoch 14/100 | Step 18001/20074 | Loss: 0.0070\n",
      "Epoch 14/100 | Step 18101/20074 | Loss: 0.7453\n",
      "Epoch 14/100 | Step 18201/20074 | Loss: 0.3856\n",
      "Epoch 14/100 | Step 18301/20074 | Loss: 3.1730\n",
      "Epoch 14/100 | Step 18401/20074 | Loss: 1.2994\n",
      "Epoch 14/100 | Step 18501/20074 | Loss: 0.6516\n",
      "Epoch 14/100 | Step 18601/20074 | Loss: 0.0010\n",
      "Epoch 14/100 | Step 18701/20074 | Loss: 0.1431\n",
      "Epoch 14/100 | Step 18801/20074 | Loss: 0.0683\n",
      "Epoch 14/100 | Step 18901/20074 | Loss: 0.2005\n",
      "Epoch 14/100 | Step 19001/20074 | Loss: 1.0459\n",
      "Epoch 14/100 | Step 19101/20074 | Loss: 0.0530\n",
      "Epoch 14/100 | Step 19201/20074 | Loss: 0.5629\n",
      "Epoch 14/100 | Step 19301/20074 | Loss: 0.0003\n",
      "Epoch 14/100 | Step 19401/20074 | Loss: 2.8209\n",
      "Epoch 14/100 | Step 19501/20074 | Loss: 0.8909\n",
      "Epoch 14/100 | Step 19601/20074 | Loss: 0.0700\n",
      "Epoch 14/100 | Step 19701/20074 | Loss: 2.7759\n",
      "Epoch 14/100 | Step 19801/20074 | Loss: 0.6902\n",
      "Epoch 14/100 | Step 19901/20074 | Loss: 0.4732\n",
      "Epoch 14/100 | Step 20001/20074 | Loss: 0.5383\n",
      "Epoch 15/100 | Step 1/20074 | Loss: 0.0082\n",
      "Epoch 15/100 | Step 101/20074 | Loss: 1.0379\n",
      "Epoch 15/100 | Step 201/20074 | Loss: 0.5431\n",
      "Epoch 15/100 | Step 301/20074 | Loss: 0.0044\n",
      "Epoch 15/100 | Step 401/20074 | Loss: 0.9665\n",
      "Epoch 15/100 | Step 501/20074 | Loss: 0.2122\n",
      "Epoch 15/100 | Step 601/20074 | Loss: 0.4324\n",
      "Epoch 15/100 | Step 701/20074 | Loss: 2.1954\n",
      "Epoch 15/100 | Step 801/20074 | Loss: 0.0004\n",
      "Epoch 15/100 | Step 901/20074 | Loss: 2.7373\n",
      "Epoch 15/100 | Step 1001/20074 | Loss: 4.5064\n",
      "Epoch 15/100 | Step 1101/20074 | Loss: 0.0068\n",
      "Epoch 15/100 | Step 1201/20074 | Loss: 0.0134\n",
      "Epoch 15/100 | Step 1301/20074 | Loss: 2.9841\n",
      "Epoch 15/100 | Step 1401/20074 | Loss: 0.8130\n",
      "Epoch 15/100 | Step 1501/20074 | Loss: 3.4591\n",
      "Epoch 15/100 | Step 1601/20074 | Loss: 0.2991\n",
      "Epoch 15/100 | Step 1701/20074 | Loss: 0.6323\n",
      "Epoch 15/100 | Step 1801/20074 | Loss: 0.4951\n",
      "Epoch 15/100 | Step 1901/20074 | Loss: 0.3341\n",
      "Epoch 15/100 | Step 2001/20074 | Loss: 3.0070\n",
      "Epoch 15/100 | Step 2101/20074 | Loss: 0.0009\n",
      "Epoch 15/100 | Step 2201/20074 | Loss: 1.6910\n",
      "Epoch 15/100 | Step 2301/20074 | Loss: 0.0132\n",
      "Epoch 15/100 | Step 2401/20074 | Loss: 0.3356\n",
      "Epoch 15/100 | Step 2501/20074 | Loss: 0.0415\n",
      "Epoch 15/100 | Step 2601/20074 | Loss: 0.3773\n",
      "Epoch 15/100 | Step 2701/20074 | Loss: 2.4632\n",
      "Epoch 15/100 | Step 2801/20074 | Loss: 0.0001\n",
      "Epoch 15/100 | Step 2901/20074 | Loss: 0.0009\n",
      "Epoch 15/100 | Step 3001/20074 | Loss: 0.2984\n",
      "Epoch 15/100 | Step 3101/20074 | Loss: 0.4901\n",
      "Epoch 15/100 | Step 3201/20074 | Loss: 2.7250\n",
      "Epoch 15/100 | Step 3301/20074 | Loss: 0.1040\n",
      "Epoch 15/100 | Step 3401/20074 | Loss: 3.9939\n",
      "Epoch 15/100 | Step 3501/20074 | Loss: 0.0025\n",
      "Epoch 15/100 | Step 3601/20074 | Loss: 0.0032\n",
      "Epoch 15/100 | Step 3701/20074 | Loss: 0.9049\n",
      "Epoch 15/100 | Step 3801/20074 | Loss: 0.0013\n",
      "Epoch 15/100 | Step 3901/20074 | Loss: 1.7532\n",
      "Epoch 15/100 | Step 4001/20074 | Loss: 0.5804\n",
      "Epoch 15/100 | Step 4101/20074 | Loss: 0.2924\n",
      "Epoch 15/100 | Step 4201/20074 | Loss: 0.0067\n",
      "Epoch 15/100 | Step 4301/20074 | Loss: 1.8139\n",
      "Epoch 15/100 | Step 4401/20074 | Loss: 0.1885\n",
      "Epoch 15/100 | Step 4501/20074 | Loss: 0.4866\n",
      "Epoch 15/100 | Step 4601/20074 | Loss: 0.1086\n",
      "Epoch 15/100 | Step 4701/20074 | Loss: 0.0033\n",
      "Epoch 15/100 | Step 4801/20074 | Loss: 0.0091\n",
      "Epoch 15/100 | Step 4901/20074 | Loss: 0.0251\n",
      "Epoch 15/100 | Step 5001/20074 | Loss: 0.0092\n",
      "Epoch 15/100 | Step 5101/20074 | Loss: 0.0031\n",
      "Epoch 15/100 | Step 5201/20074 | Loss: 0.0217\n",
      "Epoch 15/100 | Step 5301/20074 | Loss: 0.0589\n",
      "Epoch 15/100 | Step 5401/20074 | Loss: 2.1237\n",
      "Epoch 15/100 | Step 5501/20074 | Loss: 0.1956\n",
      "Epoch 15/100 | Step 5601/20074 | Loss: 0.0017\n",
      "Epoch 15/100 | Step 5701/20074 | Loss: 0.7585\n",
      "Epoch 15/100 | Step 5801/20074 | Loss: 0.0046\n",
      "Epoch 15/100 | Step 5901/20074 | Loss: 2.5278\n",
      "Epoch 15/100 | Step 6001/20074 | Loss: 0.0004\n",
      "Epoch 15/100 | Step 6101/20074 | Loss: 0.1610\n",
      "Epoch 15/100 | Step 6201/20074 | Loss: 0.2955\n",
      "Epoch 15/100 | Step 6301/20074 | Loss: 1.8329\n",
      "Epoch 15/100 | Step 6401/20074 | Loss: 3.0103\n",
      "Epoch 15/100 | Step 6501/20074 | Loss: 0.1834\n",
      "Epoch 15/100 | Step 6601/20074 | Loss: 0.0537\n",
      "Epoch 15/100 | Step 6701/20074 | Loss: 0.2952\n",
      "Epoch 15/100 | Step 6801/20074 | Loss: 1.1778\n",
      "Epoch 15/100 | Step 6901/20074 | Loss: 0.4926\n",
      "Epoch 15/100 | Step 7001/20074 | Loss: 0.4871\n",
      "Epoch 15/100 | Step 7101/20074 | Loss: 1.0601\n",
      "Epoch 15/100 | Step 7201/20074 | Loss: 0.0065\n",
      "Epoch 15/100 | Step 7301/20074 | Loss: 0.0006\n",
      "Epoch 15/100 | Step 7401/20074 | Loss: 0.1339\n",
      "Epoch 15/100 | Step 7501/20074 | Loss: 3.5045\n",
      "Epoch 15/100 | Step 7601/20074 | Loss: 0.8266\n",
      "Epoch 15/100 | Step 7701/20074 | Loss: 0.4190\n",
      "Epoch 15/100 | Step 7801/20074 | Loss: 0.0191\n",
      "Epoch 15/100 | Step 7901/20074 | Loss: 0.0058\n",
      "Epoch 15/100 | Step 8001/20074 | Loss: 1.0095\n",
      "Epoch 15/100 | Step 8101/20074 | Loss: 4.2639\n",
      "Epoch 15/100 | Step 8201/20074 | Loss: 3.5466\n",
      "Epoch 15/100 | Step 8301/20074 | Loss: 0.0551\n",
      "Epoch 15/100 | Step 8401/20074 | Loss: 2.4674\n",
      "Epoch 15/100 | Step 8501/20074 | Loss: 0.2066\n",
      "Epoch 15/100 | Step 8601/20074 | Loss: 0.0142\n",
      "Epoch 15/100 | Step 8701/20074 | Loss: 1.7595\n",
      "Epoch 15/100 | Step 8801/20074 | Loss: 1.2242\n",
      "Epoch 15/100 | Step 8901/20074 | Loss: 0.6743\n",
      "Epoch 15/100 | Step 9001/20074 | Loss: 3.2341\n",
      "Epoch 15/100 | Step 9101/20074 | Loss: 0.7591\n",
      "Epoch 15/100 | Step 9201/20074 | Loss: 2.2509\n",
      "Epoch 15/100 | Step 9301/20074 | Loss: 0.8316\n",
      "Epoch 15/100 | Step 9401/20074 | Loss: 1.1644\n",
      "Epoch 15/100 | Step 9501/20074 | Loss: 0.1324\n",
      "Epoch 15/100 | Step 9601/20074 | Loss: 0.0880\n",
      "Epoch 15/100 | Step 9701/20074 | Loss: 0.1190\n",
      "Epoch 15/100 | Step 9801/20074 | Loss: 0.4060\n",
      "Epoch 15/100 | Step 9901/20074 | Loss: 1.6869\n",
      "Epoch 15/100 | Step 10001/20074 | Loss: 0.0087\n",
      "Epoch 15/100 | Step 10101/20074 | Loss: 0.3127\n",
      "Epoch 15/100 | Step 10201/20074 | Loss: 0.0510\n",
      "Epoch 15/100 | Step 10301/20074 | Loss: 0.0014\n",
      "Epoch 15/100 | Step 10401/20074 | Loss: 1.8049\n",
      "Epoch 15/100 | Step 10501/20074 | Loss: 0.0012\n",
      "Epoch 15/100 | Step 10601/20074 | Loss: 0.0002\n",
      "Epoch 15/100 | Step 10701/20074 | Loss: 0.0105\n",
      "Epoch 15/100 | Step 10801/20074 | Loss: 1.4296\n",
      "Epoch 15/100 | Step 10901/20074 | Loss: 3.6150\n",
      "Epoch 15/100 | Step 11001/20074 | Loss: 0.1092\n",
      "Epoch 15/100 | Step 11101/20074 | Loss: 0.2189\n",
      "Epoch 15/100 | Step 11201/20074 | Loss: 0.7949\n",
      "Epoch 15/100 | Step 11301/20074 | Loss: 0.4111\n",
      "Epoch 15/100 | Step 11401/20074 | Loss: 0.1334\n",
      "Epoch 15/100 | Step 11501/20074 | Loss: 1.2443\n",
      "Epoch 15/100 | Step 11601/20074 | Loss: 4.1819\n",
      "Epoch 15/100 | Step 11701/20074 | Loss: 0.8758\n",
      "Epoch 15/100 | Step 11801/20074 | Loss: 0.1317\n",
      "Epoch 15/100 | Step 11901/20074 | Loss: 0.0270\n",
      "Epoch 15/100 | Step 12001/20074 | Loss: 0.0020\n",
      "Epoch 15/100 | Step 12101/20074 | Loss: 0.9044\n",
      "Epoch 15/100 | Step 12201/20074 | Loss: 3.0023\n",
      "Epoch 15/100 | Step 12301/20074 | Loss: 3.3619\n",
      "Epoch 15/100 | Step 12401/20074 | Loss: 2.1514\n",
      "Epoch 15/100 | Step 12501/20074 | Loss: 0.0216\n",
      "Epoch 15/100 | Step 12601/20074 | Loss: 1.0693\n",
      "Epoch 15/100 | Step 12701/20074 | Loss: 0.4385\n",
      "Epoch 15/100 | Step 12801/20074 | Loss: 0.4491\n",
      "Epoch 15/100 | Step 12901/20074 | Loss: 0.0108\n",
      "Epoch 15/100 | Step 13001/20074 | Loss: 0.0065\n",
      "Epoch 15/100 | Step 13101/20074 | Loss: 1.5747\n",
      "Epoch 15/100 | Step 13201/20074 | Loss: 0.0011\n",
      "Epoch 15/100 | Step 13301/20074 | Loss: 0.0001\n",
      "Epoch 15/100 | Step 13401/20074 | Loss: 0.0013\n",
      "Epoch 15/100 | Step 13501/20074 | Loss: 0.3753\n",
      "Epoch 15/100 | Step 13601/20074 | Loss: 0.0976\n",
      "Epoch 15/100 | Step 13701/20074 | Loss: 0.0834\n",
      "Epoch 15/100 | Step 13801/20074 | Loss: 2.6620\n",
      "Epoch 15/100 | Step 13901/20074 | Loss: 0.0157\n",
      "Epoch 15/100 | Step 14001/20074 | Loss: 0.1085\n",
      "Epoch 15/100 | Step 14101/20074 | Loss: 0.0009\n",
      "Epoch 15/100 | Step 14201/20074 | Loss: 0.0027\n",
      "Epoch 15/100 | Step 14301/20074 | Loss: 1.0819\n",
      "Epoch 15/100 | Step 14401/20074 | Loss: 3.6161\n",
      "Epoch 15/100 | Step 14501/20074 | Loss: 0.6035\n",
      "Epoch 15/100 | Step 14601/20074 | Loss: 0.5763\n",
      "Epoch 15/100 | Step 14701/20074 | Loss: 0.0017\n",
      "Epoch 15/100 | Step 14801/20074 | Loss: 0.0074\n",
      "Epoch 15/100 | Step 14901/20074 | Loss: 0.2293\n",
      "Epoch 15/100 | Step 15001/20074 | Loss: 0.0644\n",
      "Epoch 15/100 | Step 15101/20074 | Loss: 0.0223\n",
      "Epoch 15/100 | Step 15201/20074 | Loss: 0.3823\n",
      "Epoch 15/100 | Step 15301/20074 | Loss: 0.5335\n",
      "Epoch 15/100 | Step 15401/20074 | Loss: 0.1877\n",
      "Epoch 15/100 | Step 15501/20074 | Loss: 1.0324\n",
      "Epoch 15/100 | Step 15601/20074 | Loss: 0.5126\n",
      "Epoch 15/100 | Step 15701/20074 | Loss: 2.2918\n",
      "Epoch 15/100 | Step 15801/20074 | Loss: 0.0404\n",
      "Epoch 15/100 | Step 15901/20074 | Loss: 0.1288\n",
      "Epoch 15/100 | Step 16001/20074 | Loss: 0.0017\n",
      "Epoch 15/100 | Step 16101/20074 | Loss: 0.0818\n",
      "Epoch 15/100 | Step 16201/20074 | Loss: 0.1609\n",
      "Epoch 15/100 | Step 16301/20074 | Loss: 0.0008\n",
      "Epoch 15/100 | Step 16401/20074 | Loss: 0.7324\n",
      "Epoch 15/100 | Step 16501/20074 | Loss: 0.5186\n",
      "Epoch 15/100 | Step 16601/20074 | Loss: 0.0045\n",
      "Epoch 15/100 | Step 16701/20074 | Loss: 3.1116\n",
      "Epoch 15/100 | Step 16801/20074 | Loss: 0.6485\n",
      "Epoch 15/100 | Step 16901/20074 | Loss: 0.0924\n",
      "Epoch 15/100 | Step 17001/20074 | Loss: 0.0051\n",
      "Epoch 15/100 | Step 17101/20074 | Loss: 0.6301\n",
      "Epoch 15/100 | Step 17201/20074 | Loss: 0.0446\n",
      "Epoch 15/100 | Step 17301/20074 | Loss: 0.0098\n",
      "Epoch 15/100 | Step 17401/20074 | Loss: 0.0721\n",
      "Epoch 15/100 | Step 17501/20074 | Loss: 0.0101\n",
      "Epoch 15/100 | Step 17601/20074 | Loss: 0.0001\n",
      "Epoch 15/100 | Step 17701/20074 | Loss: 0.5925\n",
      "Epoch 15/100 | Step 17801/20074 | Loss: 0.0629\n",
      "Epoch 15/100 | Step 17901/20074 | Loss: 0.4048\n",
      "Epoch 15/100 | Step 18001/20074 | Loss: 0.0250\n",
      "Epoch 15/100 | Step 18101/20074 | Loss: 0.7910\n",
      "Epoch 15/100 | Step 18201/20074 | Loss: 0.1691\n",
      "Epoch 15/100 | Step 18301/20074 | Loss: 0.0049\n",
      "Epoch 15/100 | Step 18401/20074 | Loss: 0.0356\n",
      "Epoch 15/100 | Step 18501/20074 | Loss: 0.0024\n",
      "Epoch 15/100 | Step 18601/20074 | Loss: 0.1685\n",
      "Epoch 15/100 | Step 18701/20074 | Loss: 0.8546\n",
      "Epoch 15/100 | Step 18801/20074 | Loss: 0.0067\n",
      "Epoch 15/100 | Step 18901/20074 | Loss: 0.0010\n",
      "Epoch 15/100 | Step 19001/20074 | Loss: 2.1848\n",
      "Epoch 15/100 | Step 19101/20074 | Loss: 3.1629\n",
      "Epoch 15/100 | Step 19201/20074 | Loss: 0.0296\n",
      "Epoch 15/100 | Step 19301/20074 | Loss: 0.3968\n",
      "Epoch 15/100 | Step 19401/20074 | Loss: 0.3535\n",
      "Epoch 15/100 | Step 19501/20074 | Loss: 0.0023\n",
      "Epoch 15/100 | Step 19601/20074 | Loss: 4.4070\n",
      "Epoch 15/100 | Step 19701/20074 | Loss: 0.2899\n",
      "Epoch 15/100 | Step 19801/20074 | Loss: 0.0035\n",
      "Epoch 15/100 | Step 19901/20074 | Loss: 0.0811\n",
      "Epoch 15/100 | Step 20001/20074 | Loss: 2.0805\n",
      "Epoch 16/100 | Step 1/20074 | Loss: 3.2211\n",
      "Epoch 16/100 | Step 101/20074 | Loss: 0.4565\n",
      "Epoch 16/100 | Step 201/20074 | Loss: 0.0109\n",
      "Epoch 16/100 | Step 301/20074 | Loss: 2.1322\n",
      "Epoch 16/100 | Step 401/20074 | Loss: 2.1450\n",
      "Epoch 16/100 | Step 501/20074 | Loss: 2.1941\n",
      "Epoch 16/100 | Step 601/20074 | Loss: 0.9444\n",
      "Epoch 16/100 | Step 701/20074 | Loss: 0.0024\n",
      "Epoch 16/100 | Step 801/20074 | Loss: 0.2850\n",
      "Epoch 16/100 | Step 901/20074 | Loss: 4.3000\n",
      "Epoch 16/100 | Step 1001/20074 | Loss: 0.0001\n",
      "Epoch 16/100 | Step 1101/20074 | Loss: 0.2898\n",
      "Epoch 16/100 | Step 1201/20074 | Loss: 1.3104\n",
      "Epoch 16/100 | Step 1301/20074 | Loss: 0.3119\n",
      "Epoch 16/100 | Step 1401/20074 | Loss: 0.8908\n",
      "Epoch 16/100 | Step 1501/20074 | Loss: 0.3606\n",
      "Epoch 16/100 | Step 1601/20074 | Loss: 0.0004\n",
      "Epoch 16/100 | Step 1701/20074 | Loss: 0.5628\n",
      "Epoch 16/100 | Step 1801/20074 | Loss: 0.1869\n",
      "Epoch 16/100 | Step 1901/20074 | Loss: 0.0231\n",
      "Epoch 16/100 | Step 2001/20074 | Loss: 0.0036\n",
      "Epoch 16/100 | Step 2101/20074 | Loss: 3.0746\n",
      "Epoch 16/100 | Step 2201/20074 | Loss: 1.7975\n",
      "Epoch 16/100 | Step 2301/20074 | Loss: 0.0003\n",
      "Epoch 16/100 | Step 2401/20074 | Loss: 0.0491\n",
      "Epoch 16/100 | Step 2501/20074 | Loss: 1.8188\n",
      "Epoch 16/100 | Step 2601/20074 | Loss: 0.0005\n",
      "Epoch 16/100 | Step 2701/20074 | Loss: 0.5018\n",
      "Epoch 16/100 | Step 2801/20074 | Loss: 0.0004\n",
      "Epoch 16/100 | Step 2901/20074 | Loss: 0.1528\n",
      "Epoch 16/100 | Step 3001/20074 | Loss: 3.8204\n",
      "Epoch 16/100 | Step 3101/20074 | Loss: 1.3064\n",
      "Epoch 16/100 | Step 3201/20074 | Loss: 0.0088\n",
      "Epoch 16/100 | Step 3301/20074 | Loss: 0.6485\n",
      "Epoch 16/100 | Step 3401/20074 | Loss: 0.0006\n",
      "Epoch 16/100 | Step 3501/20074 | Loss: 0.0282\n",
      "Epoch 16/100 | Step 3601/20074 | Loss: 1.1768\n",
      "Epoch 16/100 | Step 3701/20074 | Loss: 0.0005\n",
      "Epoch 16/100 | Step 3801/20074 | Loss: 0.0008\n",
      "Epoch 16/100 | Step 3901/20074 | Loss: 0.0060\n",
      "Epoch 16/100 | Step 4001/20074 | Loss: 0.0055\n",
      "Epoch 16/100 | Step 4101/20074 | Loss: 0.1221\n",
      "Epoch 16/100 | Step 4201/20074 | Loss: 0.0402\n",
      "Epoch 16/100 | Step 4301/20074 | Loss: 4.4179\n",
      "Epoch 16/100 | Step 4401/20074 | Loss: 2.1167\n",
      "Epoch 16/100 | Step 4501/20074 | Loss: 0.0307\n",
      "Epoch 16/100 | Step 4601/20074 | Loss: 0.0031\n",
      "Epoch 16/100 | Step 4701/20074 | Loss: 1.8987\n",
      "Epoch 16/100 | Step 4801/20074 | Loss: 0.0459\n",
      "Epoch 16/100 | Step 4901/20074 | Loss: 0.0025\n",
      "Epoch 16/100 | Step 5001/20074 | Loss: 0.0403\n",
      "Epoch 16/100 | Step 5101/20074 | Loss: 0.0566\n",
      "Epoch 16/100 | Step 5201/20074 | Loss: 0.0216\n",
      "Epoch 16/100 | Step 5301/20074 | Loss: 0.0033\n",
      "Epoch 16/100 | Step 5401/20074 | Loss: 4.7859\n",
      "Epoch 16/100 | Step 5501/20074 | Loss: 1.1304\n",
      "Epoch 16/100 | Step 5601/20074 | Loss: 1.5234\n",
      "Epoch 16/100 | Step 5701/20074 | Loss: 0.2642\n",
      "Epoch 16/100 | Step 5801/20074 | Loss: 0.6871\n",
      "Epoch 16/100 | Step 5901/20074 | Loss: 0.3407\n",
      "Epoch 16/100 | Step 6001/20074 | Loss: 0.1411\n",
      "Epoch 16/100 | Step 6101/20074 | Loss: 0.0051\n",
      "Epoch 16/100 | Step 6201/20074 | Loss: 0.0001\n",
      "Epoch 16/100 | Step 6301/20074 | Loss: 0.3112\n",
      "Epoch 16/100 | Step 6401/20074 | Loss: 2.6710\n",
      "Epoch 16/100 | Step 6501/20074 | Loss: 0.2092\n",
      "Epoch 16/100 | Step 6601/20074 | Loss: 2.0842\n",
      "Epoch 16/100 | Step 6701/20074 | Loss: 0.0023\n",
      "Epoch 16/100 | Step 6801/20074 | Loss: 0.0426\n",
      "Epoch 16/100 | Step 6901/20074 | Loss: 1.2970\n",
      "Epoch 16/100 | Step 7001/20074 | Loss: 0.4524\n",
      "Epoch 16/100 | Step 7101/20074 | Loss: 0.2320\n",
      "Epoch 16/100 | Step 7201/20074 | Loss: 0.3647\n",
      "Epoch 16/100 | Step 7301/20074 | Loss: 0.0013\n",
      "Epoch 16/100 | Step 7401/20074 | Loss: 0.1942\n",
      "Epoch 16/100 | Step 7501/20074 | Loss: 0.0044\n",
      "Epoch 16/100 | Step 7601/20074 | Loss: 0.8112\n",
      "Epoch 16/100 | Step 7701/20074 | Loss: 0.4557\n",
      "Epoch 16/100 | Step 7801/20074 | Loss: 0.0494\n",
      "Epoch 16/100 | Step 7901/20074 | Loss: 0.0578\n",
      "Epoch 16/100 | Step 8001/20074 | Loss: 0.0012\n",
      "Epoch 16/100 | Step 8101/20074 | Loss: 0.0051\n",
      "Epoch 16/100 | Step 8201/20074 | Loss: 2.8457\n",
      "Epoch 16/100 | Step 8301/20074 | Loss: 0.2021\n",
      "Epoch 16/100 | Step 8401/20074 | Loss: 2.0845\n",
      "Epoch 16/100 | Step 8501/20074 | Loss: 3.1577\n",
      "Epoch 16/100 | Step 8601/20074 | Loss: 0.0383\n",
      "Epoch 16/100 | Step 8701/20074 | Loss: 0.0009\n",
      "Epoch 16/100 | Step 8801/20074 | Loss: 1.9121\n",
      "Epoch 16/100 | Step 8901/20074 | Loss: 0.8678\n",
      "Epoch 16/100 | Step 9001/20074 | Loss: 0.0002\n",
      "Epoch 16/100 | Step 9101/20074 | Loss: 0.0832\n",
      "Epoch 16/100 | Step 9201/20074 | Loss: 0.0026\n",
      "Epoch 16/100 | Step 9301/20074 | Loss: 2.1437\n",
      "Epoch 16/100 | Step 9401/20074 | Loss: 2.7101\n",
      "Epoch 16/100 | Step 9501/20074 | Loss: 0.5002\n",
      "Epoch 16/100 | Step 9601/20074 | Loss: 0.0669\n",
      "Epoch 16/100 | Step 9701/20074 | Loss: 0.0002\n",
      "Epoch 16/100 | Step 9801/20074 | Loss: 0.0018\n",
      "Epoch 16/100 | Step 9901/20074 | Loss: 1.2182\n",
      "Epoch 16/100 | Step 10001/20074 | Loss: 0.0018\n",
      "Epoch 16/100 | Step 10101/20074 | Loss: 0.7902\n",
      "Epoch 16/100 | Step 10201/20074 | Loss: 0.5578\n",
      "Epoch 16/100 | Step 10301/20074 | Loss: 0.6220\n",
      "Epoch 16/100 | Step 10401/20074 | Loss: 1.0456\n",
      "Epoch 16/100 | Step 10501/20074 | Loss: 3.4852\n",
      "Epoch 16/100 | Step 10601/20074 | Loss: 1.5105\n",
      "Epoch 16/100 | Step 10701/20074 | Loss: 0.5521\n",
      "Epoch 16/100 | Step 10801/20074 | Loss: 0.7587\n",
      "Epoch 16/100 | Step 10901/20074 | Loss: 0.4730\n",
      "Epoch 16/100 | Step 11001/20074 | Loss: 0.7501\n",
      "Epoch 16/100 | Step 11101/20074 | Loss: 1.5851\n",
      "Epoch 16/100 | Step 11201/20074 | Loss: 0.2780\n",
      "Epoch 16/100 | Step 11301/20074 | Loss: 0.0759\n",
      "Epoch 16/100 | Step 11401/20074 | Loss: 0.0170\n",
      "Epoch 16/100 | Step 11501/20074 | Loss: 0.2003\n",
      "Epoch 16/100 | Step 11601/20074 | Loss: 0.0012\n",
      "Epoch 16/100 | Step 11701/20074 | Loss: 0.4055\n",
      "Epoch 16/100 | Step 11801/20074 | Loss: 0.2025\n",
      "Epoch 16/100 | Step 11901/20074 | Loss: 0.7189\n",
      "Epoch 16/100 | Step 12001/20074 | Loss: 0.3102\n",
      "Epoch 16/100 | Step 12101/20074 | Loss: 0.0002\n",
      "Epoch 16/100 | Step 12201/20074 | Loss: 0.2842\n",
      "Epoch 16/100 | Step 12301/20074 | Loss: 3.4838\n",
      "Epoch 16/100 | Step 12401/20074 | Loss: 1.1753\n",
      "Epoch 16/100 | Step 12501/20074 | Loss: 0.0010\n",
      "Epoch 16/100 | Step 12601/20074 | Loss: 3.5640\n",
      "Epoch 16/100 | Step 12701/20074 | Loss: 4.1881\n",
      "Epoch 16/100 | Step 12801/20074 | Loss: 0.3473\n",
      "Epoch 16/100 | Step 12901/20074 | Loss: 0.2200\n",
      "Epoch 16/100 | Step 13001/20074 | Loss: 1.5637\n",
      "Epoch 16/100 | Step 13101/20074 | Loss: 0.4599\n",
      "Epoch 16/100 | Step 13201/20074 | Loss: 0.0492\n",
      "Epoch 16/100 | Step 13301/20074 | Loss: 0.0683\n",
      "Epoch 16/100 | Step 13401/20074 | Loss: 0.0041\n",
      "Epoch 16/100 | Step 13501/20074 | Loss: 0.8566\n",
      "Epoch 16/100 | Step 13601/20074 | Loss: 0.0020\n",
      "Epoch 16/100 | Step 13701/20074 | Loss: 0.8182\n",
      "Epoch 16/100 | Step 13801/20074 | Loss: 0.1536\n",
      "Epoch 16/100 | Step 13901/20074 | Loss: 0.3325\n",
      "Epoch 16/100 | Step 14001/20074 | Loss: 0.4260\n",
      "Epoch 16/100 | Step 14101/20074 | Loss: 0.8835\n",
      "Epoch 16/100 | Step 14201/20074 | Loss: 0.4017\n",
      "Epoch 16/100 | Step 14301/20074 | Loss: 2.3476\n",
      "Epoch 16/100 | Step 14401/20074 | Loss: 0.4587\n",
      "Epoch 16/100 | Step 14501/20074 | Loss: 0.0001\n",
      "Epoch 16/100 | Step 14601/20074 | Loss: 0.0793\n",
      "Epoch 16/100 | Step 14701/20074 | Loss: 5.5917\n",
      "Epoch 16/100 | Step 14801/20074 | Loss: 0.0002\n",
      "Epoch 16/100 | Step 14901/20074 | Loss: 0.6028\n",
      "Epoch 16/100 | Step 15001/20074 | Loss: 0.2735\n",
      "Epoch 16/100 | Step 15101/20074 | Loss: 1.5219\n",
      "Epoch 16/100 | Step 15201/20074 | Loss: 0.1904\n",
      "Epoch 16/100 | Step 15301/20074 | Loss: 0.1524\n",
      "Epoch 16/100 | Step 15401/20074 | Loss: 0.0248\n",
      "Epoch 16/100 | Step 15501/20074 | Loss: 0.0024\n",
      "Epoch 16/100 | Step 15601/20074 | Loss: 0.0023\n",
      "Epoch 16/100 | Step 15701/20074 | Loss: 0.0001\n",
      "Epoch 16/100 | Step 15801/20074 | Loss: 0.4647\n",
      "Epoch 16/100 | Step 15901/20074 | Loss: 0.4844\n",
      "Epoch 16/100 | Step 16001/20074 | Loss: 0.2342\n",
      "Epoch 16/100 | Step 16101/20074 | Loss: 0.0060\n",
      "Epoch 16/100 | Step 16201/20074 | Loss: 0.6256\n",
      "Epoch 16/100 | Step 16301/20074 | Loss: 0.5016\n",
      "Epoch 16/100 | Step 16401/20074 | Loss: 0.3674\n",
      "Epoch 16/100 | Step 16501/20074 | Loss: 2.0729\n",
      "Epoch 16/100 | Step 16601/20074 | Loss: 0.0955\n",
      "Epoch 16/100 | Step 16701/20074 | Loss: 0.3221\n",
      "Epoch 16/100 | Step 16801/20074 | Loss: 0.0538\n",
      "Epoch 16/100 | Step 16901/20074 | Loss: 1.3095\n",
      "Epoch 16/100 | Step 17001/20074 | Loss: 0.1283\n",
      "Epoch 16/100 | Step 17101/20074 | Loss: 0.0006\n",
      "Epoch 16/100 | Step 17201/20074 | Loss: 0.1253\n",
      "Epoch 16/100 | Step 17301/20074 | Loss: 1.6006\n",
      "Epoch 16/100 | Step 17401/20074 | Loss: 1.1146\n",
      "Epoch 16/100 | Step 17501/20074 | Loss: 0.6523\n",
      "Epoch 16/100 | Step 17601/20074 | Loss: 0.0005\n",
      "Epoch 16/100 | Step 17701/20074 | Loss: 0.0127\n",
      "Epoch 16/100 | Step 17801/20074 | Loss: 0.0095\n",
      "Epoch 16/100 | Step 17901/20074 | Loss: 1.7744\n",
      "Epoch 16/100 | Step 18001/20074 | Loss: 0.1428\n",
      "Epoch 16/100 | Step 18101/20074 | Loss: 0.0321\n",
      "Epoch 16/100 | Step 18201/20074 | Loss: 0.1860\n",
      "Epoch 16/100 | Step 18301/20074 | Loss: 0.6820\n",
      "Epoch 16/100 | Step 18401/20074 | Loss: 0.0021\n",
      "Epoch 16/100 | Step 18501/20074 | Loss: 0.3524\n",
      "Epoch 16/100 | Step 18601/20074 | Loss: 0.0022\n",
      "Epoch 16/100 | Step 18701/20074 | Loss: 0.4807\n",
      "Epoch 16/100 | Step 18801/20074 | Loss: 0.0028\n",
      "Epoch 16/100 | Step 18901/20074 | Loss: 0.0104\n",
      "Epoch 16/100 | Step 19001/20074 | Loss: 1.2781\n",
      "Epoch 16/100 | Step 19101/20074 | Loss: 0.0391\n",
      "Epoch 16/100 | Step 19201/20074 | Loss: 0.0071\n",
      "Epoch 16/100 | Step 19301/20074 | Loss: 0.3563\n",
      "Epoch 16/100 | Step 19401/20074 | Loss: 4.3832\n",
      "Epoch 16/100 | Step 19501/20074 | Loss: 0.0306\n",
      "Epoch 16/100 | Step 19601/20074 | Loss: 0.9388\n",
      "Epoch 16/100 | Step 19701/20074 | Loss: 2.0666\n",
      "Epoch 16/100 | Step 19801/20074 | Loss: 0.3666\n",
      "Epoch 16/100 | Step 19901/20074 | Loss: 0.0046\n",
      "Epoch 16/100 | Step 20001/20074 | Loss: 5.2791\n",
      "Epoch 17/100 | Step 1/20074 | Loss: 0.0010\n",
      "Epoch 17/100 | Step 101/20074 | Loss: 0.6870\n",
      "Epoch 17/100 | Step 201/20074 | Loss: 0.0022\n",
      "Epoch 17/100 | Step 301/20074 | Loss: 0.0139\n",
      "Epoch 17/100 | Step 401/20074 | Loss: 0.5876\n",
      "Epoch 17/100 | Step 501/20074 | Loss: 2.1151\n",
      "Epoch 17/100 | Step 601/20074 | Loss: 0.0070\n",
      "Epoch 17/100 | Step 701/20074 | Loss: 0.1052\n",
      "Epoch 17/100 | Step 801/20074 | Loss: 0.0012\n",
      "Epoch 17/100 | Step 901/20074 | Loss: 0.0032\n",
      "Epoch 17/100 | Step 1001/20074 | Loss: 0.0010\n",
      "Epoch 17/100 | Step 1101/20074 | Loss: 0.9733\n",
      "Epoch 17/100 | Step 1201/20074 | Loss: 0.0019\n",
      "Epoch 17/100 | Step 1301/20074 | Loss: 0.0231\n",
      "Epoch 17/100 | Step 1401/20074 | Loss: 0.4991\n",
      "Epoch 17/100 | Step 1501/20074 | Loss: 0.1250\n",
      "Epoch 17/100 | Step 1601/20074 | Loss: 0.0986\n",
      "Epoch 17/100 | Step 1701/20074 | Loss: 2.0048\n",
      "Epoch 17/100 | Step 1801/20074 | Loss: 0.5110\n",
      "Epoch 17/100 | Step 1901/20074 | Loss: 0.0723\n",
      "Epoch 17/100 | Step 2001/20074 | Loss: 0.0030\n",
      "Epoch 17/100 | Step 2101/20074 | Loss: 1.8539\n",
      "Epoch 17/100 | Step 2201/20074 | Loss: 0.0135\n",
      "Epoch 17/100 | Step 2301/20074 | Loss: 1.1370\n",
      "Epoch 17/100 | Step 2401/20074 | Loss: 0.5060\n",
      "Epoch 17/100 | Step 2501/20074 | Loss: 0.0059\n",
      "Epoch 17/100 | Step 2601/20074 | Loss: 0.3595\n",
      "Epoch 17/100 | Step 2701/20074 | Loss: 2.8028\n",
      "Epoch 17/100 | Step 2801/20074 | Loss: 0.8159\n",
      "Epoch 17/100 | Step 2901/20074 | Loss: 1.2342\n",
      "Epoch 17/100 | Step 3001/20074 | Loss: 0.0151\n",
      "Epoch 17/100 | Step 3101/20074 | Loss: 0.3151\n",
      "Epoch 17/100 | Step 3201/20074 | Loss: 0.6565\n",
      "Epoch 17/100 | Step 3301/20074 | Loss: 0.3697\n",
      "Epoch 17/100 | Step 3401/20074 | Loss: 0.1485\n",
      "Epoch 17/100 | Step 3501/20074 | Loss: 0.0900\n",
      "Epoch 17/100 | Step 3601/20074 | Loss: 0.2874\n",
      "Epoch 17/100 | Step 3701/20074 | Loss: 1.7969\n",
      "Epoch 17/100 | Step 3801/20074 | Loss: 2.0152\n",
      "Epoch 17/100 | Step 3901/20074 | Loss: 0.1112\n",
      "Epoch 17/100 | Step 4001/20074 | Loss: 1.0070\n",
      "Epoch 17/100 | Step 4101/20074 | Loss: 0.0403\n",
      "Epoch 17/100 | Step 4201/20074 | Loss: 0.3089\n",
      "Epoch 17/100 | Step 4301/20074 | Loss: 0.5233\n",
      "Epoch 17/100 | Step 4401/20074 | Loss: 0.2004\n",
      "Epoch 17/100 | Step 4501/20074 | Loss: 0.5097\n",
      "Epoch 17/100 | Step 4601/20074 | Loss: 0.0175\n",
      "Epoch 17/100 | Step 4701/20074 | Loss: 1.0079\n",
      "Epoch 17/100 | Step 4801/20074 | Loss: 0.5306\n",
      "Epoch 17/100 | Step 4901/20074 | Loss: 0.4871\n",
      "Epoch 17/100 | Step 5001/20074 | Loss: 1.3986\n",
      "Epoch 17/100 | Step 5101/20074 | Loss: 0.7663\n",
      "Epoch 17/100 | Step 5201/20074 | Loss: 2.1682\n",
      "Epoch 17/100 | Step 5301/20074 | Loss: 0.0003\n",
      "Epoch 17/100 | Step 5401/20074 | Loss: 3.1936\n",
      "Epoch 17/100 | Step 5501/20074 | Loss: 0.1248\n",
      "Epoch 17/100 | Step 5601/20074 | Loss: 3.9750\n",
      "Epoch 17/100 | Step 5701/20074 | Loss: 0.2563\n",
      "Epoch 17/100 | Step 5801/20074 | Loss: 4.9387\n",
      "Epoch 17/100 | Step 5901/20074 | Loss: 0.0243\n",
      "Epoch 17/100 | Step 6001/20074 | Loss: 1.4065\n",
      "Epoch 17/100 | Step 6101/20074 | Loss: 3.5579\n",
      "Epoch 17/100 | Step 6201/20074 | Loss: 0.9290\n",
      "Epoch 17/100 | Step 6301/20074 | Loss: 2.2824\n",
      "Epoch 17/100 | Step 6401/20074 | Loss: 0.4410\n",
      "Epoch 17/100 | Step 6501/20074 | Loss: 0.3283\n",
      "Epoch 17/100 | Step 6601/20074 | Loss: 0.7815\n",
      "Epoch 17/100 | Step 6701/20074 | Loss: 0.0131\n",
      "Epoch 17/100 | Step 6801/20074 | Loss: 0.0123\n",
      "Epoch 17/100 | Step 6901/20074 | Loss: 0.0332\n",
      "Epoch 17/100 | Step 7001/20074 | Loss: 0.1047\n",
      "Epoch 17/100 | Step 7101/20074 | Loss: 0.5315\n",
      "Epoch 17/100 | Step 7201/20074 | Loss: 0.0708\n",
      "Epoch 17/100 | Step 7301/20074 | Loss: 0.4124\n",
      "Epoch 17/100 | Step 7401/20074 | Loss: 1.9274\n",
      "Epoch 17/100 | Step 7501/20074 | Loss: 1.9517\n",
      "Epoch 17/100 | Step 7601/20074 | Loss: 0.3392\n",
      "Epoch 17/100 | Step 7701/20074 | Loss: 0.0828\n",
      "Epoch 17/100 | Step 7801/20074 | Loss: 0.0259\n",
      "Epoch 17/100 | Step 7901/20074 | Loss: 0.3295\n",
      "Epoch 17/100 | Step 8001/20074 | Loss: 2.5062\n",
      "Epoch 17/100 | Step 8101/20074 | Loss: 0.6592\n",
      "Epoch 17/100 | Step 8201/20074 | Loss: 0.1324\n",
      "Epoch 17/100 | Step 8301/20074 | Loss: 4.1947\n",
      "Epoch 17/100 | Step 8401/20074 | Loss: 0.3530\n",
      "Epoch 17/100 | Step 8501/20074 | Loss: 0.0078\n",
      "Epoch 17/100 | Step 8601/20074 | Loss: 1.6536\n",
      "Epoch 17/100 | Step 8701/20074 | Loss: 0.0863\n",
      "Epoch 17/100 | Step 8801/20074 | Loss: 0.0005\n",
      "Epoch 17/100 | Step 8901/20074 | Loss: 0.1472\n",
      "Epoch 17/100 | Step 9001/20074 | Loss: 0.0080\n",
      "Epoch 17/100 | Step 9101/20074 | Loss: 0.0132\n",
      "Epoch 17/100 | Step 9201/20074 | Loss: 0.2251\n",
      "Epoch 17/100 | Step 9301/20074 | Loss: 3.0598\n",
      "Epoch 17/100 | Step 9401/20074 | Loss: 1.2987\n",
      "Epoch 17/100 | Step 9501/20074 | Loss: 1.2061\n",
      "Epoch 17/100 | Step 9601/20074 | Loss: 1.8758\n",
      "Epoch 17/100 | Step 9701/20074 | Loss: 0.0092\n",
      "Epoch 17/100 | Step 9801/20074 | Loss: 1.8811\n",
      "Epoch 17/100 | Step 9901/20074 | Loss: 0.4359\n",
      "Epoch 17/100 | Step 10001/20074 | Loss: 0.0010\n",
      "Epoch 17/100 | Step 10101/20074 | Loss: 0.5071\n",
      "Epoch 17/100 | Step 10201/20074 | Loss: 0.3123\n",
      "Epoch 17/100 | Step 10301/20074 | Loss: 0.0004\n",
      "Epoch 17/100 | Step 10401/20074 | Loss: 0.4129\n",
      "Epoch 17/100 | Step 10501/20074 | Loss: 0.3858\n",
      "Epoch 17/100 | Step 10601/20074 | Loss: 0.0181\n",
      "Epoch 17/100 | Step 10701/20074 | Loss: 0.0879\n",
      "Epoch 17/100 | Step 10801/20074 | Loss: 1.2412\n",
      "Epoch 17/100 | Step 10901/20074 | Loss: 0.0007\n",
      "Epoch 17/100 | Step 11001/20074 | Loss: 0.5384\n",
      "Epoch 17/100 | Step 11101/20074 | Loss: 2.4872\n",
      "Epoch 17/100 | Step 11201/20074 | Loss: 0.0059\n",
      "Epoch 17/100 | Step 11301/20074 | Loss: 0.0625\n",
      "Epoch 17/100 | Step 11401/20074 | Loss: 0.6795\n",
      "Epoch 17/100 | Step 11501/20074 | Loss: 0.2542\n",
      "Epoch 17/100 | Step 11601/20074 | Loss: 1.7303\n",
      "Epoch 17/100 | Step 11701/20074 | Loss: 0.3994\n",
      "Epoch 17/100 | Step 11801/20074 | Loss: 0.0026\n",
      "Epoch 17/100 | Step 11901/20074 | Loss: 0.0021\n",
      "Epoch 17/100 | Step 12001/20074 | Loss: 0.0053\n",
      "Epoch 17/100 | Step 12101/20074 | Loss: 0.7909\n",
      "Epoch 17/100 | Step 12201/20074 | Loss: 0.0018\n",
      "Epoch 17/100 | Step 12301/20074 | Loss: 1.2725\n",
      "Epoch 17/100 | Step 12401/20074 | Loss: 0.3507\n",
      "Epoch 17/100 | Step 12501/20074 | Loss: 0.0010\n",
      "Epoch 17/100 | Step 12601/20074 | Loss: 0.4328\n",
      "Epoch 17/100 | Step 12701/20074 | Loss: 0.0222\n",
      "Epoch 17/100 | Step 12801/20074 | Loss: 0.0010\n",
      "Epoch 17/100 | Step 12901/20074 | Loss: 0.5784\n",
      "Epoch 17/100 | Step 13001/20074 | Loss: 0.0077\n",
      "Epoch 17/100 | Step 13101/20074 | Loss: 0.0042\n",
      "Epoch 17/100 | Step 13201/20074 | Loss: 1.6786\n",
      "Epoch 17/100 | Step 13301/20074 | Loss: 0.6472\n",
      "Epoch 17/100 | Step 13401/20074 | Loss: 2.9294\n",
      "Epoch 17/100 | Step 13501/20074 | Loss: 0.7146\n",
      "Epoch 17/100 | Step 13601/20074 | Loss: 0.1634\n",
      "Epoch 17/100 | Step 13701/20074 | Loss: 0.0875\n",
      "Epoch 17/100 | Step 13801/20074 | Loss: 0.1214\n",
      "Epoch 17/100 | Step 13901/20074 | Loss: 0.0073\n",
      "Epoch 17/100 | Step 14001/20074 | Loss: 0.6909\n",
      "Epoch 17/100 | Step 14101/20074 | Loss: 0.4196\n",
      "Epoch 17/100 | Step 14201/20074 | Loss: 1.1266\n",
      "Epoch 17/100 | Step 14301/20074 | Loss: 0.0240\n",
      "Epoch 17/100 | Step 14401/20074 | Loss: 0.0004\n",
      "Epoch 17/100 | Step 14501/20074 | Loss: 0.0006\n",
      "Epoch 17/100 | Step 14601/20074 | Loss: 1.6050\n",
      "Epoch 17/100 | Step 14701/20074 | Loss: 0.0020\n",
      "Epoch 17/100 | Step 14801/20074 | Loss: 0.0206\n",
      "Epoch 17/100 | Step 14901/20074 | Loss: 0.1154\n",
      "Epoch 17/100 | Step 15001/20074 | Loss: 0.1230\n",
      "Epoch 17/100 | Step 15101/20074 | Loss: 0.0941\n",
      "Epoch 17/100 | Step 15201/20074 | Loss: 0.0828\n",
      "Epoch 17/100 | Step 15301/20074 | Loss: 1.9299\n",
      "Epoch 17/100 | Step 15401/20074 | Loss: 3.6424\n",
      "Epoch 17/100 | Step 15501/20074 | Loss: 0.6604\n",
      "Epoch 17/100 | Step 15601/20074 | Loss: 0.0230\n",
      "Epoch 17/100 | Step 15701/20074 | Loss: 0.0061\n",
      "Epoch 17/100 | Step 15801/20074 | Loss: 2.3682\n",
      "Epoch 17/100 | Step 15901/20074 | Loss: 0.3346\n",
      "Epoch 17/100 | Step 16001/20074 | Loss: 0.0053\n",
      "Epoch 17/100 | Step 16101/20074 | Loss: 2.1371\n",
      "Epoch 17/100 | Step 16201/20074 | Loss: 0.0413\n",
      "Epoch 17/100 | Step 16301/20074 | Loss: 0.0023\n",
      "Epoch 17/100 | Step 16401/20074 | Loss: 0.0045\n",
      "Epoch 17/100 | Step 16501/20074 | Loss: 0.7992\n",
      "Epoch 17/100 | Step 16601/20074 | Loss: 1.6134\n",
      "Epoch 17/100 | Step 16701/20074 | Loss: 5.2707\n",
      "Epoch 17/100 | Step 16801/20074 | Loss: 0.0115\n",
      "Epoch 17/100 | Step 16901/20074 | Loss: 0.1972\n",
      "Epoch 17/100 | Step 17001/20074 | Loss: 4.3607\n",
      "Epoch 17/100 | Step 17101/20074 | Loss: 0.0001\n",
      "Epoch 17/100 | Step 17201/20074 | Loss: 4.0804\n",
      "Epoch 17/100 | Step 17301/20074 | Loss: 0.0002\n",
      "Epoch 17/100 | Step 17401/20074 | Loss: 1.0205\n",
      "Epoch 17/100 | Step 17501/20074 | Loss: 0.8293\n",
      "Epoch 17/100 | Step 17601/20074 | Loss: 1.8133\n",
      "Epoch 17/100 | Step 17701/20074 | Loss: 0.0122\n",
      "Epoch 17/100 | Step 17801/20074 | Loss: 1.4921\n",
      "Epoch 17/100 | Step 17901/20074 | Loss: 1.6266\n",
      "Epoch 17/100 | Step 18001/20074 | Loss: 0.5288\n",
      "Epoch 17/100 | Step 18101/20074 | Loss: 0.6941\n",
      "Epoch 17/100 | Step 18201/20074 | Loss: 0.8149\n",
      "Epoch 17/100 | Step 18301/20074 | Loss: 0.3573\n",
      "Epoch 17/100 | Step 18401/20074 | Loss: 0.0002\n",
      "Epoch 17/100 | Step 18501/20074 | Loss: 2.0644\n",
      "Epoch 17/100 | Step 18601/20074 | Loss: 4.8828\n",
      "Epoch 17/100 | Step 18701/20074 | Loss: 0.0032\n",
      "Epoch 17/100 | Step 18801/20074 | Loss: 0.7330\n",
      "Epoch 17/100 | Step 18901/20074 | Loss: 0.3213\n",
      "Epoch 17/100 | Step 19001/20074 | Loss: 0.0261\n",
      "Epoch 17/100 | Step 19101/20074 | Loss: 0.3130\n",
      "Epoch 17/100 | Step 19201/20074 | Loss: 0.3459\n",
      "Epoch 17/100 | Step 19301/20074 | Loss: 0.7208\n",
      "Epoch 17/100 | Step 19401/20074 | Loss: 0.0545\n",
      "Epoch 17/100 | Step 19501/20074 | Loss: 0.0247\n",
      "Epoch 17/100 | Step 19601/20074 | Loss: 0.3100\n",
      "Epoch 17/100 | Step 19701/20074 | Loss: 0.0083\n",
      "Epoch 17/100 | Step 19801/20074 | Loss: 4.5199\n",
      "Epoch 17/100 | Step 19901/20074 | Loss: 2.4161\n",
      "Epoch 17/100 | Step 20001/20074 | Loss: 3.7605\n",
      "Epoch 18/100 | Step 1/20074 | Loss: 1.4882\n",
      "Epoch 18/100 | Step 101/20074 | Loss: 0.0008\n",
      "Epoch 18/100 | Step 201/20074 | Loss: 0.0009\n",
      "Epoch 18/100 | Step 301/20074 | Loss: 4.1635\n",
      "Epoch 18/100 | Step 401/20074 | Loss: 0.0398\n",
      "Epoch 18/100 | Step 501/20074 | Loss: 0.4583\n",
      "Epoch 18/100 | Step 601/20074 | Loss: 0.0979\n",
      "Epoch 18/100 | Step 701/20074 | Loss: 0.0075\n",
      "Epoch 18/100 | Step 801/20074 | Loss: 2.4919\n",
      "Epoch 18/100 | Step 901/20074 | Loss: 0.0023\n",
      "Epoch 18/100 | Step 1001/20074 | Loss: 0.4610\n",
      "Epoch 18/100 | Step 1101/20074 | Loss: 0.0072\n",
      "Epoch 18/100 | Step 1201/20074 | Loss: 0.2648\n",
      "Epoch 18/100 | Step 1301/20074 | Loss: 0.1029\n",
      "Epoch 18/100 | Step 1401/20074 | Loss: 0.0011\n",
      "Epoch 18/100 | Step 1501/20074 | Loss: 0.0480\n",
      "Epoch 18/100 | Step 1601/20074 | Loss: 0.7136\n",
      "Epoch 18/100 | Step 1701/20074 | Loss: 0.3123\n",
      "Epoch 18/100 | Step 1801/20074 | Loss: 0.3788\n",
      "Epoch 18/100 | Step 1901/20074 | Loss: 0.1491\n",
      "Epoch 18/100 | Step 2001/20074 | Loss: 0.0002\n",
      "Epoch 18/100 | Step 2101/20074 | Loss: 0.8053\n",
      "Epoch 18/100 | Step 2201/20074 | Loss: 0.0035\n",
      "Epoch 18/100 | Step 2301/20074 | Loss: 0.0000\n",
      "Epoch 18/100 | Step 2401/20074 | Loss: 0.0024\n",
      "Epoch 18/100 | Step 2501/20074 | Loss: 0.2676\n",
      "Epoch 18/100 | Step 2601/20074 | Loss: 0.0900\n",
      "Epoch 18/100 | Step 2701/20074 | Loss: 0.0001\n",
      "Epoch 18/100 | Step 2801/20074 | Loss: 0.6988\n",
      "Epoch 18/100 | Step 2901/20074 | Loss: 0.0048\n",
      "Epoch 18/100 | Step 3001/20074 | Loss: 0.7696\n",
      "Epoch 18/100 | Step 3101/20074 | Loss: 0.5332\n",
      "Epoch 18/100 | Step 3201/20074 | Loss: 3.7096\n",
      "Epoch 18/100 | Step 3301/20074 | Loss: 0.5276\n",
      "Epoch 18/100 | Step 3401/20074 | Loss: 1.0016\n",
      "Epoch 18/100 | Step 3501/20074 | Loss: 0.0018\n",
      "Epoch 18/100 | Step 3601/20074 | Loss: 0.2638\n",
      "Epoch 18/100 | Step 3701/20074 | Loss: 3.4233\n",
      "Epoch 18/100 | Step 3801/20074 | Loss: 0.0221\n",
      "Epoch 18/100 | Step 3901/20074 | Loss: 0.0527\n",
      "Epoch 18/100 | Step 4001/20074 | Loss: 2.0207\n",
      "Epoch 18/100 | Step 4101/20074 | Loss: 0.2765\n",
      "Epoch 18/100 | Step 4201/20074 | Loss: 0.2885\n",
      "Epoch 18/100 | Step 4301/20074 | Loss: 0.0536\n",
      "Epoch 18/100 | Step 4401/20074 | Loss: 1.4842\n",
      "Epoch 18/100 | Step 4501/20074 | Loss: 2.0178\n",
      "Epoch 18/100 | Step 4601/20074 | Loss: 0.0232\n",
      "Epoch 18/100 | Step 4701/20074 | Loss: 0.0004\n",
      "Epoch 18/100 | Step 4801/20074 | Loss: 0.7122\n",
      "Epoch 18/100 | Step 4901/20074 | Loss: 1.3296\n",
      "Epoch 18/100 | Step 5001/20074 | Loss: 0.0001\n",
      "Epoch 18/100 | Step 5101/20074 | Loss: 0.0442\n",
      "Epoch 18/100 | Step 5201/20074 | Loss: 0.0002\n",
      "Epoch 18/100 | Step 5301/20074 | Loss: 4.3645\n",
      "Epoch 18/100 | Step 5401/20074 | Loss: 0.0624\n",
      "Epoch 18/100 | Step 5501/20074 | Loss: 0.0002\n",
      "Epoch 18/100 | Step 5601/20074 | Loss: 0.0085\n",
      "Epoch 18/100 | Step 5701/20074 | Loss: 0.4657\n",
      "Epoch 18/100 | Step 5801/20074 | Loss: 0.0003\n",
      "Epoch 18/100 | Step 5901/20074 | Loss: 0.0011\n",
      "Epoch 18/100 | Step 6001/20074 | Loss: 0.0352\n",
      "Epoch 18/100 | Step 6101/20074 | Loss: 3.3138\n",
      "Epoch 18/100 | Step 6201/20074 | Loss: 0.1644\n",
      "Epoch 18/100 | Step 6301/20074 | Loss: 0.2029\n",
      "Epoch 18/100 | Step 6401/20074 | Loss: 0.0336\n",
      "Epoch 18/100 | Step 6501/20074 | Loss: 1.0083\n",
      "Epoch 18/100 | Step 6601/20074 | Loss: 0.0491\n",
      "Epoch 18/100 | Step 6701/20074 | Loss: 0.0391\n",
      "Epoch 18/100 | Step 6801/20074 | Loss: 3.9662\n",
      "Epoch 18/100 | Step 6901/20074 | Loss: 0.6322\n",
      "Epoch 18/100 | Step 7001/20074 | Loss: 1.2066\n",
      "Epoch 18/100 | Step 7101/20074 | Loss: 2.7143\n",
      "Epoch 18/100 | Step 7201/20074 | Loss: 0.3716\n",
      "Epoch 18/100 | Step 7301/20074 | Loss: 2.5976\n",
      "Epoch 18/100 | Step 7401/20074 | Loss: 0.4195\n",
      "Epoch 18/100 | Step 7501/20074 | Loss: 0.0750\n",
      "Epoch 18/100 | Step 7601/20074 | Loss: 0.0084\n",
      "Epoch 18/100 | Step 7701/20074 | Loss: 0.3158\n",
      "Epoch 18/100 | Step 7801/20074 | Loss: 0.0010\n",
      "Epoch 18/100 | Step 7901/20074 | Loss: 0.0029\n",
      "Epoch 18/100 | Step 8001/20074 | Loss: 0.3228\n",
      "Epoch 18/100 | Step 8101/20074 | Loss: 0.0505\n",
      "Epoch 18/100 | Step 8201/20074 | Loss: 0.3389\n",
      "Epoch 18/100 | Step 8301/20074 | Loss: 3.5337\n",
      "Epoch 18/100 | Step 8401/20074 | Loss: 0.2383\n",
      "Epoch 18/100 | Step 8501/20074 | Loss: 4.4837\n",
      "Epoch 18/100 | Step 8601/20074 | Loss: 1.7082\n",
      "Epoch 18/100 | Step 8701/20074 | Loss: 1.1129\n",
      "Epoch 18/100 | Step 8801/20074 | Loss: 0.0037\n",
      "Epoch 18/100 | Step 8901/20074 | Loss: 1.0942\n",
      "Epoch 18/100 | Step 9001/20074 | Loss: 0.5256\n",
      "Epoch 18/100 | Step 9101/20074 | Loss: 0.1695\n",
      "Epoch 18/100 | Step 9201/20074 | Loss: 0.5468\n",
      "Epoch 18/100 | Step 9301/20074 | Loss: 0.5164\n",
      "Epoch 18/100 | Step 9401/20074 | Loss: 0.0606\n",
      "Epoch 18/100 | Step 9501/20074 | Loss: 0.2673\n",
      "Epoch 18/100 | Step 9601/20074 | Loss: 0.6897\n",
      "Epoch 18/100 | Step 9701/20074 | Loss: 0.0006\n",
      "Epoch 18/100 | Step 9801/20074 | Loss: 0.3509\n",
      "Epoch 18/100 | Step 9901/20074 | Loss: 0.0004\n",
      "Epoch 18/100 | Step 10001/20074 | Loss: 0.0326\n",
      "Epoch 18/100 | Step 10101/20074 | Loss: 0.4716\n",
      "Epoch 18/100 | Step 10201/20074 | Loss: 0.3052\n",
      "Epoch 18/100 | Step 10301/20074 | Loss: 0.0891\n",
      "Epoch 18/100 | Step 10401/20074 | Loss: 0.0002\n",
      "Epoch 18/100 | Step 10501/20074 | Loss: 0.2100\n",
      "Epoch 18/100 | Step 10601/20074 | Loss: 2.9486\n",
      "Epoch 18/100 | Step 10701/20074 | Loss: 1.1517\n",
      "Epoch 18/100 | Step 10801/20074 | Loss: 0.8398\n",
      "Epoch 18/100 | Step 10901/20074 | Loss: 0.7352\n",
      "Epoch 18/100 | Step 11001/20074 | Loss: 0.2450\n",
      "Epoch 18/100 | Step 11101/20074 | Loss: 0.0898\n",
      "Epoch 18/100 | Step 11201/20074 | Loss: 0.3047\n",
      "Epoch 18/100 | Step 11301/20074 | Loss: 0.0167\n",
      "Epoch 18/100 | Step 11401/20074 | Loss: 0.0002\n",
      "Epoch 18/100 | Step 11501/20074 | Loss: 0.5840\n",
      "Epoch 18/100 | Step 11601/20074 | Loss: 1.6623\n",
      "Epoch 18/100 | Step 11701/20074 | Loss: 0.8384\n",
      "Epoch 18/100 | Step 11801/20074 | Loss: 0.0113\n",
      "Epoch 18/100 | Step 11901/20074 | Loss: 1.7144\n",
      "Epoch 18/100 | Step 12001/20074 | Loss: 0.0177\n",
      "Epoch 18/100 | Step 12101/20074 | Loss: 2.6780\n",
      "Epoch 18/100 | Step 12201/20074 | Loss: 0.0006\n",
      "Epoch 18/100 | Step 12301/20074 | Loss: 0.1299\n",
      "Epoch 18/100 | Step 12401/20074 | Loss: 0.3217\n",
      "Epoch 18/100 | Step 12501/20074 | Loss: 1.6537\n",
      "Epoch 18/100 | Step 12601/20074 | Loss: 0.0189\n",
      "Epoch 18/100 | Step 12701/20074 | Loss: 0.5613\n",
      "Epoch 18/100 | Step 12801/20074 | Loss: 0.0021\n",
      "Epoch 18/100 | Step 12901/20074 | Loss: 0.0033\n",
      "Epoch 18/100 | Step 13001/20074 | Loss: 0.3178\n",
      "Epoch 18/100 | Step 13101/20074 | Loss: 1.1752\n",
      "Epoch 18/100 | Step 13201/20074 | Loss: 1.7404\n",
      "Epoch 18/100 | Step 13301/20074 | Loss: 2.1265\n",
      "Epoch 18/100 | Step 13401/20074 | Loss: 0.2288\n",
      "Epoch 18/100 | Step 13501/20074 | Loss: 0.1106\n",
      "Epoch 18/100 | Step 13601/20074 | Loss: 0.6738\n",
      "Epoch 18/100 | Step 13701/20074 | Loss: 2.1938\n",
      "Epoch 18/100 | Step 13801/20074 | Loss: 0.0027\n",
      "Epoch 18/100 | Step 13901/20074 | Loss: 4.0342\n",
      "Epoch 18/100 | Step 14001/20074 | Loss: 0.0037\n",
      "Epoch 18/100 | Step 14101/20074 | Loss: 0.4253\n",
      "Epoch 18/100 | Step 14201/20074 | Loss: 2.6635\n",
      "Epoch 18/100 | Step 14301/20074 | Loss: 0.6777\n",
      "Epoch 18/100 | Step 14401/20074 | Loss: 1.5868\n",
      "Epoch 18/100 | Step 14501/20074 | Loss: 0.1809\n",
      "Epoch 18/100 | Step 14601/20074 | Loss: 2.3673\n",
      "Epoch 18/100 | Step 14701/20074 | Loss: 0.5221\n",
      "Epoch 18/100 | Step 14801/20074 | Loss: 0.3752\n",
      "Epoch 18/100 | Step 14901/20074 | Loss: 0.0037\n",
      "Epoch 18/100 | Step 15001/20074 | Loss: 0.1254\n",
      "Epoch 18/100 | Step 15101/20074 | Loss: 0.0905\n",
      "Epoch 18/100 | Step 15201/20074 | Loss: 0.0129\n",
      "Epoch 18/100 | Step 15301/20074 | Loss: 0.0010\n",
      "Epoch 18/100 | Step 15401/20074 | Loss: 0.0008\n",
      "Epoch 18/100 | Step 15501/20074 | Loss: 1.8798\n",
      "Epoch 18/100 | Step 15601/20074 | Loss: 0.1744\n",
      "Epoch 18/100 | Step 15701/20074 | Loss: 2.8668\n",
      "Epoch 18/100 | Step 15801/20074 | Loss: 3.9688\n",
      "Epoch 18/100 | Step 15901/20074 | Loss: 0.2229\n",
      "Epoch 18/100 | Step 16001/20074 | Loss: 1.1038\n",
      "Epoch 18/100 | Step 16101/20074 | Loss: 0.3417\n",
      "Epoch 18/100 | Step 16201/20074 | Loss: 1.0592\n",
      "Epoch 18/100 | Step 16301/20074 | Loss: 2.6825\n",
      "Epoch 18/100 | Step 16401/20074 | Loss: 2.1745\n",
      "Epoch 18/100 | Step 16501/20074 | Loss: 0.2397\n",
      "Epoch 18/100 | Step 16601/20074 | Loss: 0.2621\n",
      "Epoch 18/100 | Step 16701/20074 | Loss: 0.4234\n",
      "Epoch 18/100 | Step 16801/20074 | Loss: 0.3817\n",
      "Epoch 18/100 | Step 16901/20074 | Loss: 0.1084\n",
      "Epoch 18/100 | Step 17001/20074 | Loss: 0.0728\n",
      "Epoch 18/100 | Step 17101/20074 | Loss: 0.2343\n",
      "Epoch 18/100 | Step 17201/20074 | Loss: 7.2236\n",
      "Epoch 18/100 | Step 17301/20074 | Loss: 0.0002\n",
      "Epoch 18/100 | Step 17401/20074 | Loss: 0.0004\n",
      "Epoch 18/100 | Step 17501/20074 | Loss: 0.1428\n",
      "Epoch 18/100 | Step 17601/20074 | Loss: 0.0015\n",
      "Epoch 18/100 | Step 17701/20074 | Loss: 0.0105\n",
      "Epoch 18/100 | Step 17801/20074 | Loss: 0.4648\n",
      "Epoch 18/100 | Step 17901/20074 | Loss: 0.9785\n",
      "Epoch 18/100 | Step 18001/20074 | Loss: 0.8820\n",
      "Epoch 18/100 | Step 18101/20074 | Loss: 2.7798\n",
      "Epoch 18/100 | Step 18201/20074 | Loss: 1.0170\n",
      "Epoch 18/100 | Step 18301/20074 | Loss: 2.4843\n",
      "Epoch 18/100 | Step 18401/20074 | Loss: 4.6016\n",
      "Epoch 18/100 | Step 18501/20074 | Loss: 0.6839\n",
      "Epoch 18/100 | Step 18601/20074 | Loss: 0.0140\n",
      "Epoch 18/100 | Step 18701/20074 | Loss: 0.0000\n",
      "Epoch 18/100 | Step 18801/20074 | Loss: 0.8983\n",
      "Epoch 18/100 | Step 18901/20074 | Loss: 1.2575\n",
      "Epoch 18/100 | Step 19001/20074 | Loss: 0.0036\n",
      "Epoch 18/100 | Step 19101/20074 | Loss: 0.0005\n",
      "Epoch 18/100 | Step 19201/20074 | Loss: 0.0591\n",
      "Epoch 18/100 | Step 19301/20074 | Loss: 1.8002\n",
      "Epoch 18/100 | Step 19401/20074 | Loss: 0.2254\n",
      "Epoch 18/100 | Step 19501/20074 | Loss: 0.0009\n",
      "Epoch 18/100 | Step 19601/20074 | Loss: 0.0100\n",
      "Epoch 18/100 | Step 19701/20074 | Loss: 0.0154\n",
      "Epoch 18/100 | Step 19801/20074 | Loss: 0.0017\n",
      "Epoch 18/100 | Step 19901/20074 | Loss: 1.6441\n",
      "Epoch 18/100 | Step 20001/20074 | Loss: 5.6256\n",
      "Epoch 19/100 | Step 1/20074 | Loss: 0.2820\n",
      "Epoch 19/100 | Step 101/20074 | Loss: 1.1663\n",
      "Epoch 19/100 | Step 201/20074 | Loss: 0.7783\n",
      "Epoch 19/100 | Step 301/20074 | Loss: 0.0023\n",
      "Epoch 19/100 | Step 401/20074 | Loss: 0.3327\n",
      "Epoch 19/100 | Step 501/20074 | Loss: 0.2130\n",
      "Epoch 19/100 | Step 601/20074 | Loss: 0.5429\n",
      "Epoch 19/100 | Step 701/20074 | Loss: 0.0005\n",
      "Epoch 19/100 | Step 801/20074 | Loss: 3.0207\n",
      "Epoch 19/100 | Step 901/20074 | Loss: 0.6688\n",
      "Epoch 19/100 | Step 1001/20074 | Loss: 1.0917\n",
      "Epoch 19/100 | Step 1101/20074 | Loss: 0.7272\n",
      "Epoch 19/100 | Step 1201/20074 | Loss: 0.6635\n",
      "Epoch 19/100 | Step 1301/20074 | Loss: 0.3213\n",
      "Epoch 19/100 | Step 1401/20074 | Loss: 1.2941\n",
      "Epoch 19/100 | Step 1501/20074 | Loss: 1.9605\n",
      "Epoch 19/100 | Step 1601/20074 | Loss: 2.5058\n",
      "Epoch 19/100 | Step 1701/20074 | Loss: 0.2894\n",
      "Epoch 19/100 | Step 1801/20074 | Loss: 0.1645\n",
      "Epoch 19/100 | Step 1901/20074 | Loss: 0.4688\n",
      "Epoch 19/100 | Step 2001/20074 | Loss: 0.3455\n",
      "Epoch 19/100 | Step 2101/20074 | Loss: 0.3654\n",
      "Epoch 19/100 | Step 2201/20074 | Loss: 0.0159\n",
      "Epoch 19/100 | Step 2301/20074 | Loss: 1.2159\n",
      "Epoch 19/100 | Step 2401/20074 | Loss: 0.3268\n",
      "Epoch 19/100 | Step 2501/20074 | Loss: 0.0008\n",
      "Epoch 19/100 | Step 2601/20074 | Loss: 0.0000\n",
      "Epoch 19/100 | Step 2701/20074 | Loss: 0.0000\n",
      "Epoch 19/100 | Step 2801/20074 | Loss: 0.3166\n",
      "Epoch 19/100 | Step 2901/20074 | Loss: 4.0412\n",
      "Epoch 19/100 | Step 3001/20074 | Loss: 0.0005\n",
      "Epoch 19/100 | Step 3101/20074 | Loss: 0.6200\n",
      "Epoch 19/100 | Step 3201/20074 | Loss: 2.1657\n",
      "Epoch 19/100 | Step 3301/20074 | Loss: 0.0044\n",
      "Epoch 19/100 | Step 3401/20074 | Loss: 0.9870\n",
      "Epoch 19/100 | Step 3501/20074 | Loss: 0.1391\n",
      "Epoch 19/100 | Step 3601/20074 | Loss: 1.1047\n",
      "Epoch 19/100 | Step 3701/20074 | Loss: 0.0445\n",
      "Epoch 19/100 | Step 3801/20074 | Loss: 0.5615\n",
      "Epoch 19/100 | Step 3901/20074 | Loss: 0.8337\n",
      "Epoch 19/100 | Step 4001/20074 | Loss: 0.1338\n",
      "Epoch 19/100 | Step 4101/20074 | Loss: 0.1520\n",
      "Epoch 19/100 | Step 4201/20074 | Loss: 0.5938\n",
      "Epoch 19/100 | Step 4301/20074 | Loss: 0.2300\n",
      "Epoch 19/100 | Step 4401/20074 | Loss: 0.0481\n",
      "Epoch 19/100 | Step 4501/20074 | Loss: 0.2858\n",
      "Epoch 19/100 | Step 4601/20074 | Loss: 0.1548\n",
      "Epoch 19/100 | Step 4701/20074 | Loss: 0.2872\n",
      "Epoch 19/100 | Step 4801/20074 | Loss: 2.9465\n",
      "Epoch 19/100 | Step 4901/20074 | Loss: 0.0051\n",
      "Epoch 19/100 | Step 5001/20074 | Loss: 0.8011\n",
      "Epoch 19/100 | Step 5101/20074 | Loss: 0.2877\n",
      "Epoch 19/100 | Step 5201/20074 | Loss: 2.1034\n",
      "Epoch 19/100 | Step 5301/20074 | Loss: 0.2259\n",
      "Epoch 19/100 | Step 5401/20074 | Loss: 0.0072\n",
      "Epoch 19/100 | Step 5501/20074 | Loss: 0.0720\n",
      "Epoch 19/100 | Step 5601/20074 | Loss: 1.0531\n",
      "Epoch 19/100 | Step 5701/20074 | Loss: 2.3983\n",
      "Epoch 19/100 | Step 5801/20074 | Loss: 0.0038\n",
      "Epoch 19/100 | Step 5901/20074 | Loss: 0.5127\n",
      "Epoch 19/100 | Step 6001/20074 | Loss: 0.8781\n",
      "Epoch 19/100 | Step 6101/20074 | Loss: 1.9027\n",
      "Epoch 19/100 | Step 6201/20074 | Loss: 0.0051\n",
      "Epoch 19/100 | Step 6301/20074 | Loss: 0.0619\n",
      "Epoch 19/100 | Step 6401/20074 | Loss: 0.0010\n",
      "Epoch 19/100 | Step 6501/20074 | Loss: 2.6311\n",
      "Epoch 19/100 | Step 6601/20074 | Loss: 0.0073\n",
      "Epoch 19/100 | Step 6701/20074 | Loss: 0.4409\n",
      "Epoch 19/100 | Step 6801/20074 | Loss: 0.7039\n",
      "Epoch 19/100 | Step 6901/20074 | Loss: 0.1846\n",
      "Epoch 19/100 | Step 7001/20074 | Loss: 0.0007\n",
      "Epoch 19/100 | Step 7101/20074 | Loss: 0.0066\n",
      "Epoch 19/100 | Step 7201/20074 | Loss: 0.4045\n",
      "Epoch 19/100 | Step 7301/20074 | Loss: 0.0049\n",
      "Epoch 19/100 | Step 7401/20074 | Loss: 0.6821\n",
      "Epoch 19/100 | Step 7501/20074 | Loss: 0.0250\n",
      "Epoch 19/100 | Step 7601/20074 | Loss: 0.3464\n",
      "Epoch 19/100 | Step 7701/20074 | Loss: 3.4761\n",
      "Epoch 19/100 | Step 7801/20074 | Loss: 1.9781\n",
      "Epoch 19/100 | Step 7901/20074 | Loss: 0.9424\n",
      "Epoch 19/100 | Step 8001/20074 | Loss: 0.0025\n",
      "Epoch 19/100 | Step 8101/20074 | Loss: 0.0288\n",
      "Epoch 19/100 | Step 8201/20074 | Loss: 1.6082\n",
      "Epoch 19/100 | Step 8301/20074 | Loss: 0.5760\n",
      "Epoch 19/100 | Step 8401/20074 | Loss: 0.0109\n",
      "Epoch 19/100 | Step 8501/20074 | Loss: 0.0003\n",
      "Epoch 19/100 | Step 8601/20074 | Loss: 0.0115\n",
      "Epoch 19/100 | Step 8701/20074 | Loss: 0.8377\n",
      "Epoch 19/100 | Step 8801/20074 | Loss: 0.0796\n",
      "Epoch 19/100 | Step 8901/20074 | Loss: 0.8979\n",
      "Epoch 19/100 | Step 9001/20074 | Loss: 0.0180\n",
      "Epoch 19/100 | Step 9101/20074 | Loss: 4.3478\n",
      "Epoch 19/100 | Step 9201/20074 | Loss: 3.9255\n",
      "Epoch 19/100 | Step 9301/20074 | Loss: 1.4890\n",
      "Epoch 19/100 | Step 9401/20074 | Loss: 2.8306\n",
      "Epoch 19/100 | Step 9501/20074 | Loss: 0.6730\n",
      "Epoch 19/100 | Step 9601/20074 | Loss: 0.1980\n",
      "Epoch 19/100 | Step 9701/20074 | Loss: 0.7005\n",
      "Epoch 19/100 | Step 9801/20074 | Loss: 0.0159\n",
      "Epoch 19/100 | Step 9901/20074 | Loss: 0.2027\n",
      "Epoch 19/100 | Step 10001/20074 | Loss: 6.5339\n",
      "Epoch 19/100 | Step 10101/20074 | Loss: 0.0018\n",
      "Epoch 19/100 | Step 10201/20074 | Loss: 1.0133\n",
      "Epoch 19/100 | Step 10301/20074 | Loss: 0.7842\n",
      "Epoch 19/100 | Step 10401/20074 | Loss: 0.8588\n",
      "Epoch 19/100 | Step 10501/20074 | Loss: 0.0216\n",
      "Epoch 19/100 | Step 10601/20074 | Loss: 0.2444\n",
      "Epoch 19/100 | Step 10701/20074 | Loss: 0.0002\n",
      "Epoch 19/100 | Step 10801/20074 | Loss: 0.1649\n",
      "Epoch 19/100 | Step 10901/20074 | Loss: 0.3674\n",
      "Epoch 19/100 | Step 11001/20074 | Loss: 1.7845\n",
      "Epoch 19/100 | Step 11101/20074 | Loss: 1.3266\n",
      "Epoch 19/100 | Step 11201/20074 | Loss: 0.0334\n",
      "Epoch 19/100 | Step 11301/20074 | Loss: 0.1952\n",
      "Epoch 19/100 | Step 11401/20074 | Loss: 0.0462\n",
      "Epoch 19/100 | Step 11501/20074 | Loss: 0.4093\n",
      "Epoch 19/100 | Step 11601/20074 | Loss: 2.3154\n",
      "Epoch 19/100 | Step 11701/20074 | Loss: 0.3320\n",
      "Epoch 19/100 | Step 11801/20074 | Loss: 0.0718\n",
      "Epoch 19/100 | Step 11901/20074 | Loss: 0.1279\n",
      "Epoch 19/100 | Step 12001/20074 | Loss: 0.0463\n",
      "Epoch 19/100 | Step 12101/20074 | Loss: 2.9208\n",
      "Epoch 19/100 | Step 12201/20074 | Loss: 0.0580\n",
      "Epoch 19/100 | Step 12301/20074 | Loss: 0.0344\n",
      "Epoch 19/100 | Step 12401/20074 | Loss: 0.0004\n",
      "Epoch 19/100 | Step 12501/20074 | Loss: 1.0301\n",
      "Epoch 19/100 | Step 12601/20074 | Loss: 0.9621\n",
      "Epoch 19/100 | Step 12701/20074 | Loss: 0.0076\n",
      "Epoch 19/100 | Step 12801/20074 | Loss: 4.4592\n",
      "Epoch 19/100 | Step 12901/20074 | Loss: 0.0420\n",
      "Epoch 19/100 | Step 13001/20074 | Loss: 0.4103\n",
      "Epoch 19/100 | Step 13101/20074 | Loss: 0.4965\n",
      "Epoch 19/100 | Step 13201/20074 | Loss: 3.1494\n",
      "Epoch 19/100 | Step 13301/20074 | Loss: 0.0352\n",
      "Epoch 19/100 | Step 13401/20074 | Loss: 0.1864\n",
      "Epoch 19/100 | Step 13501/20074 | Loss: 1.3500\n",
      "Epoch 19/100 | Step 13601/20074 | Loss: 0.2368\n",
      "Epoch 19/100 | Step 13701/20074 | Loss: 0.8262\n",
      "Epoch 19/100 | Step 13801/20074 | Loss: 0.0207\n",
      "Epoch 19/100 | Step 13901/20074 | Loss: 0.0861\n",
      "Epoch 19/100 | Step 14001/20074 | Loss: 3.4318\n",
      "Epoch 19/100 | Step 14101/20074 | Loss: 0.0334\n",
      "Epoch 19/100 | Step 14201/20074 | Loss: 2.2659\n",
      "Epoch 19/100 | Step 14301/20074 | Loss: 2.4181\n",
      "Epoch 19/100 | Step 14401/20074 | Loss: 0.0049\n",
      "Epoch 19/100 | Step 14501/20074 | Loss: 0.3550\n",
      "Epoch 19/100 | Step 14601/20074 | Loss: 1.8562\n",
      "Epoch 19/100 | Step 14701/20074 | Loss: 0.0013\n",
      "Epoch 19/100 | Step 14801/20074 | Loss: 0.8278\n",
      "Epoch 19/100 | Step 14901/20074 | Loss: 0.3543\n",
      "Epoch 19/100 | Step 15001/20074 | Loss: 0.0181\n",
      "Epoch 19/100 | Step 15101/20074 | Loss: 0.3337\n",
      "Epoch 19/100 | Step 15201/20074 | Loss: 1.9044\n",
      "Epoch 19/100 | Step 15301/20074 | Loss: 0.0093\n",
      "Epoch 19/100 | Step 15401/20074 | Loss: 0.0013\n",
      "Epoch 19/100 | Step 15501/20074 | Loss: 0.3340\n",
      "Epoch 19/100 | Step 15601/20074 | Loss: 0.0194\n",
      "Epoch 19/100 | Step 15701/20074 | Loss: 0.7491\n",
      "Epoch 19/100 | Step 15801/20074 | Loss: 1.1458\n",
      "Epoch 19/100 | Step 15901/20074 | Loss: 1.9917\n",
      "Epoch 19/100 | Step 16001/20074 | Loss: 0.9020\n",
      "Epoch 19/100 | Step 16101/20074 | Loss: 0.0000\n",
      "Epoch 19/100 | Step 16201/20074 | Loss: 0.2759\n",
      "Epoch 19/100 | Step 16301/20074 | Loss: 1.8507\n",
      "Epoch 19/100 | Step 16401/20074 | Loss: 0.6518\n",
      "Epoch 19/100 | Step 16501/20074 | Loss: 0.0050\n",
      "Epoch 19/100 | Step 16601/20074 | Loss: 1.1489\n",
      "Epoch 19/100 | Step 16701/20074 | Loss: 0.2318\n",
      "Epoch 19/100 | Step 16801/20074 | Loss: 1.6764\n",
      "Epoch 19/100 | Step 16901/20074 | Loss: 0.0387\n",
      "Epoch 19/100 | Step 17001/20074 | Loss: 0.0006\n",
      "Epoch 19/100 | Step 17101/20074 | Loss: 0.8837\n",
      "Epoch 19/100 | Step 17201/20074 | Loss: 0.0128\n",
      "Epoch 19/100 | Step 17301/20074 | Loss: 0.3970\n",
      "Epoch 19/100 | Step 17401/20074 | Loss: 1.0378\n",
      "Epoch 19/100 | Step 17501/20074 | Loss: 3.1446\n",
      "Epoch 19/100 | Step 17601/20074 | Loss: 0.0116\n",
      "Epoch 19/100 | Step 17701/20074 | Loss: 0.0003\n",
      "Epoch 19/100 | Step 17801/20074 | Loss: 0.4998\n",
      "Epoch 19/100 | Step 17901/20074 | Loss: 0.0001\n",
      "Epoch 19/100 | Step 18001/20074 | Loss: 0.2920\n",
      "Epoch 19/100 | Step 18101/20074 | Loss: 2.2977\n",
      "Epoch 19/100 | Step 18201/20074 | Loss: 0.4776\n",
      "Epoch 19/100 | Step 18301/20074 | Loss: 0.0188\n",
      "Epoch 19/100 | Step 18401/20074 | Loss: 1.3841\n",
      "Epoch 19/100 | Step 18501/20074 | Loss: 0.0147\n",
      "Epoch 19/100 | Step 18601/20074 | Loss: 0.7662\n",
      "Epoch 19/100 | Step 18701/20074 | Loss: 0.0174\n",
      "Epoch 19/100 | Step 18801/20074 | Loss: 0.0060\n",
      "Epoch 19/100 | Step 18901/20074 | Loss: 0.6283\n",
      "Epoch 19/100 | Step 19001/20074 | Loss: 0.0938\n",
      "Epoch 19/100 | Step 19101/20074 | Loss: 2.1812\n",
      "Epoch 19/100 | Step 19201/20074 | Loss: 0.5564\n",
      "Epoch 19/100 | Step 19301/20074 | Loss: 2.7605\n",
      "Epoch 19/100 | Step 19401/20074 | Loss: 0.0013\n",
      "Epoch 19/100 | Step 19501/20074 | Loss: 0.0037\n",
      "Epoch 19/100 | Step 19601/20074 | Loss: 0.8109\n",
      "Epoch 19/100 | Step 19701/20074 | Loss: 0.4575\n",
      "Epoch 19/100 | Step 19801/20074 | Loss: 0.0938\n",
      "Epoch 19/100 | Step 19901/20074 | Loss: 0.0010\n",
      "Epoch 19/100 | Step 20001/20074 | Loss: 0.0040\n",
      "Epoch 20/100 | Step 1/20074 | Loss: 0.0019\n",
      "Epoch 20/100 | Step 101/20074 | Loss: 0.2360\n",
      "Epoch 20/100 | Step 201/20074 | Loss: 2.9777\n",
      "Epoch 20/100 | Step 301/20074 | Loss: 1.4636\n",
      "Epoch 20/100 | Step 401/20074 | Loss: 1.8215\n",
      "Epoch 20/100 | Step 501/20074 | Loss: 0.0003\n",
      "Epoch 20/100 | Step 601/20074 | Loss: 0.0438\n",
      "Epoch 20/100 | Step 701/20074 | Loss: 0.0474\n",
      "Epoch 20/100 | Step 801/20074 | Loss: 0.2863\n",
      "Epoch 20/100 | Step 901/20074 | Loss: 0.0046\n",
      "Epoch 20/100 | Step 1001/20074 | Loss: 0.0184\n",
      "Epoch 20/100 | Step 1101/20074 | Loss: 0.1487\n",
      "Epoch 20/100 | Step 1201/20074 | Loss: 0.0003\n",
      "Epoch 20/100 | Step 1301/20074 | Loss: 2.1169\n",
      "Epoch 20/100 | Step 1401/20074 | Loss: 1.4189\n",
      "Epoch 20/100 | Step 1501/20074 | Loss: 0.0694\n",
      "Epoch 20/100 | Step 1601/20074 | Loss: 4.7005\n",
      "Epoch 20/100 | Step 1701/20074 | Loss: 1.7740\n",
      "Epoch 20/100 | Step 1801/20074 | Loss: 0.2058\n",
      "Epoch 20/100 | Step 1901/20074 | Loss: 0.0006\n",
      "Epoch 20/100 | Step 2001/20074 | Loss: 0.0140\n",
      "Epoch 20/100 | Step 2101/20074 | Loss: 0.0110\n",
      "Epoch 20/100 | Step 2201/20074 | Loss: 0.2381\n",
      "Epoch 20/100 | Step 2301/20074 | Loss: 0.0034\n",
      "Epoch 20/100 | Step 2401/20074 | Loss: 0.9874\n",
      "Epoch 20/100 | Step 2501/20074 | Loss: 0.3915\n",
      "Epoch 20/100 | Step 2601/20074 | Loss: 3.1578\n",
      "Epoch 20/100 | Step 2701/20074 | Loss: 2.5065\n",
      "Epoch 20/100 | Step 2801/20074 | Loss: 0.0661\n",
      "Epoch 20/100 | Step 2901/20074 | Loss: 0.3632\n",
      "Epoch 20/100 | Step 3001/20074 | Loss: 0.2072\n",
      "Epoch 20/100 | Step 3101/20074 | Loss: 0.0010\n",
      "Epoch 20/100 | Step 3201/20074 | Loss: 0.5261\n",
      "Epoch 20/100 | Step 3301/20074 | Loss: 0.9557\n",
      "Epoch 20/100 | Step 3401/20074 | Loss: 0.6331\n",
      "Epoch 20/100 | Step 3501/20074 | Loss: 0.5036\n",
      "Epoch 20/100 | Step 3601/20074 | Loss: 0.0016\n",
      "Epoch 20/100 | Step 3701/20074 | Loss: 0.1383\n",
      "Epoch 20/100 | Step 3801/20074 | Loss: 0.0007\n",
      "Epoch 20/100 | Step 3901/20074 | Loss: 0.4576\n",
      "Epoch 20/100 | Step 4001/20074 | Loss: 4.2331\n",
      "Epoch 20/100 | Step 4101/20074 | Loss: 0.6596\n",
      "Epoch 20/100 | Step 4201/20074 | Loss: 0.0295\n",
      "Epoch 20/100 | Step 4301/20074 | Loss: 0.5305\n",
      "Epoch 20/100 | Step 4401/20074 | Loss: 0.0017\n",
      "Epoch 20/100 | Step 4501/20074 | Loss: 0.1671\n",
      "Epoch 20/100 | Step 4601/20074 | Loss: 1.3107\n",
      "Epoch 20/100 | Step 4701/20074 | Loss: 1.3449\n",
      "Epoch 20/100 | Step 4801/20074 | Loss: 3.1976\n",
      "Epoch 20/100 | Step 4901/20074 | Loss: 0.2646\n",
      "Epoch 20/100 | Step 5001/20074 | Loss: 0.0801\n",
      "Epoch 20/100 | Step 5101/20074 | Loss: 0.8834\n",
      "Epoch 20/100 | Step 5201/20074 | Loss: 0.0010\n",
      "Epoch 20/100 | Step 5301/20074 | Loss: 0.0066\n",
      "Epoch 20/100 | Step 5401/20074 | Loss: 3.9714\n",
      "Epoch 20/100 | Step 5501/20074 | Loss: 0.4850\n",
      "Epoch 20/100 | Step 5601/20074 | Loss: 0.1433\n",
      "Epoch 20/100 | Step 5701/20074 | Loss: 2.3159\n",
      "Epoch 20/100 | Step 5801/20074 | Loss: 3.4051\n",
      "Epoch 20/100 | Step 5901/20074 | Loss: 2.0937\n",
      "Epoch 20/100 | Step 6001/20074 | Loss: 0.1818\n",
      "Epoch 20/100 | Step 6101/20074 | Loss: 0.2611\n",
      "Epoch 20/100 | Step 6201/20074 | Loss: 2.7643\n",
      "Epoch 20/100 | Step 6301/20074 | Loss: 0.3535\n",
      "Epoch 20/100 | Step 6401/20074 | Loss: 0.0178\n",
      "Epoch 20/100 | Step 6501/20074 | Loss: 0.4464\n",
      "Epoch 20/100 | Step 6601/20074 | Loss: 0.9028\n",
      "Epoch 20/100 | Step 6701/20074 | Loss: 0.0645\n",
      "Epoch 20/100 | Step 6801/20074 | Loss: 0.5641\n",
      "Epoch 20/100 | Step 6901/20074 | Loss: 0.0007\n",
      "Epoch 20/100 | Step 7001/20074 | Loss: 0.0341\n",
      "Epoch 20/100 | Step 7101/20074 | Loss: 0.0007\n",
      "Epoch 20/100 | Step 7201/20074 | Loss: 0.4645\n",
      "Epoch 20/100 | Step 7301/20074 | Loss: 0.0133\n",
      "Epoch 20/100 | Step 7401/20074 | Loss: 2.8681\n",
      "Epoch 20/100 | Step 7501/20074 | Loss: 0.1811\n",
      "Epoch 20/100 | Step 7601/20074 | Loss: 0.1330\n",
      "Epoch 20/100 | Step 7701/20074 | Loss: 0.6799\n",
      "Epoch 20/100 | Step 7801/20074 | Loss: 3.7594\n",
      "Epoch 20/100 | Step 7901/20074 | Loss: 0.0013\n",
      "Epoch 20/100 | Step 8001/20074 | Loss: 0.1082\n",
      "Epoch 20/100 | Step 8101/20074 | Loss: 0.9088\n",
      "Epoch 20/100 | Step 8201/20074 | Loss: 0.4389\n",
      "Epoch 20/100 | Step 8301/20074 | Loss: 0.0141\n",
      "Epoch 20/100 | Step 8401/20074 | Loss: 2.7069\n",
      "Epoch 20/100 | Step 8501/20074 | Loss: 0.3507\n",
      "Epoch 20/100 | Step 8601/20074 | Loss: 0.0016\n",
      "Epoch 20/100 | Step 8701/20074 | Loss: 0.0677\n",
      "Epoch 20/100 | Step 8801/20074 | Loss: 0.2875\n",
      "Epoch 20/100 | Step 8901/20074 | Loss: 0.5109\n",
      "Epoch 20/100 | Step 9001/20074 | Loss: 0.0318\n",
      "Epoch 20/100 | Step 9101/20074 | Loss: 0.0023\n",
      "Epoch 20/100 | Step 9201/20074 | Loss: 0.3676\n",
      "Epoch 20/100 | Step 9301/20074 | Loss: 0.0232\n",
      "Epoch 20/100 | Step 9401/20074 | Loss: 0.1367\n",
      "Epoch 20/100 | Step 9501/20074 | Loss: 0.1130\n",
      "Epoch 20/100 | Step 9601/20074 | Loss: 0.0503\n",
      "Epoch 20/100 | Step 9701/20074 | Loss: 0.7789\n",
      "Epoch 20/100 | Step 9801/20074 | Loss: 0.0000\n",
      "Epoch 20/100 | Step 9901/20074 | Loss: 2.1440\n",
      "Epoch 20/100 | Step 10001/20074 | Loss: 0.0407\n",
      "Epoch 20/100 | Step 10101/20074 | Loss: 0.7196\n",
      "Epoch 20/100 | Step 10201/20074 | Loss: 3.3196\n",
      "Epoch 20/100 | Step 10301/20074 | Loss: 0.0662\n",
      "Epoch 20/100 | Step 10401/20074 | Loss: 2.0081\n",
      "Epoch 20/100 | Step 10501/20074 | Loss: 3.8462\n",
      "Epoch 20/100 | Step 10601/20074 | Loss: 0.0723\n",
      "Epoch 20/100 | Step 10701/20074 | Loss: 3.2812\n",
      "Epoch 20/100 | Step 10801/20074 | Loss: 1.3292\n",
      "Epoch 20/100 | Step 10901/20074 | Loss: 0.0005\n",
      "Epoch 20/100 | Step 11001/20074 | Loss: 0.0001\n",
      "Epoch 20/100 | Step 11101/20074 | Loss: 0.0008\n",
      "Epoch 20/100 | Step 11201/20074 | Loss: 2.0733\n",
      "Epoch 20/100 | Step 11301/20074 | Loss: 0.0041\n",
      "Epoch 20/100 | Step 11401/20074 | Loss: 0.0010\n",
      "Epoch 20/100 | Step 11501/20074 | Loss: 0.9352\n",
      "Epoch 20/100 | Step 11601/20074 | Loss: 0.0060\n",
      "Epoch 20/100 | Step 11701/20074 | Loss: 2.1715\n",
      "Epoch 20/100 | Step 11801/20074 | Loss: 0.1977\n",
      "Epoch 20/100 | Step 11901/20074 | Loss: 3.4576\n",
      "Epoch 20/100 | Step 12001/20074 | Loss: 0.7137\n",
      "Epoch 20/100 | Step 12101/20074 | Loss: 2.4545\n",
      "Epoch 20/100 | Step 12201/20074 | Loss: 0.1399\n",
      "Epoch 20/100 | Step 12301/20074 | Loss: 0.1201\n",
      "Epoch 20/100 | Step 12401/20074 | Loss: 1.8011\n",
      "Epoch 20/100 | Step 12501/20074 | Loss: 0.6345\n",
      "Epoch 20/100 | Step 12601/20074 | Loss: 1.6240\n",
      "Epoch 20/100 | Step 12701/20074 | Loss: 0.0090\n",
      "Epoch 20/100 | Step 12801/20074 | Loss: 0.0049\n",
      "Epoch 20/100 | Step 12901/20074 | Loss: 0.2057\n",
      "Epoch 20/100 | Step 13001/20074 | Loss: 1.6131\n",
      "Epoch 20/100 | Step 13101/20074 | Loss: 1.0420\n",
      "Epoch 20/100 | Step 13201/20074 | Loss: 0.0067\n",
      "Epoch 20/100 | Step 13301/20074 | Loss: 0.0001\n",
      "Epoch 20/100 | Step 13401/20074 | Loss: 0.0131\n",
      "Epoch 20/100 | Step 13501/20074 | Loss: 0.0911\n",
      "Epoch 20/100 | Step 13601/20074 | Loss: 0.0102\n",
      "Epoch 20/100 | Step 13701/20074 | Loss: 0.9229\n",
      "Epoch 20/100 | Step 13801/20074 | Loss: 0.0740\n",
      "Epoch 20/100 | Step 13901/20074 | Loss: 0.0003\n",
      "Epoch 20/100 | Step 14001/20074 | Loss: 0.0007\n",
      "Epoch 20/100 | Step 14101/20074 | Loss: 0.2824\n",
      "Epoch 20/100 | Step 14201/20074 | Loss: 1.5149\n",
      "Epoch 20/100 | Step 14301/20074 | Loss: 0.8640\n",
      "Epoch 20/100 | Step 14401/20074 | Loss: 0.6211\n",
      "Epoch 20/100 | Step 14501/20074 | Loss: 3.8305\n",
      "Epoch 20/100 | Step 14601/20074 | Loss: 0.0014\n",
      "Epoch 20/100 | Step 14701/20074 | Loss: 1.3336\n",
      "Epoch 20/100 | Step 14801/20074 | Loss: 0.0002\n",
      "Epoch 20/100 | Step 14901/20074 | Loss: 0.2958\n",
      "Epoch 20/100 | Step 15001/20074 | Loss: 0.0013\n",
      "Epoch 20/100 | Step 15101/20074 | Loss: 0.0045\n",
      "Epoch 20/100 | Step 15201/20074 | Loss: 0.1866\n",
      "Epoch 20/100 | Step 15301/20074 | Loss: 0.4013\n",
      "Epoch 20/100 | Step 15401/20074 | Loss: 2.2326\n",
      "Epoch 20/100 | Step 15501/20074 | Loss: 0.0522\n",
      "Epoch 20/100 | Step 15601/20074 | Loss: 0.0006\n",
      "Epoch 20/100 | Step 15701/20074 | Loss: 0.1530\n",
      "Epoch 20/100 | Step 15801/20074 | Loss: 0.7349\n",
      "Epoch 20/100 | Step 15901/20074 | Loss: 0.7938\n",
      "Epoch 20/100 | Step 16001/20074 | Loss: 1.6380\n",
      "Epoch 20/100 | Step 16101/20074 | Loss: 0.0973\n",
      "Epoch 20/100 | Step 16201/20074 | Loss: 0.0017\n",
      "Epoch 20/100 | Step 16301/20074 | Loss: 2.6599\n",
      "Epoch 20/100 | Step 16401/20074 | Loss: 3.6004\n",
      "Epoch 20/100 | Step 16501/20074 | Loss: 1.1394\n",
      "Epoch 20/100 | Step 16601/20074 | Loss: 0.0000\n",
      "Epoch 20/100 | Step 16701/20074 | Loss: 2.3930\n",
      "Epoch 20/100 | Step 16801/20074 | Loss: 0.1690\n",
      "Epoch 20/100 | Step 16901/20074 | Loss: 0.0003\n",
      "Epoch 20/100 | Step 17001/20074 | Loss: 0.0002\n",
      "Epoch 20/100 | Step 17101/20074 | Loss: 0.2547\n",
      "Epoch 20/100 | Step 17201/20074 | Loss: 0.3165\n",
      "Epoch 20/100 | Step 17301/20074 | Loss: 0.3182\n",
      "Epoch 20/100 | Step 17401/20074 | Loss: 0.0100\n",
      "Epoch 20/100 | Step 17501/20074 | Loss: 1.4442\n",
      "Epoch 20/100 | Step 17601/20074 | Loss: 0.1723\n",
      "Epoch 20/100 | Step 17701/20074 | Loss: 0.0009\n",
      "Epoch 20/100 | Step 17801/20074 | Loss: 1.1609\n",
      "Epoch 20/100 | Step 17901/20074 | Loss: 0.4835\n",
      "Epoch 20/100 | Step 18001/20074 | Loss: 1.4834\n",
      "Epoch 20/100 | Step 18101/20074 | Loss: 0.0012\n",
      "Epoch 20/100 | Step 18201/20074 | Loss: 0.8699\n",
      "Epoch 20/100 | Step 18301/20074 | Loss: 0.0010\n",
      "Epoch 20/100 | Step 18401/20074 | Loss: 0.0046\n",
      "Epoch 20/100 | Step 18501/20074 | Loss: 0.1958\n",
      "Epoch 20/100 | Step 18601/20074 | Loss: 2.1135\n",
      "Epoch 20/100 | Step 18701/20074 | Loss: 2.0352\n",
      "Epoch 20/100 | Step 18801/20074 | Loss: 0.4478\n",
      "Epoch 20/100 | Step 18901/20074 | Loss: 0.2770\n",
      "Epoch 20/100 | Step 19001/20074 | Loss: 1.5507\n",
      "Epoch 20/100 | Step 19101/20074 | Loss: 0.2704\n",
      "Epoch 20/100 | Step 19201/20074 | Loss: 0.3705\n",
      "Epoch 20/100 | Step 19301/20074 | Loss: 0.0058\n",
      "Epoch 20/100 | Step 19401/20074 | Loss: 0.0309\n",
      "Epoch 20/100 | Step 19501/20074 | Loss: 0.6854\n",
      "Epoch 20/100 | Step 19601/20074 | Loss: 0.0322\n",
      "Epoch 20/100 | Step 19701/20074 | Loss: 0.7567\n",
      "Epoch 20/100 | Step 19801/20074 | Loss: 0.2345\n",
      "Epoch 20/100 | Step 19901/20074 | Loss: 0.0004\n",
      "Epoch 20/100 | Step 20001/20074 | Loss: 0.3657\n",
      "Epoch 21/100 | Step 1/20074 | Loss: 0.2662\n",
      "Epoch 21/100 | Step 101/20074 | Loss: 1.2055\n",
      "Epoch 21/100 | Step 201/20074 | Loss: 0.3274\n",
      "Epoch 21/100 | Step 301/20074 | Loss: 2.0995\n",
      "Epoch 21/100 | Step 401/20074 | Loss: 1.4228\n",
      "Epoch 21/100 | Step 501/20074 | Loss: 0.0022\n",
      "Epoch 21/100 | Step 601/20074 | Loss: 0.5486\n",
      "Epoch 21/100 | Step 701/20074 | Loss: 0.0207\n",
      "Epoch 21/100 | Step 801/20074 | Loss: 3.7671\n",
      "Epoch 21/100 | Step 901/20074 | Loss: 0.0104\n",
      "Epoch 21/100 | Step 1001/20074 | Loss: 0.0003\n",
      "Epoch 21/100 | Step 1101/20074 | Loss: 0.0001\n",
      "Epoch 21/100 | Step 1201/20074 | Loss: 0.0435\n",
      "Epoch 21/100 | Step 1301/20074 | Loss: 0.3809\n",
      "Epoch 21/100 | Step 1401/20074 | Loss: 0.3601\n",
      "Epoch 21/100 | Step 1501/20074 | Loss: 0.2218\n",
      "Epoch 21/100 | Step 1601/20074 | Loss: 0.0003\n",
      "Epoch 21/100 | Step 1701/20074 | Loss: 0.4005\n",
      "Epoch 21/100 | Step 1801/20074 | Loss: 2.3383\n",
      "Epoch 21/100 | Step 1901/20074 | Loss: 0.1272\n",
      "Epoch 21/100 | Step 2001/20074 | Loss: 3.5127\n",
      "Epoch 21/100 | Step 2101/20074 | Loss: 0.0004\n",
      "Epoch 21/100 | Step 2201/20074 | Loss: 0.0196\n",
      "Epoch 21/100 | Step 2301/20074 | Loss: 3.4080\n",
      "Epoch 21/100 | Step 2401/20074 | Loss: 0.0059\n",
      "Epoch 21/100 | Step 2501/20074 | Loss: 0.2944\n",
      "Epoch 21/100 | Step 2601/20074 | Loss: 0.3143\n",
      "Epoch 21/100 | Step 2701/20074 | Loss: 0.3459\n",
      "Epoch 21/100 | Step 2801/20074 | Loss: 0.0012\n",
      "Epoch 21/100 | Step 2901/20074 | Loss: 1.4176\n",
      "Epoch 21/100 | Step 3001/20074 | Loss: 0.0090\n",
      "Epoch 21/100 | Step 3101/20074 | Loss: 1.2361\n",
      "Epoch 21/100 | Step 3201/20074 | Loss: 0.8039\n",
      "Epoch 21/100 | Step 3301/20074 | Loss: 0.7984\n",
      "Epoch 21/100 | Step 3401/20074 | Loss: 0.1619\n",
      "Epoch 21/100 | Step 3501/20074 | Loss: 0.0018\n",
      "Epoch 21/100 | Step 3601/20074 | Loss: 0.5842\n",
      "Epoch 21/100 | Step 3701/20074 | Loss: 0.0005\n",
      "Epoch 21/100 | Step 3801/20074 | Loss: 0.0499\n",
      "Epoch 21/100 | Step 3901/20074 | Loss: 0.3737\n",
      "Epoch 21/100 | Step 4001/20074 | Loss: 0.1505\n",
      "Epoch 21/100 | Step 4101/20074 | Loss: 0.0004\n",
      "Epoch 21/100 | Step 4201/20074 | Loss: 0.0946\n",
      "Epoch 21/100 | Step 4301/20074 | Loss: 0.1183\n",
      "Epoch 21/100 | Step 4401/20074 | Loss: 0.0500\n",
      "Epoch 21/100 | Step 4501/20074 | Loss: 3.6980\n",
      "Epoch 21/100 | Step 4601/20074 | Loss: 0.0210\n",
      "Epoch 21/100 | Step 4701/20074 | Loss: 2.3804\n",
      "Epoch 21/100 | Step 4801/20074 | Loss: 2.5720\n",
      "Epoch 21/100 | Step 4901/20074 | Loss: 0.0000\n",
      "Epoch 21/100 | Step 5001/20074 | Loss: 1.2367\n",
      "Epoch 21/100 | Step 5101/20074 | Loss: 0.7211\n",
      "Epoch 21/100 | Step 5201/20074 | Loss: 0.4226\n",
      "Epoch 21/100 | Step 5301/20074 | Loss: 2.1319\n",
      "Epoch 21/100 | Step 5401/20074 | Loss: 0.5520\n",
      "Epoch 21/100 | Step 5501/20074 | Loss: 0.9780\n",
      "Epoch 21/100 | Step 5601/20074 | Loss: 0.0115\n",
      "Epoch 21/100 | Step 5701/20074 | Loss: 0.7048\n",
      "Epoch 21/100 | Step 5801/20074 | Loss: 0.4091\n",
      "Epoch 21/100 | Step 5901/20074 | Loss: 0.0165\n",
      "Epoch 21/100 | Step 6001/20074 | Loss: 4.9424\n",
      "Epoch 21/100 | Step 6101/20074 | Loss: 0.2979\n",
      "Epoch 21/100 | Step 6201/20074 | Loss: 0.8277\n",
      "Epoch 21/100 | Step 6301/20074 | Loss: 0.5197\n",
      "Epoch 21/100 | Step 6401/20074 | Loss: 0.4120\n",
      "Epoch 21/100 | Step 6501/20074 | Loss: 1.3261\n",
      "Epoch 21/100 | Step 6601/20074 | Loss: 0.1438\n",
      "Epoch 21/100 | Step 6701/20074 | Loss: 0.0289\n",
      "Epoch 21/100 | Step 6801/20074 | Loss: 0.2072\n",
      "Epoch 21/100 | Step 6901/20074 | Loss: 0.2837\n",
      "Epoch 21/100 | Step 7001/20074 | Loss: 0.0007\n",
      "Epoch 21/100 | Step 7101/20074 | Loss: 0.0474\n",
      "Epoch 21/100 | Step 7201/20074 | Loss: 0.1998\n",
      "Epoch 21/100 | Step 7301/20074 | Loss: 0.0004\n",
      "Epoch 21/100 | Step 7401/20074 | Loss: 0.8445\n",
      "Epoch 21/100 | Step 7501/20074 | Loss: 1.1573\n",
      "Epoch 21/100 | Step 7601/20074 | Loss: 0.0511\n",
      "Epoch 21/100 | Step 7701/20074 | Loss: 0.0930\n",
      "Epoch 21/100 | Step 7801/20074 | Loss: 0.0236\n",
      "Epoch 21/100 | Step 7901/20074 | Loss: 1.0765\n",
      "Epoch 21/100 | Step 8001/20074 | Loss: 0.1240\n",
      "Epoch 21/100 | Step 8101/20074 | Loss: 2.0703\n",
      "Epoch 21/100 | Step 8201/20074 | Loss: 0.0874\n",
      "Epoch 21/100 | Step 8301/20074 | Loss: 0.0028\n",
      "Epoch 21/100 | Step 8401/20074 | Loss: 0.5876\n",
      "Epoch 21/100 | Step 8501/20074 | Loss: 0.0011\n",
      "Epoch 21/100 | Step 8601/20074 | Loss: 0.3160\n",
      "Epoch 21/100 | Step 8701/20074 | Loss: 3.0758\n",
      "Epoch 21/100 | Step 8801/20074 | Loss: 0.4891\n",
      "Epoch 21/100 | Step 8901/20074 | Loss: 0.8073\n",
      "Epoch 21/100 | Step 9001/20074 | Loss: 0.1452\n",
      "Epoch 21/100 | Step 9101/20074 | Loss: 2.4597\n",
      "Epoch 21/100 | Step 9201/20074 | Loss: 0.0023\n",
      "Epoch 21/100 | Step 9301/20074 | Loss: 0.0017\n",
      "Epoch 21/100 | Step 9401/20074 | Loss: 1.4557\n",
      "Epoch 21/100 | Step 9501/20074 | Loss: 0.1244\n",
      "Epoch 21/100 | Step 9601/20074 | Loss: 2.5099\n",
      "Epoch 21/100 | Step 9701/20074 | Loss: 0.0002\n",
      "Epoch 21/100 | Step 9801/20074 | Loss: 0.1865\n",
      "Epoch 21/100 | Step 9901/20074 | Loss: 0.3303\n",
      "Epoch 21/100 | Step 10001/20074 | Loss: 0.0558\n",
      "Epoch 21/100 | Step 10101/20074 | Loss: 0.1082\n",
      "Epoch 21/100 | Step 10201/20074 | Loss: 0.1391\n",
      "Epoch 21/100 | Step 10301/20074 | Loss: 0.0042\n",
      "Epoch 21/100 | Step 10401/20074 | Loss: 0.3097\n",
      "Epoch 21/100 | Step 10501/20074 | Loss: 4.5513\n",
      "Epoch 21/100 | Step 10601/20074 | Loss: 3.2713\n",
      "Epoch 21/100 | Step 10701/20074 | Loss: 0.1162\n",
      "Epoch 21/100 | Step 10801/20074 | Loss: 0.0375\n",
      "Epoch 21/100 | Step 10901/20074 | Loss: 0.0676\n",
      "Epoch 21/100 | Step 11001/20074 | Loss: 3.1433\n",
      "Epoch 21/100 | Step 11101/20074 | Loss: 0.8892\n",
      "Epoch 21/100 | Step 11201/20074 | Loss: 0.0039\n",
      "Epoch 21/100 | Step 11301/20074 | Loss: 0.0005\n",
      "Epoch 21/100 | Step 11401/20074 | Loss: 3.0497\n",
      "Epoch 21/100 | Step 11501/20074 | Loss: 0.8012\n",
      "Epoch 21/100 | Step 11601/20074 | Loss: 0.4425\n",
      "Epoch 21/100 | Step 11701/20074 | Loss: 1.5792\n",
      "Epoch 21/100 | Step 11801/20074 | Loss: 0.4538\n",
      "Epoch 21/100 | Step 11901/20074 | Loss: 0.0009\n",
      "Epoch 21/100 | Step 12001/20074 | Loss: 0.1811\n",
      "Epoch 21/100 | Step 12101/20074 | Loss: 1.5803\n",
      "Epoch 21/100 | Step 12201/20074 | Loss: 0.6207\n",
      "Epoch 21/100 | Step 12301/20074 | Loss: 0.7252\n",
      "Epoch 21/100 | Step 12401/20074 | Loss: 0.7659\n",
      "Epoch 21/100 | Step 12501/20074 | Loss: 2.3882\n",
      "Epoch 21/100 | Step 12601/20074 | Loss: 1.2207\n",
      "Epoch 21/100 | Step 12701/20074 | Loss: 2.1922\n",
      "Epoch 21/100 | Step 12801/20074 | Loss: 0.0218\n",
      "Epoch 21/100 | Step 12901/20074 | Loss: 0.0003\n",
      "Epoch 21/100 | Step 13001/20074 | Loss: 0.5179\n",
      "Epoch 21/100 | Step 13101/20074 | Loss: 0.6472\n",
      "Epoch 21/100 | Step 13201/20074 | Loss: 0.0629\n",
      "Epoch 21/100 | Step 13301/20074 | Loss: 0.2005\n",
      "Epoch 21/100 | Step 13401/20074 | Loss: 0.0667\n",
      "Epoch 21/100 | Step 13501/20074 | Loss: 0.0135\n",
      "Epoch 21/100 | Step 13601/20074 | Loss: 1.0781\n",
      "Epoch 21/100 | Step 13701/20074 | Loss: 0.6263\n",
      "Epoch 21/100 | Step 13801/20074 | Loss: 0.1473\n",
      "Epoch 21/100 | Step 13901/20074 | Loss: 0.0002\n",
      "Epoch 21/100 | Step 14001/20074 | Loss: 0.0091\n",
      "Epoch 21/100 | Step 14101/20074 | Loss: 0.0325\n",
      "Epoch 21/100 | Step 14201/20074 | Loss: 1.0359\n",
      "Epoch 21/100 | Step 14301/20074 | Loss: 1.4049\n",
      "Epoch 21/100 | Step 14401/20074 | Loss: 0.2686\n",
      "Epoch 21/100 | Step 14501/20074 | Loss: 0.5648\n",
      "Epoch 21/100 | Step 14601/20074 | Loss: 0.1554\n",
      "Epoch 21/100 | Step 14701/20074 | Loss: 0.4115\n",
      "Epoch 21/100 | Step 14801/20074 | Loss: 0.2444\n",
      "Epoch 21/100 | Step 14901/20074 | Loss: 4.3399\n",
      "Epoch 21/100 | Step 15001/20074 | Loss: 0.9484\n",
      "Epoch 21/100 | Step 15101/20074 | Loss: 0.7781\n",
      "Epoch 21/100 | Step 15201/20074 | Loss: 0.4782\n",
      "Epoch 21/100 | Step 15301/20074 | Loss: 0.0029\n",
      "Epoch 21/100 | Step 15401/20074 | Loss: 0.0035\n",
      "Epoch 21/100 | Step 15501/20074 | Loss: 0.0008\n",
      "Epoch 21/100 | Step 15601/20074 | Loss: 0.0027\n",
      "Epoch 21/100 | Step 15701/20074 | Loss: 1.0289\n",
      "Epoch 21/100 | Step 15801/20074 | Loss: 3.5844\n",
      "Epoch 21/100 | Step 15901/20074 | Loss: 2.9651\n",
      "Epoch 21/100 | Step 16001/20074 | Loss: 1.4852\n",
      "Epoch 21/100 | Step 16101/20074 | Loss: 0.0022\n",
      "Epoch 21/100 | Step 16201/20074 | Loss: 0.0025\n",
      "Epoch 21/100 | Step 16301/20074 | Loss: 0.0371\n",
      "Epoch 21/100 | Step 16401/20074 | Loss: 0.0075\n",
      "Epoch 21/100 | Step 16501/20074 | Loss: 0.1031\n",
      "Epoch 21/100 | Step 16601/20074 | Loss: 1.1993\n",
      "Epoch 21/100 | Step 16701/20074 | Loss: 2.1137\n",
      "Epoch 21/100 | Step 16801/20074 | Loss: 0.0231\n",
      "Epoch 21/100 | Step 16901/20074 | Loss: 0.0031\n",
      "Epoch 21/100 | Step 17001/20074 | Loss: 1.0347\n",
      "Epoch 21/100 | Step 17101/20074 | Loss: 0.3399\n",
      "Epoch 21/100 | Step 17201/20074 | Loss: 3.8169\n",
      "Epoch 21/100 | Step 17301/20074 | Loss: 0.3728\n",
      "Epoch 21/100 | Step 17401/20074 | Loss: 0.0550\n",
      "Epoch 21/100 | Step 17501/20074 | Loss: 3.4293\n",
      "Epoch 21/100 | Step 17601/20074 | Loss: 0.9913\n",
      "Epoch 21/100 | Step 17701/20074 | Loss: 1.5203\n",
      "Epoch 21/100 | Step 17801/20074 | Loss: 0.4831\n",
      "Epoch 21/100 | Step 17901/20074 | Loss: 3.7961\n",
      "Epoch 21/100 | Step 18001/20074 | Loss: 0.0035\n",
      "Epoch 21/100 | Step 18101/20074 | Loss: 0.6895\n",
      "Epoch 21/100 | Step 18201/20074 | Loss: 0.1329\n",
      "Epoch 21/100 | Step 18301/20074 | Loss: 0.3592\n",
      "Epoch 21/100 | Step 18401/20074 | Loss: 0.8193\n",
      "Epoch 21/100 | Step 18501/20074 | Loss: 6.3146\n",
      "Epoch 21/100 | Step 18601/20074 | Loss: 0.3352\n",
      "Epoch 21/100 | Step 18701/20074 | Loss: 0.2271\n",
      "Epoch 21/100 | Step 18801/20074 | Loss: 0.0000\n",
      "Epoch 21/100 | Step 18901/20074 | Loss: 0.0010\n",
      "Epoch 21/100 | Step 19001/20074 | Loss: 0.5741\n",
      "Epoch 21/100 | Step 19101/20074 | Loss: 0.0312\n",
      "Epoch 21/100 | Step 19201/20074 | Loss: 0.3735\n",
      "Epoch 21/100 | Step 19301/20074 | Loss: 0.1452\n",
      "Epoch 21/100 | Step 19401/20074 | Loss: 0.0605\n",
      "Epoch 21/100 | Step 19501/20074 | Loss: 3.5651\n",
      "Epoch 21/100 | Step 19601/20074 | Loss: 0.1573\n",
      "Epoch 21/100 | Step 19701/20074 | Loss: 0.8633\n",
      "Epoch 21/100 | Step 19801/20074 | Loss: 0.0286\n",
      "Epoch 21/100 | Step 19901/20074 | Loss: 0.3725\n",
      "Epoch 21/100 | Step 20001/20074 | Loss: 0.0059\n",
      "Epoch 22/100 | Step 1/20074 | Loss: 0.6110\n",
      "Epoch 22/100 | Step 101/20074 | Loss: 0.3728\n",
      "Epoch 22/100 | Step 201/20074 | Loss: 0.0005\n",
      "Epoch 22/100 | Step 301/20074 | Loss: 0.3855\n",
      "Epoch 22/100 | Step 401/20074 | Loss: 0.0040\n",
      "Epoch 22/100 | Step 501/20074 | Loss: 0.0630\n",
      "Epoch 22/100 | Step 601/20074 | Loss: 1.1914\n",
      "Epoch 22/100 | Step 701/20074 | Loss: 1.5862\n",
      "Epoch 22/100 | Step 801/20074 | Loss: 0.0028\n",
      "Epoch 22/100 | Step 901/20074 | Loss: 0.5624\n",
      "Epoch 22/100 | Step 1001/20074 | Loss: 0.0990\n",
      "Epoch 22/100 | Step 1101/20074 | Loss: 0.0012\n",
      "Epoch 22/100 | Step 1201/20074 | Loss: 2.7363\n",
      "Epoch 22/100 | Step 1301/20074 | Loss: 0.0000\n",
      "Epoch 22/100 | Step 1401/20074 | Loss: 1.1179\n",
      "Epoch 22/100 | Step 1501/20074 | Loss: 0.2251\n",
      "Epoch 22/100 | Step 1601/20074 | Loss: 2.0359\n",
      "Epoch 22/100 | Step 1701/20074 | Loss: 0.0294\n",
      "Epoch 22/100 | Step 1801/20074 | Loss: 0.4317\n",
      "Epoch 22/100 | Step 1901/20074 | Loss: 3.3606\n",
      "Epoch 22/100 | Step 2001/20074 | Loss: 2.1178\n",
      "Epoch 22/100 | Step 2101/20074 | Loss: 2.1966\n",
      "Epoch 22/100 | Step 2201/20074 | Loss: 1.2252\n",
      "Epoch 22/100 | Step 2301/20074 | Loss: 0.7355\n",
      "Epoch 22/100 | Step 2401/20074 | Loss: 0.5813\n",
      "Epoch 22/100 | Step 2501/20074 | Loss: 0.0002\n",
      "Epoch 22/100 | Step 2601/20074 | Loss: 0.9356\n",
      "Epoch 22/100 | Step 2701/20074 | Loss: 0.0228\n",
      "Epoch 22/100 | Step 2801/20074 | Loss: 0.7144\n",
      "Epoch 22/100 | Step 2901/20074 | Loss: 0.0021\n",
      "Epoch 22/100 | Step 3001/20074 | Loss: 0.0004\n",
      "Epoch 22/100 | Step 3101/20074 | Loss: 0.0002\n",
      "Epoch 22/100 | Step 3201/20074 | Loss: 0.0004\n",
      "Epoch 22/100 | Step 3301/20074 | Loss: 0.0086\n",
      "Epoch 22/100 | Step 3401/20074 | Loss: 0.1195\n",
      "Epoch 22/100 | Step 3501/20074 | Loss: 0.3581\n",
      "Epoch 22/100 | Step 3601/20074 | Loss: 1.3339\n",
      "Epoch 22/100 | Step 3701/20074 | Loss: 0.0819\n",
      "Epoch 22/100 | Step 3801/20074 | Loss: 0.1653\n",
      "Epoch 22/100 | Step 3901/20074 | Loss: 0.0003\n",
      "Epoch 22/100 | Step 4001/20074 | Loss: 0.1070\n",
      "Epoch 22/100 | Step 4101/20074 | Loss: 0.3322\n",
      "Epoch 22/100 | Step 4201/20074 | Loss: 0.3589\n",
      "Epoch 22/100 | Step 4301/20074 | Loss: 0.0319\n",
      "Epoch 22/100 | Step 4401/20074 | Loss: 0.0079\n",
      "Epoch 22/100 | Step 4501/20074 | Loss: 0.2755\n",
      "Epoch 22/100 | Step 4601/20074 | Loss: 0.0006\n",
      "Epoch 22/100 | Step 4701/20074 | Loss: 0.1748\n",
      "Epoch 22/100 | Step 4801/20074 | Loss: 0.0760\n",
      "Epoch 22/100 | Step 4901/20074 | Loss: 0.7423\n",
      "Epoch 22/100 | Step 5001/20074 | Loss: 0.1205\n",
      "Epoch 22/100 | Step 5101/20074 | Loss: 0.3969\n",
      "Epoch 22/100 | Step 5201/20074 | Loss: 4.9836\n",
      "Epoch 22/100 | Step 5301/20074 | Loss: 2.4181\n",
      "Epoch 22/100 | Step 5401/20074 | Loss: 0.4598\n",
      "Epoch 22/100 | Step 5501/20074 | Loss: 0.0001\n",
      "Epoch 22/100 | Step 5601/20074 | Loss: 0.4213\n",
      "Epoch 22/100 | Step 5701/20074 | Loss: 0.7649\n",
      "Epoch 22/100 | Step 5801/20074 | Loss: 0.7547\n",
      "Epoch 22/100 | Step 5901/20074 | Loss: 0.0125\n",
      "Epoch 22/100 | Step 6001/20074 | Loss: 0.1595\n",
      "Epoch 22/100 | Step 6101/20074 | Loss: 0.0222\n",
      "Epoch 22/100 | Step 6201/20074 | Loss: 0.4982\n",
      "Epoch 22/100 | Step 6301/20074 | Loss: 1.4156\n",
      "Epoch 22/100 | Step 6401/20074 | Loss: 0.1742\n",
      "Epoch 22/100 | Step 6501/20074 | Loss: 0.2319\n",
      "Epoch 22/100 | Step 6601/20074 | Loss: 0.0000\n",
      "Epoch 22/100 | Step 6701/20074 | Loss: 0.3309\n",
      "Epoch 22/100 | Step 6801/20074 | Loss: 1.1332\n",
      "Epoch 22/100 | Step 6901/20074 | Loss: 0.0804\n",
      "Epoch 22/100 | Step 7001/20074 | Loss: 0.0013\n",
      "Epoch 22/100 | Step 7101/20074 | Loss: 0.1613\n",
      "Epoch 22/100 | Step 7201/20074 | Loss: 0.1766\n",
      "Epoch 22/100 | Step 7301/20074 | Loss: 4.8906\n",
      "Epoch 22/100 | Step 7401/20074 | Loss: 0.2165\n",
      "Epoch 22/100 | Step 7501/20074 | Loss: 0.2356\n",
      "Epoch 22/100 | Step 7601/20074 | Loss: 1.6774\n",
      "Epoch 22/100 | Step 7701/20074 | Loss: 1.5963\n",
      "Epoch 22/100 | Step 7801/20074 | Loss: 0.0362\n",
      "Epoch 22/100 | Step 7901/20074 | Loss: 0.0019\n",
      "Epoch 22/100 | Step 8001/20074 | Loss: 0.3520\n",
      "Epoch 22/100 | Step 8101/20074 | Loss: 1.6618\n",
      "Epoch 22/100 | Step 8201/20074 | Loss: 0.0005\n",
      "Epoch 22/100 | Step 8301/20074 | Loss: 1.7422\n",
      "Epoch 22/100 | Step 8401/20074 | Loss: 0.0004\n",
      "Epoch 22/100 | Step 8501/20074 | Loss: 0.6262\n",
      "Epoch 22/100 | Step 8601/20074 | Loss: 1.5615\n",
      "Epoch 22/100 | Step 8701/20074 | Loss: 0.0570\n",
      "Epoch 22/100 | Step 8801/20074 | Loss: 0.6474\n",
      "Epoch 22/100 | Step 8901/20074 | Loss: 0.2655\n",
      "Epoch 22/100 | Step 9001/20074 | Loss: 0.0017\n",
      "Epoch 22/100 | Step 9101/20074 | Loss: 0.0001\n",
      "Epoch 22/100 | Step 9201/20074 | Loss: 0.0002\n",
      "Epoch 22/100 | Step 9301/20074 | Loss: 1.7404\n",
      "Epoch 22/100 | Step 9401/20074 | Loss: 0.3925\n",
      "Epoch 22/100 | Step 9501/20074 | Loss: 0.4958\n",
      "Epoch 22/100 | Step 9601/20074 | Loss: 0.1652\n",
      "Epoch 22/100 | Step 9701/20074 | Loss: 0.0019\n",
      "Epoch 22/100 | Step 9801/20074 | Loss: 0.2129\n",
      "Epoch 22/100 | Step 9901/20074 | Loss: 0.0017\n",
      "Epoch 22/100 | Step 10001/20074 | Loss: 2.3535\n",
      "Epoch 22/100 | Step 10101/20074 | Loss: 0.3231\n",
      "Epoch 22/100 | Step 10201/20074 | Loss: 1.3337\n",
      "Epoch 22/100 | Step 10301/20074 | Loss: 0.0012\n",
      "Epoch 22/100 | Step 10401/20074 | Loss: 3.0096\n",
      "Epoch 22/100 | Step 10501/20074 | Loss: 0.2981\n",
      "Epoch 22/100 | Step 10601/20074 | Loss: 0.0676\n",
      "Epoch 22/100 | Step 10701/20074 | Loss: 0.2283\n",
      "Epoch 22/100 | Step 10801/20074 | Loss: 0.0033\n",
      "Epoch 22/100 | Step 10901/20074 | Loss: 4.2375\n",
      "Epoch 22/100 | Step 11001/20074 | Loss: 0.0003\n",
      "Epoch 22/100 | Step 11101/20074 | Loss: 2.9802\n",
      "Epoch 22/100 | Step 11201/20074 | Loss: 0.0015\n",
      "Epoch 22/100 | Step 11301/20074 | Loss: 5.1683\n",
      "Epoch 22/100 | Step 11401/20074 | Loss: 2.1874\n",
      "Epoch 22/100 | Step 11501/20074 | Loss: 0.0009\n",
      "Epoch 22/100 | Step 11601/20074 | Loss: 5.1865\n",
      "Epoch 22/100 | Step 11701/20074 | Loss: 1.0846\n",
      "Epoch 22/100 | Step 11801/20074 | Loss: 1.2624\n",
      "Epoch 22/100 | Step 11901/20074 | Loss: 0.0275\n",
      "Epoch 22/100 | Step 12001/20074 | Loss: 0.9020\n",
      "Epoch 22/100 | Step 12101/20074 | Loss: 2.5329\n",
      "Epoch 22/100 | Step 12201/20074 | Loss: 0.4058\n",
      "Epoch 22/100 | Step 12301/20074 | Loss: 0.3507\n",
      "Epoch 22/100 | Step 12401/20074 | Loss: 0.6358\n",
      "Epoch 22/100 | Step 12501/20074 | Loss: 0.0566\n",
      "Epoch 22/100 | Step 12601/20074 | Loss: 0.0012\n",
      "Epoch 22/100 | Step 12701/20074 | Loss: 0.1836\n",
      "Epoch 22/100 | Step 12801/20074 | Loss: 0.7948\n",
      "Epoch 22/100 | Step 12901/20074 | Loss: 2.5317\n",
      "Epoch 22/100 | Step 13001/20074 | Loss: 0.0655\n",
      "Epoch 22/100 | Step 13101/20074 | Loss: 0.2887\n",
      "Epoch 22/100 | Step 13201/20074 | Loss: 0.1823\n",
      "Epoch 22/100 | Step 13301/20074 | Loss: 0.0317\n",
      "Epoch 22/100 | Step 13401/20074 | Loss: 0.1731\n",
      "Epoch 22/100 | Step 13501/20074 | Loss: 0.4596\n",
      "Epoch 22/100 | Step 13601/20074 | Loss: 0.3531\n",
      "Epoch 22/100 | Step 13701/20074 | Loss: 0.0005\n",
      "Epoch 22/100 | Step 13801/20074 | Loss: 0.0189\n",
      "Epoch 22/100 | Step 13901/20074 | Loss: 0.0002\n",
      "Epoch 22/100 | Step 14001/20074 | Loss: 0.6971\n",
      "Epoch 22/100 | Step 14101/20074 | Loss: 0.9564\n",
      "Epoch 22/100 | Step 14201/20074 | Loss: 0.1544\n",
      "Epoch 22/100 | Step 14301/20074 | Loss: 0.0621\n",
      "Epoch 22/100 | Step 14401/20074 | Loss: 0.0062\n",
      "Epoch 22/100 | Step 14501/20074 | Loss: 1.5304\n",
      "Epoch 22/100 | Step 14601/20074 | Loss: 0.2173\n",
      "Epoch 22/100 | Step 14701/20074 | Loss: 0.0122\n",
      "Epoch 22/100 | Step 14801/20074 | Loss: 2.4432\n",
      "Epoch 22/100 | Step 14901/20074 | Loss: 0.0717\n",
      "Epoch 22/100 | Step 15001/20074 | Loss: 0.2857\n",
      "Epoch 22/100 | Step 15101/20074 | Loss: 0.0450\n",
      "Epoch 22/100 | Step 15201/20074 | Loss: 0.0656\n",
      "Epoch 22/100 | Step 15301/20074 | Loss: 0.5709\n",
      "Epoch 22/100 | Step 15401/20074 | Loss: 0.3140\n",
      "Epoch 22/100 | Step 15501/20074 | Loss: 2.9180\n",
      "Epoch 22/100 | Step 15601/20074 | Loss: 0.0003\n",
      "Epoch 22/100 | Step 15701/20074 | Loss: 0.0530\n",
      "Epoch 22/100 | Step 15801/20074 | Loss: 0.5967\n",
      "Epoch 22/100 | Step 15901/20074 | Loss: 0.0507\n",
      "Epoch 22/100 | Step 16001/20074 | Loss: 3.2709\n",
      "Epoch 22/100 | Step 16101/20074 | Loss: 1.9007\n",
      "Epoch 22/100 | Step 16201/20074 | Loss: 0.0001\n",
      "Epoch 22/100 | Step 16301/20074 | Loss: 1.4846\n",
      "Epoch 22/100 | Step 16401/20074 | Loss: 0.5359\n",
      "Epoch 22/100 | Step 16501/20074 | Loss: 4.2875\n",
      "Epoch 22/100 | Step 16601/20074 | Loss: 0.0090\n",
      "Epoch 22/100 | Step 16701/20074 | Loss: 0.0013\n",
      "Epoch 22/100 | Step 16801/20074 | Loss: 0.6483\n",
      "Epoch 22/100 | Step 16901/20074 | Loss: 1.4006\n",
      "Epoch 22/100 | Step 17001/20074 | Loss: 0.5794\n",
      "Epoch 22/100 | Step 17101/20074 | Loss: 0.0046\n",
      "Epoch 22/100 | Step 17201/20074 | Loss: 1.1181\n",
      "Epoch 22/100 | Step 17301/20074 | Loss: 2.9094\n",
      "Epoch 22/100 | Step 17401/20074 | Loss: 0.7623\n",
      "Epoch 22/100 | Step 17501/20074 | Loss: 0.1687\n",
      "Epoch 22/100 | Step 17601/20074 | Loss: 0.1003\n",
      "Epoch 22/100 | Step 17701/20074 | Loss: 0.0479\n",
      "Epoch 22/100 | Step 17801/20074 | Loss: 2.0266\n",
      "Epoch 22/100 | Step 17901/20074 | Loss: 0.1383\n",
      "Epoch 22/100 | Step 18001/20074 | Loss: 1.2768\n",
      "Epoch 22/100 | Step 18101/20074 | Loss: 0.0148\n",
      "Epoch 22/100 | Step 18201/20074 | Loss: 0.8758\n",
      "Epoch 22/100 | Step 18301/20074 | Loss: 0.0142\n",
      "Epoch 22/100 | Step 18401/20074 | Loss: 0.0240\n",
      "Epoch 22/100 | Step 18501/20074 | Loss: 0.0008\n",
      "Epoch 22/100 | Step 18601/20074 | Loss: 4.3167\n",
      "Epoch 22/100 | Step 18701/20074 | Loss: 0.0049\n",
      "Epoch 22/100 | Step 18801/20074 | Loss: 0.1002\n",
      "Epoch 22/100 | Step 18901/20074 | Loss: 5.5231\n",
      "Epoch 22/100 | Step 19001/20074 | Loss: 0.4103\n",
      "Epoch 22/100 | Step 19101/20074 | Loss: 0.0002\n",
      "Epoch 22/100 | Step 19201/20074 | Loss: 0.7143\n",
      "Epoch 22/100 | Step 19301/20074 | Loss: 0.3934\n",
      "Epoch 22/100 | Step 19401/20074 | Loss: 3.4534\n",
      "Epoch 22/100 | Step 19501/20074 | Loss: 0.0271\n",
      "Epoch 22/100 | Step 19601/20074 | Loss: 0.6676\n",
      "Epoch 22/100 | Step 19701/20074 | Loss: 2.6501\n",
      "Epoch 22/100 | Step 19801/20074 | Loss: 1.4246\n",
      "Epoch 22/100 | Step 19901/20074 | Loss: 0.0098\n",
      "Epoch 22/100 | Step 20001/20074 | Loss: 0.0053\n",
      "Epoch 23/100 | Step 1/20074 | Loss: 0.1777\n",
      "Epoch 23/100 | Step 101/20074 | Loss: 0.4502\n",
      "Epoch 23/100 | Step 201/20074 | Loss: 0.5755\n",
      "Epoch 23/100 | Step 301/20074 | Loss: 0.5633\n",
      "Epoch 23/100 | Step 401/20074 | Loss: 0.0051\n",
      "Epoch 23/100 | Step 501/20074 | Loss: 0.0002\n",
      "Epoch 23/100 | Step 601/20074 | Loss: 0.0016\n",
      "Epoch 23/100 | Step 701/20074 | Loss: 0.0007\n",
      "Epoch 23/100 | Step 801/20074 | Loss: 3.1280\n",
      "Epoch 23/100 | Step 901/20074 | Loss: 0.0231\n",
      "Epoch 23/100 | Step 1001/20074 | Loss: 0.0004\n",
      "Epoch 23/100 | Step 1101/20074 | Loss: 0.0020\n",
      "Epoch 23/100 | Step 1201/20074 | Loss: 0.0540\n",
      "Epoch 23/100 | Step 1301/20074 | Loss: 0.2872\n",
      "Epoch 23/100 | Step 1401/20074 | Loss: 0.4932\n",
      "Epoch 23/100 | Step 1501/20074 | Loss: 1.7893\n",
      "Epoch 23/100 | Step 1601/20074 | Loss: 0.0191\n",
      "Epoch 23/100 | Step 1701/20074 | Loss: 0.0570\n",
      "Epoch 23/100 | Step 1801/20074 | Loss: 0.0002\n",
      "Epoch 23/100 | Step 1901/20074 | Loss: 1.1152\n",
      "Epoch 23/100 | Step 2001/20074 | Loss: 0.0097\n",
      "Epoch 23/100 | Step 2101/20074 | Loss: 0.0011\n",
      "Epoch 23/100 | Step 2201/20074 | Loss: 0.2085\n",
      "Epoch 23/100 | Step 2301/20074 | Loss: 0.3868\n",
      "Epoch 23/100 | Step 2401/20074 | Loss: 1.5505\n",
      "Epoch 23/100 | Step 2501/20074 | Loss: 0.0007\n",
      "Epoch 23/100 | Step 2601/20074 | Loss: 0.1665\n",
      "Epoch 23/100 | Step 2701/20074 | Loss: 1.2561\n",
      "Epoch 23/100 | Step 2801/20074 | Loss: 1.2187\n",
      "Epoch 23/100 | Step 2901/20074 | Loss: 5.2387\n",
      "Epoch 23/100 | Step 3001/20074 | Loss: 1.9084\n",
      "Epoch 23/100 | Step 3101/20074 | Loss: 0.3344\n",
      "Epoch 23/100 | Step 3201/20074 | Loss: 4.3400\n",
      "Epoch 23/100 | Step 3301/20074 | Loss: 0.7782\n",
      "Epoch 23/100 | Step 3401/20074 | Loss: 0.0008\n",
      "Epoch 23/100 | Step 3501/20074 | Loss: 0.0935\n",
      "Epoch 23/100 | Step 3601/20074 | Loss: 0.1497\n",
      "Epoch 23/100 | Step 3701/20074 | Loss: 1.0373\n",
      "Epoch 23/100 | Step 3801/20074 | Loss: 2.2317\n",
      "Epoch 23/100 | Step 3901/20074 | Loss: 0.0017\n",
      "Epoch 23/100 | Step 4001/20074 | Loss: 3.2139\n",
      "Epoch 23/100 | Step 4101/20074 | Loss: 0.4195\n",
      "Epoch 23/100 | Step 4201/20074 | Loss: 1.3075\n",
      "Epoch 23/100 | Step 4301/20074 | Loss: 0.0007\n",
      "Epoch 23/100 | Step 4401/20074 | Loss: 1.4618\n",
      "Epoch 23/100 | Step 4501/20074 | Loss: 0.4379\n",
      "Epoch 23/100 | Step 4601/20074 | Loss: 0.0224\n",
      "Epoch 23/100 | Step 4701/20074 | Loss: 0.0198\n",
      "Epoch 23/100 | Step 4801/20074 | Loss: 0.0008\n",
      "Epoch 23/100 | Step 4901/20074 | Loss: 0.0413\n",
      "Epoch 23/100 | Step 5001/20074 | Loss: 0.0003\n",
      "Epoch 23/100 | Step 5101/20074 | Loss: 0.1585\n",
      "Epoch 23/100 | Step 5201/20074 | Loss: 0.0009\n",
      "Epoch 23/100 | Step 5301/20074 | Loss: 0.2293\n",
      "Epoch 23/100 | Step 5401/20074 | Loss: 0.0830\n",
      "Epoch 23/100 | Step 5501/20074 | Loss: 4.2191\n",
      "Epoch 23/100 | Step 5601/20074 | Loss: 0.0531\n",
      "Epoch 23/100 | Step 5701/20074 | Loss: 0.0555\n",
      "Epoch 23/100 | Step 5801/20074 | Loss: 0.0038\n",
      "Epoch 23/100 | Step 5901/20074 | Loss: 0.0262\n",
      "Epoch 23/100 | Step 6001/20074 | Loss: 1.9511\n",
      "Epoch 23/100 | Step 6101/20074 | Loss: 0.0034\n",
      "Epoch 23/100 | Step 6201/20074 | Loss: 0.2898\n",
      "Epoch 23/100 | Step 6301/20074 | Loss: 0.0043\n",
      "Epoch 23/100 | Step 6401/20074 | Loss: 0.0085\n",
      "Epoch 23/100 | Step 6501/20074 | Loss: 0.0248\n",
      "Epoch 23/100 | Step 6601/20074 | Loss: 0.6000\n",
      "Epoch 23/100 | Step 6701/20074 | Loss: 0.9043\n",
      "Epoch 23/100 | Step 6801/20074 | Loss: 0.2161\n",
      "Epoch 23/100 | Step 6901/20074 | Loss: 0.1240\n",
      "Epoch 23/100 | Step 7001/20074 | Loss: 1.5409\n",
      "Epoch 23/100 | Step 7101/20074 | Loss: 0.3267\n",
      "Epoch 23/100 | Step 7201/20074 | Loss: 0.9586\n",
      "Epoch 23/100 | Step 7301/20074 | Loss: 2.1646\n",
      "Epoch 23/100 | Step 7401/20074 | Loss: 0.1069\n",
      "Epoch 23/100 | Step 7501/20074 | Loss: 3.1418\n",
      "Epoch 23/100 | Step 7601/20074 | Loss: 4.0866\n",
      "Epoch 23/100 | Step 7701/20074 | Loss: 0.1484\n",
      "Epoch 23/100 | Step 7801/20074 | Loss: 0.0075\n",
      "Epoch 23/100 | Step 7901/20074 | Loss: 0.0004\n",
      "Epoch 23/100 | Step 8001/20074 | Loss: 0.0138\n",
      "Epoch 23/100 | Step 8101/20074 | Loss: 0.0004\n",
      "Epoch 23/100 | Step 8201/20074 | Loss: 0.1751\n",
      "Epoch 23/100 | Step 8301/20074 | Loss: 0.6466\n",
      "Epoch 23/100 | Step 8401/20074 | Loss: 0.1435\n",
      "Epoch 23/100 | Step 8501/20074 | Loss: 0.6410\n",
      "Epoch 23/100 | Step 8601/20074 | Loss: 0.3830\n",
      "Epoch 23/100 | Step 8701/20074 | Loss: 0.0001\n",
      "Epoch 23/100 | Step 8801/20074 | Loss: 0.1855\n",
      "Epoch 23/100 | Step 8901/20074 | Loss: 3.5800\n",
      "Epoch 23/100 | Step 9001/20074 | Loss: 0.0006\n",
      "Epoch 23/100 | Step 9101/20074 | Loss: 0.6728\n",
      "Epoch 23/100 | Step 9201/20074 | Loss: 0.0101\n",
      "Epoch 23/100 | Step 9301/20074 | Loss: 0.0018\n",
      "Epoch 23/100 | Step 9401/20074 | Loss: 0.0010\n",
      "Epoch 23/100 | Step 9501/20074 | Loss: 0.9979\n",
      "Epoch 23/100 | Step 9601/20074 | Loss: 0.0019\n",
      "Epoch 23/100 | Step 9701/20074 | Loss: 2.2656\n",
      "Epoch 23/100 | Step 9801/20074 | Loss: 2.4055\n",
      "Epoch 23/100 | Step 9901/20074 | Loss: 0.0002\n",
      "Epoch 23/100 | Step 10001/20074 | Loss: 0.1609\n",
      "Epoch 23/100 | Step 10101/20074 | Loss: 0.1411\n",
      "Epoch 23/100 | Step 10201/20074 | Loss: 0.0795\n",
      "Epoch 23/100 | Step 10301/20074 | Loss: 0.5783\n",
      "Epoch 23/100 | Step 10401/20074 | Loss: 0.0122\n",
      "Epoch 23/100 | Step 10501/20074 | Loss: 3.1997\n",
      "Epoch 23/100 | Step 10601/20074 | Loss: 0.1691\n",
      "Epoch 23/100 | Step 10701/20074 | Loss: 0.1955\n",
      "Epoch 23/100 | Step 10801/20074 | Loss: 0.0546\n",
      "Epoch 23/100 | Step 10901/20074 | Loss: 0.1067\n",
      "Epoch 23/100 | Step 11001/20074 | Loss: 0.3682\n",
      "Epoch 23/100 | Step 11101/20074 | Loss: 0.6489\n",
      "Epoch 23/100 | Step 11201/20074 | Loss: 0.0026\n",
      "Epoch 23/100 | Step 11301/20074 | Loss: 0.1350\n",
      "Epoch 23/100 | Step 11401/20074 | Loss: 0.9967\n",
      "Epoch 23/100 | Step 11501/20074 | Loss: 3.6633\n",
      "Epoch 23/100 | Step 11601/20074 | Loss: 0.7417\n",
      "Epoch 23/100 | Step 11701/20074 | Loss: 0.4466\n",
      "Epoch 23/100 | Step 11801/20074 | Loss: 0.0043\n",
      "Epoch 23/100 | Step 11901/20074 | Loss: 0.0179\n",
      "Epoch 23/100 | Step 12001/20074 | Loss: 0.6102\n",
      "Epoch 23/100 | Step 12101/20074 | Loss: 0.7362\n",
      "Epoch 23/100 | Step 12201/20074 | Loss: 0.6941\n",
      "Epoch 23/100 | Step 12301/20074 | Loss: 0.0027\n",
      "Epoch 23/100 | Step 12401/20074 | Loss: 0.6339\n",
      "Epoch 23/100 | Step 12501/20074 | Loss: 3.1467\n",
      "Epoch 23/100 | Step 12601/20074 | Loss: 1.7837\n",
      "Epoch 23/100 | Step 12701/20074 | Loss: 1.0245\n",
      "Epoch 23/100 | Step 12801/20074 | Loss: 0.0166\n",
      "Epoch 23/100 | Step 12901/20074 | Loss: 0.0521\n",
      "Epoch 23/100 | Step 13001/20074 | Loss: 2.0505\n",
      "Epoch 23/100 | Step 13101/20074 | Loss: 2.5656\n",
      "Epoch 23/100 | Step 13201/20074 | Loss: 4.8692\n",
      "Epoch 23/100 | Step 13301/20074 | Loss: 1.2031\n",
      "Epoch 23/100 | Step 13401/20074 | Loss: 0.0001\n",
      "Epoch 23/100 | Step 13501/20074 | Loss: 0.0009\n",
      "Epoch 23/100 | Step 13601/20074 | Loss: 0.0679\n",
      "Epoch 23/100 | Step 13701/20074 | Loss: 0.1044\n",
      "Epoch 23/100 | Step 13801/20074 | Loss: 0.0010\n",
      "Epoch 23/100 | Step 13901/20074 | Loss: 0.9485\n",
      "Epoch 23/100 | Step 14001/20074 | Loss: 0.4459\n",
      "Epoch 23/100 | Step 14101/20074 | Loss: 0.0881\n",
      "Epoch 23/100 | Step 14201/20074 | Loss: 1.5130\n",
      "Epoch 23/100 | Step 14301/20074 | Loss: 2.5747\n",
      "Epoch 23/100 | Step 14401/20074 | Loss: 0.2079\n",
      "Epoch 23/100 | Step 14501/20074 | Loss: 0.7192\n",
      "Epoch 23/100 | Step 14601/20074 | Loss: 0.0134\n",
      "Epoch 23/100 | Step 14701/20074 | Loss: 0.0000\n",
      "Epoch 23/100 | Step 14801/20074 | Loss: 0.1413\n",
      "Epoch 23/100 | Step 14901/20074 | Loss: 0.3477\n",
      "Epoch 23/100 | Step 15001/20074 | Loss: 2.7790\n",
      "Epoch 23/100 | Step 15101/20074 | Loss: 1.2777\n",
      "Epoch 23/100 | Step 15201/20074 | Loss: 0.0637\n",
      "Epoch 23/100 | Step 15301/20074 | Loss: 0.1419\n",
      "Epoch 23/100 | Step 15401/20074 | Loss: 0.6098\n",
      "Epoch 23/100 | Step 15501/20074 | Loss: 0.6505\n",
      "Epoch 23/100 | Step 15601/20074 | Loss: 0.7524\n",
      "Epoch 23/100 | Step 15701/20074 | Loss: 1.9743\n",
      "Epoch 23/100 | Step 15801/20074 | Loss: 1.9008\n",
      "Epoch 23/100 | Step 15901/20074 | Loss: 0.3867\n",
      "Epoch 23/100 | Step 16001/20074 | Loss: 0.3856\n",
      "Epoch 23/100 | Step 16101/20074 | Loss: 0.0007\n",
      "Epoch 23/100 | Step 16201/20074 | Loss: 0.5455\n",
      "Epoch 23/100 | Step 16301/20074 | Loss: 2.0932\n",
      "Epoch 23/100 | Step 16401/20074 | Loss: 1.4543\n",
      "Epoch 23/100 | Step 16501/20074 | Loss: 0.6709\n",
      "Epoch 23/100 | Step 16601/20074 | Loss: 0.8557\n",
      "Epoch 23/100 | Step 16701/20074 | Loss: 0.7858\n",
      "Epoch 23/100 | Step 16801/20074 | Loss: 0.1716\n",
      "Epoch 23/100 | Step 16901/20074 | Loss: 0.0005\n",
      "Epoch 23/100 | Step 17001/20074 | Loss: 0.0170\n",
      "Epoch 23/100 | Step 17101/20074 | Loss: 0.0000\n",
      "Epoch 23/100 | Step 17201/20074 | Loss: 0.0003\n",
      "Epoch 23/100 | Step 17301/20074 | Loss: 1.3941\n",
      "Epoch 23/100 | Step 17401/20074 | Loss: 0.0006\n",
      "Epoch 23/100 | Step 17501/20074 | Loss: 0.6412\n",
      "Epoch 23/100 | Step 17601/20074 | Loss: 2.8807\n",
      "Epoch 23/100 | Step 17701/20074 | Loss: 0.4023\n",
      "Epoch 23/100 | Step 17801/20074 | Loss: 0.0047\n",
      "Epoch 23/100 | Step 17901/20074 | Loss: 0.0781\n",
      "Epoch 23/100 | Step 18001/20074 | Loss: 1.8681\n",
      "Epoch 23/100 | Step 18101/20074 | Loss: 0.1310\n",
      "Epoch 23/100 | Step 18201/20074 | Loss: 0.2000\n",
      "Epoch 23/100 | Step 18301/20074 | Loss: 0.1050\n",
      "Epoch 23/100 | Step 18401/20074 | Loss: 0.9052\n",
      "Epoch 23/100 | Step 18501/20074 | Loss: 0.0812\n",
      "Epoch 23/100 | Step 18601/20074 | Loss: 2.6736\n",
      "Epoch 23/100 | Step 18701/20074 | Loss: 0.0477\n",
      "Epoch 23/100 | Step 18801/20074 | Loss: 1.5880\n",
      "Epoch 23/100 | Step 18901/20074 | Loss: 0.2068\n",
      "Epoch 23/100 | Step 19001/20074 | Loss: 1.2676\n",
      "Epoch 23/100 | Step 19101/20074 | Loss: 0.7351\n",
      "Epoch 23/100 | Step 19201/20074 | Loss: 0.5531\n",
      "Epoch 23/100 | Step 19301/20074 | Loss: 0.0806\n",
      "Epoch 23/100 | Step 19401/20074 | Loss: 0.0720\n",
      "Epoch 23/100 | Step 19501/20074 | Loss: 0.9537\n",
      "Epoch 23/100 | Step 19601/20074 | Loss: 0.5101\n",
      "Epoch 23/100 | Step 19701/20074 | Loss: 0.3465\n",
      "Epoch 23/100 | Step 19801/20074 | Loss: 0.3618\n",
      "Epoch 23/100 | Step 19901/20074 | Loss: 2.8302\n",
      "Epoch 23/100 | Step 20001/20074 | Loss: 0.2182\n",
      "Epoch 24/100 | Step 1/20074 | Loss: 0.4071\n",
      "Epoch 24/100 | Step 101/20074 | Loss: 0.3713\n",
      "Epoch 24/100 | Step 201/20074 | Loss: 0.8733\n",
      "Epoch 24/100 | Step 301/20074 | Loss: 0.1232\n",
      "Epoch 24/100 | Step 401/20074 | Loss: 0.0309\n",
      "Epoch 24/100 | Step 501/20074 | Loss: 0.9556\n",
      "Epoch 24/100 | Step 601/20074 | Loss: 0.0110\n",
      "Epoch 24/100 | Step 701/20074 | Loss: 0.3112\n",
      "Epoch 24/100 | Step 801/20074 | Loss: 0.3349\n",
      "Epoch 24/100 | Step 901/20074 | Loss: 0.1657\n",
      "Epoch 24/100 | Step 1001/20074 | Loss: 0.1117\n",
      "Epoch 24/100 | Step 1101/20074 | Loss: 2.2990\n",
      "Epoch 24/100 | Step 1201/20074 | Loss: 0.0016\n",
      "Epoch 24/100 | Step 1301/20074 | Loss: 2.2808\n",
      "Epoch 24/100 | Step 1401/20074 | Loss: 0.2701\n",
      "Epoch 24/100 | Step 1501/20074 | Loss: 0.1756\n",
      "Epoch 24/100 | Step 1601/20074 | Loss: 0.0356\n",
      "Epoch 24/100 | Step 1701/20074 | Loss: 0.4599\n",
      "Epoch 24/100 | Step 1801/20074 | Loss: 1.3944\n",
      "Epoch 24/100 | Step 1901/20074 | Loss: 0.0858\n",
      "Epoch 24/100 | Step 2001/20074 | Loss: 0.0028\n",
      "Epoch 24/100 | Step 2101/20074 | Loss: 2.9698\n",
      "Epoch 24/100 | Step 2201/20074 | Loss: 0.1756\n",
      "Epoch 24/100 | Step 2301/20074 | Loss: 0.6584\n",
      "Epoch 24/100 | Step 2401/20074 | Loss: 0.1639\n",
      "Epoch 24/100 | Step 2501/20074 | Loss: 3.6526\n",
      "Epoch 24/100 | Step 2601/20074 | Loss: 0.9268\n",
      "Epoch 24/100 | Step 2701/20074 | Loss: 0.0177\n",
      "Epoch 24/100 | Step 2801/20074 | Loss: 0.4894\n",
      "Epoch 24/100 | Step 2901/20074 | Loss: 0.0215\n",
      "Epoch 24/100 | Step 3001/20074 | Loss: 0.0007\n",
      "Epoch 24/100 | Step 3101/20074 | Loss: 0.3747\n",
      "Epoch 24/100 | Step 3201/20074 | Loss: 0.0003\n",
      "Epoch 24/100 | Step 3301/20074 | Loss: 0.4274\n",
      "Epoch 24/100 | Step 3401/20074 | Loss: 0.2747\n",
      "Epoch 24/100 | Step 3501/20074 | Loss: 1.0607\n",
      "Epoch 24/100 | Step 3601/20074 | Loss: 0.0506\n",
      "Epoch 24/100 | Step 3701/20074 | Loss: 0.1076\n",
      "Epoch 24/100 | Step 3801/20074 | Loss: 0.0004\n",
      "Epoch 24/100 | Step 3901/20074 | Loss: 0.7370\n",
      "Epoch 24/100 | Step 4001/20074 | Loss: 0.2699\n",
      "Epoch 24/100 | Step 4101/20074 | Loss: 0.0835\n",
      "Epoch 24/100 | Step 4201/20074 | Loss: 0.0020\n",
      "Epoch 24/100 | Step 4301/20074 | Loss: 0.0660\n",
      "Epoch 24/100 | Step 4401/20074 | Loss: 0.3391\n",
      "Epoch 24/100 | Step 4501/20074 | Loss: 0.7447\n",
      "Epoch 24/100 | Step 4601/20074 | Loss: 1.1406\n",
      "Epoch 24/100 | Step 4701/20074 | Loss: 0.1654\n",
      "Epoch 24/100 | Step 4801/20074 | Loss: 0.0363\n",
      "Epoch 24/100 | Step 4901/20074 | Loss: 2.5134\n",
      "Epoch 24/100 | Step 5001/20074 | Loss: 0.7315\n",
      "Epoch 24/100 | Step 5101/20074 | Loss: 0.4376\n",
      "Epoch 24/100 | Step 5201/20074 | Loss: 0.6062\n",
      "Epoch 24/100 | Step 5301/20074 | Loss: 0.4556\n",
      "Epoch 24/100 | Step 5401/20074 | Loss: 0.0007\n",
      "Epoch 24/100 | Step 5501/20074 | Loss: 0.0006\n",
      "Epoch 24/100 | Step 5601/20074 | Loss: 0.0472\n",
      "Epoch 24/100 | Step 5701/20074 | Loss: 1.1630\n",
      "Epoch 24/100 | Step 5801/20074 | Loss: 0.8313\n",
      "Epoch 24/100 | Step 5901/20074 | Loss: 1.4634\n",
      "Epoch 24/100 | Step 6001/20074 | Loss: 0.0905\n",
      "Epoch 24/100 | Step 6101/20074 | Loss: 0.1454\n",
      "Epoch 24/100 | Step 6201/20074 | Loss: 1.0366\n",
      "Epoch 24/100 | Step 6301/20074 | Loss: 2.5936\n",
      "Epoch 24/100 | Step 6401/20074 | Loss: 0.0001\n",
      "Epoch 24/100 | Step 6501/20074 | Loss: 2.1166\n",
      "Epoch 24/100 | Step 6601/20074 | Loss: 2.0619\n",
      "Epoch 24/100 | Step 6701/20074 | Loss: 0.4043\n",
      "Epoch 24/100 | Step 6801/20074 | Loss: 0.0858\n",
      "Epoch 24/100 | Step 6901/20074 | Loss: 0.0222\n",
      "Epoch 24/100 | Step 7001/20074 | Loss: 0.5399\n",
      "Epoch 24/100 | Step 7101/20074 | Loss: 0.7450\n",
      "Epoch 24/100 | Step 7201/20074 | Loss: 0.0045\n",
      "Epoch 24/100 | Step 7301/20074 | Loss: 0.0004\n",
      "Epoch 24/100 | Step 7401/20074 | Loss: 1.3131\n",
      "Epoch 24/100 | Step 7501/20074 | Loss: 0.0020\n",
      "Epoch 24/100 | Step 7601/20074 | Loss: 0.0001\n",
      "Epoch 24/100 | Step 7701/20074 | Loss: 4.0426\n",
      "Epoch 24/100 | Step 7801/20074 | Loss: 0.1189\n",
      "Epoch 24/100 | Step 7901/20074 | Loss: 0.0186\n",
      "Epoch 24/100 | Step 8001/20074 | Loss: 0.0001\n",
      "Epoch 24/100 | Step 8101/20074 | Loss: 0.0699\n",
      "Epoch 24/100 | Step 8201/20074 | Loss: 0.0935\n",
      "Epoch 24/100 | Step 8301/20074 | Loss: 0.1467\n",
      "Epoch 24/100 | Step 8401/20074 | Loss: 4.1270\n",
      "Epoch 24/100 | Step 8501/20074 | Loss: 0.2825\n",
      "Epoch 24/100 | Step 8601/20074 | Loss: 0.1262\n",
      "Epoch 24/100 | Step 8701/20074 | Loss: 0.8269\n",
      "Epoch 24/100 | Step 8801/20074 | Loss: 0.0037\n",
      "Epoch 24/100 | Step 8901/20074 | Loss: 0.4051\n",
      "Epoch 24/100 | Step 9001/20074 | Loss: 1.2117\n",
      "Epoch 24/100 | Step 9101/20074 | Loss: 0.0035\n",
      "Epoch 24/100 | Step 9201/20074 | Loss: 3.3601\n",
      "Epoch 24/100 | Step 9301/20074 | Loss: 0.0005\n",
      "Epoch 24/100 | Step 9401/20074 | Loss: 0.0000\n",
      "Epoch 24/100 | Step 9501/20074 | Loss: 0.4196\n",
      "Epoch 24/100 | Step 9601/20074 | Loss: 2.0243\n",
      "Epoch 24/100 | Step 9701/20074 | Loss: 0.4797\n",
      "Epoch 24/100 | Step 9801/20074 | Loss: 1.0040\n",
      "Epoch 24/100 | Step 9901/20074 | Loss: 1.7067\n",
      "Epoch 24/100 | Step 10001/20074 | Loss: 0.6824\n",
      "Epoch 24/100 | Step 10101/20074 | Loss: 0.0043\n",
      "Epoch 24/100 | Step 10201/20074 | Loss: 0.1296\n",
      "Epoch 24/100 | Step 10301/20074 | Loss: 0.1001\n",
      "Epoch 24/100 | Step 10401/20074 | Loss: 0.1256\n",
      "Epoch 24/100 | Step 10501/20074 | Loss: 0.3551\n",
      "Epoch 24/100 | Step 10601/20074 | Loss: 4.5822\n",
      "Epoch 24/100 | Step 10701/20074 | Loss: 0.0624\n",
      "Epoch 24/100 | Step 10801/20074 | Loss: 0.6242\n",
      "Epoch 24/100 | Step 10901/20074 | Loss: 0.4697\n",
      "Epoch 24/100 | Step 11001/20074 | Loss: 3.4565\n",
      "Epoch 24/100 | Step 11101/20074 | Loss: 0.0401\n",
      "Epoch 24/100 | Step 11201/20074 | Loss: 0.0222\n",
      "Epoch 24/100 | Step 11301/20074 | Loss: 0.0127\n",
      "Epoch 24/100 | Step 11401/20074 | Loss: 0.1136\n",
      "Epoch 24/100 | Step 11501/20074 | Loss: 0.1358\n",
      "Epoch 24/100 | Step 11601/20074 | Loss: 2.7841\n",
      "Epoch 24/100 | Step 11701/20074 | Loss: 0.0011\n",
      "Epoch 24/100 | Step 11801/20074 | Loss: 0.4059\n",
      "Epoch 24/100 | Step 11901/20074 | Loss: 0.0066\n",
      "Epoch 24/100 | Step 12001/20074 | Loss: 0.0001\n",
      "Epoch 24/100 | Step 12101/20074 | Loss: 0.2914\n",
      "Epoch 24/100 | Step 12201/20074 | Loss: 0.0003\n",
      "Epoch 24/100 | Step 12301/20074 | Loss: 0.0037\n",
      "Epoch 24/100 | Step 12401/20074 | Loss: 0.0687\n",
      "Epoch 24/100 | Step 12501/20074 | Loss: 0.0265\n",
      "Epoch 24/100 | Step 12601/20074 | Loss: 2.5466\n",
      "Epoch 24/100 | Step 12701/20074 | Loss: 0.0001\n",
      "Epoch 24/100 | Step 12801/20074 | Loss: 1.1822\n",
      "Epoch 24/100 | Step 12901/20074 | Loss: 0.0026\n",
      "Epoch 24/100 | Step 13001/20074 | Loss: 2.0003\n",
      "Epoch 24/100 | Step 13101/20074 | Loss: 0.4575\n",
      "Epoch 24/100 | Step 13201/20074 | Loss: 1.6400\n",
      "Epoch 24/100 | Step 13301/20074 | Loss: 2.2356\n",
      "Epoch 24/100 | Step 13401/20074 | Loss: 0.4586\n",
      "Epoch 24/100 | Step 13501/20074 | Loss: 3.3953\n",
      "Epoch 24/100 | Step 13601/20074 | Loss: 0.6499\n",
      "Epoch 24/100 | Step 13701/20074 | Loss: 0.0376\n",
      "Epoch 24/100 | Step 13801/20074 | Loss: 0.0071\n",
      "Epoch 24/100 | Step 13901/20074 | Loss: 0.0031\n",
      "Epoch 24/100 | Step 14001/20074 | Loss: 0.4490\n",
      "Epoch 24/100 | Step 14101/20074 | Loss: 0.1685\n",
      "Epoch 24/100 | Step 14201/20074 | Loss: 3.9378\n",
      "Epoch 24/100 | Step 14301/20074 | Loss: 1.3364\n",
      "Epoch 24/100 | Step 14401/20074 | Loss: 0.0026\n",
      "Epoch 24/100 | Step 14501/20074 | Loss: 0.5559\n",
      "Epoch 24/100 | Step 14601/20074 | Loss: 0.2878\n",
      "Epoch 24/100 | Step 14701/20074 | Loss: 0.7979\n",
      "Epoch 24/100 | Step 14801/20074 | Loss: 1.1508\n",
      "Epoch 24/100 | Step 14901/20074 | Loss: 0.5300\n",
      "Epoch 24/100 | Step 15001/20074 | Loss: 4.8081\n",
      "Epoch 24/100 | Step 15101/20074 | Loss: 0.0022\n",
      "Epoch 24/100 | Step 15201/20074 | Loss: 0.1332\n",
      "Epoch 24/100 | Step 15301/20074 | Loss: 4.4059\n",
      "Epoch 24/100 | Step 15401/20074 | Loss: 0.0006\n",
      "Epoch 24/100 | Step 15501/20074 | Loss: 1.2961\n",
      "Epoch 24/100 | Step 15601/20074 | Loss: 0.0092\n",
      "Epoch 24/100 | Step 15701/20074 | Loss: 4.2265\n",
      "Epoch 24/100 | Step 15801/20074 | Loss: 0.9637\n",
      "Epoch 24/100 | Step 15901/20074 | Loss: 0.5115\n",
      "Epoch 24/100 | Step 16001/20074 | Loss: 0.4404\n",
      "Epoch 24/100 | Step 16101/20074 | Loss: 0.3087\n",
      "Epoch 24/100 | Step 16201/20074 | Loss: 0.0045\n",
      "Epoch 24/100 | Step 16301/20074 | Loss: 0.0042\n",
      "Epoch 24/100 | Step 16401/20074 | Loss: 0.0524\n",
      "Epoch 24/100 | Step 16501/20074 | Loss: 0.0005\n",
      "Epoch 24/100 | Step 16601/20074 | Loss: 0.2853\n",
      "Epoch 24/100 | Step 16701/20074 | Loss: 0.0027\n",
      "Epoch 24/100 | Step 16801/20074 | Loss: 0.0435\n",
      "Epoch 24/100 | Step 16901/20074 | Loss: 0.0858\n",
      "Epoch 24/100 | Step 17001/20074 | Loss: 0.9591\n",
      "Epoch 24/100 | Step 17101/20074 | Loss: 1.6392\n",
      "Epoch 24/100 | Step 17201/20074 | Loss: 2.0387\n",
      "Epoch 24/100 | Step 17301/20074 | Loss: 2.8891\n",
      "Epoch 24/100 | Step 17401/20074 | Loss: 0.0058\n",
      "Epoch 24/100 | Step 17501/20074 | Loss: 0.8650\n",
      "Epoch 24/100 | Step 17601/20074 | Loss: 0.0010\n",
      "Epoch 24/100 | Step 17701/20074 | Loss: 0.0932\n",
      "Epoch 24/100 | Step 17801/20074 | Loss: 0.3728\n",
      "Epoch 24/100 | Step 17901/20074 | Loss: 0.0077\n",
      "Epoch 24/100 | Step 18001/20074 | Loss: 0.7275\n",
      "Epoch 24/100 | Step 18101/20074 | Loss: 3.6601\n",
      "Epoch 24/100 | Step 18201/20074 | Loss: 0.0053\n",
      "Epoch 24/100 | Step 18301/20074 | Loss: 0.0001\n",
      "Epoch 24/100 | Step 18401/20074 | Loss: 0.3298\n",
      "Epoch 24/100 | Step 18501/20074 | Loss: 1.6194\n",
      "Epoch 24/100 | Step 18601/20074 | Loss: 4.2102\n",
      "Epoch 24/100 | Step 18701/20074 | Loss: 0.0003\n",
      "Epoch 24/100 | Step 18801/20074 | Loss: 5.9926\n",
      "Epoch 24/100 | Step 18901/20074 | Loss: 0.1435\n",
      "Epoch 24/100 | Step 19001/20074 | Loss: 0.0004\n",
      "Epoch 24/100 | Step 19101/20074 | Loss: 0.2605\n",
      "Epoch 24/100 | Step 19201/20074 | Loss: 1.8706\n",
      "Epoch 24/100 | Step 19301/20074 | Loss: 1.1862\n",
      "Epoch 24/100 | Step 19401/20074 | Loss: 0.9268\n",
      "Epoch 24/100 | Step 19501/20074 | Loss: 1.0089\n",
      "Epoch 24/100 | Step 19601/20074 | Loss: 1.2788\n",
      "Epoch 24/100 | Step 19701/20074 | Loss: 0.9586\n",
      "Epoch 24/100 | Step 19801/20074 | Loss: 0.0186\n",
      "Epoch 24/100 | Step 19901/20074 | Loss: 0.0003\n",
      "Epoch 24/100 | Step 20001/20074 | Loss: 4.6689\n",
      "Epoch 25/100 | Step 1/20074 | Loss: 0.2316\n",
      "Epoch 25/100 | Step 101/20074 | Loss: 0.0305\n",
      "Epoch 25/100 | Step 201/20074 | Loss: 0.3673\n",
      "Epoch 25/100 | Step 301/20074 | Loss: 0.9339\n",
      "Epoch 25/100 | Step 401/20074 | Loss: 0.1513\n",
      "Epoch 25/100 | Step 501/20074 | Loss: 0.0405\n",
      "Epoch 25/100 | Step 601/20074 | Loss: 2.7663\n",
      "Epoch 25/100 | Step 701/20074 | Loss: 0.3322\n",
      "Epoch 25/100 | Step 801/20074 | Loss: 0.0003\n",
      "Epoch 25/100 | Step 901/20074 | Loss: 2.6920\n",
      "Epoch 25/100 | Step 1001/20074 | Loss: 0.8899\n",
      "Epoch 25/100 | Step 1101/20074 | Loss: 1.0558\n",
      "Epoch 25/100 | Step 1201/20074 | Loss: 2.2799\n",
      "Epoch 25/100 | Step 1301/20074 | Loss: 0.2840\n",
      "Epoch 25/100 | Step 1401/20074 | Loss: 0.8471\n",
      "Epoch 25/100 | Step 1501/20074 | Loss: 0.5615\n",
      "Epoch 25/100 | Step 1601/20074 | Loss: 0.0038\n",
      "Epoch 25/100 | Step 1701/20074 | Loss: 0.0695\n",
      "Epoch 25/100 | Step 1801/20074 | Loss: 0.9340\n",
      "Epoch 25/100 | Step 1901/20074 | Loss: 0.7563\n",
      "Epoch 25/100 | Step 2001/20074 | Loss: 0.0210\n",
      "Epoch 25/100 | Step 2101/20074 | Loss: 2.2284\n",
      "Epoch 25/100 | Step 2201/20074 | Loss: 0.9925\n",
      "Epoch 25/100 | Step 2301/20074 | Loss: 0.0031\n",
      "Epoch 25/100 | Step 2401/20074 | Loss: 0.3337\n",
      "Epoch 25/100 | Step 2501/20074 | Loss: 0.6267\n",
      "Epoch 25/100 | Step 2601/20074 | Loss: 0.1066\n",
      "Epoch 25/100 | Step 2701/20074 | Loss: 0.0004\n",
      "Epoch 25/100 | Step 2801/20074 | Loss: 0.6748\n",
      "Epoch 25/100 | Step 2901/20074 | Loss: 0.9071\n",
      "Epoch 25/100 | Step 3001/20074 | Loss: 0.0003\n",
      "Epoch 25/100 | Step 3101/20074 | Loss: 0.0789\n",
      "Epoch 25/100 | Step 3201/20074 | Loss: 0.3027\n",
      "Epoch 25/100 | Step 3301/20074 | Loss: 0.4536\n",
      "Epoch 25/100 | Step 3401/20074 | Loss: 0.8748\n",
      "Epoch 25/100 | Step 3501/20074 | Loss: 1.0100\n",
      "Epoch 25/100 | Step 3601/20074 | Loss: 0.0039\n",
      "Epoch 25/100 | Step 3701/20074 | Loss: 0.0667\n",
      "Epoch 25/100 | Step 3801/20074 | Loss: 0.0002\n",
      "Epoch 25/100 | Step 3901/20074 | Loss: 0.0004\n",
      "Epoch 25/100 | Step 4001/20074 | Loss: 1.4621\n",
      "Epoch 25/100 | Step 4101/20074 | Loss: 0.4168\n",
      "Epoch 25/100 | Step 4201/20074 | Loss: 0.1184\n",
      "Epoch 25/100 | Step 4301/20074 | Loss: 0.0026\n",
      "Epoch 25/100 | Step 4401/20074 | Loss: 2.3557\n",
      "Epoch 25/100 | Step 4501/20074 | Loss: 0.0102\n",
      "Epoch 25/100 | Step 4601/20074 | Loss: 2.3597\n",
      "Epoch 25/100 | Step 4701/20074 | Loss: 0.5254\n",
      "Epoch 25/100 | Step 4801/20074 | Loss: 0.5391\n",
      "Epoch 25/100 | Step 4901/20074 | Loss: 0.9829\n",
      "Epoch 25/100 | Step 5001/20074 | Loss: 0.5749\n",
      "Epoch 25/100 | Step 5101/20074 | Loss: 0.0120\n",
      "Epoch 25/100 | Step 5201/20074 | Loss: 0.7650\n",
      "Epoch 25/100 | Step 5301/20074 | Loss: 0.9003\n",
      "Epoch 25/100 | Step 5401/20074 | Loss: 0.5380\n",
      "Epoch 25/100 | Step 5501/20074 | Loss: 0.3863\n",
      "Epoch 25/100 | Step 5601/20074 | Loss: 0.9981\n",
      "Epoch 25/100 | Step 5701/20074 | Loss: 2.3436\n",
      "Epoch 25/100 | Step 5801/20074 | Loss: 0.0305\n",
      "Epoch 25/100 | Step 5901/20074 | Loss: 0.0024\n",
      "Epoch 25/100 | Step 6001/20074 | Loss: 0.3238\n",
      "Epoch 25/100 | Step 6101/20074 | Loss: 0.3715\n",
      "Epoch 25/100 | Step 6201/20074 | Loss: 0.3842\n",
      "Epoch 25/100 | Step 6301/20074 | Loss: 0.0003\n",
      "Epoch 25/100 | Step 6401/20074 | Loss: 0.0890\n",
      "Epoch 25/100 | Step 6501/20074 | Loss: 0.0031\n",
      "Epoch 25/100 | Step 6601/20074 | Loss: 0.1529\n",
      "Epoch 25/100 | Step 6701/20074 | Loss: 0.4170\n",
      "Epoch 25/100 | Step 6801/20074 | Loss: 0.2847\n",
      "Epoch 25/100 | Step 6901/20074 | Loss: 0.6621\n",
      "Epoch 25/100 | Step 7001/20074 | Loss: 0.4519\n",
      "Epoch 25/100 | Step 7101/20074 | Loss: 1.2581\n",
      "Epoch 25/100 | Step 7201/20074 | Loss: 2.7706\n",
      "Epoch 25/100 | Step 7301/20074 | Loss: 0.0967\n",
      "Epoch 25/100 | Step 7401/20074 | Loss: 0.3734\n",
      "Epoch 25/100 | Step 7501/20074 | Loss: 0.3694\n",
      "Epoch 25/100 | Step 7601/20074 | Loss: 4.8251\n",
      "Epoch 25/100 | Step 7701/20074 | Loss: 0.0428\n",
      "Epoch 25/100 | Step 7801/20074 | Loss: 0.0029\n",
      "Epoch 25/100 | Step 7901/20074 | Loss: 0.1087\n",
      "Epoch 25/100 | Step 8001/20074 | Loss: 1.1056\n",
      "Epoch 25/100 | Step 8101/20074 | Loss: 0.7529\n",
      "Epoch 25/100 | Step 8201/20074 | Loss: 0.0245\n",
      "Epoch 25/100 | Step 8301/20074 | Loss: 1.0113\n",
      "Epoch 25/100 | Step 8401/20074 | Loss: 0.0009\n",
      "Epoch 25/100 | Step 8501/20074 | Loss: 0.0007\n",
      "Epoch 25/100 | Step 8601/20074 | Loss: 0.0023\n",
      "Epoch 25/100 | Step 8701/20074 | Loss: 0.0001\n",
      "Epoch 25/100 | Step 8801/20074 | Loss: 0.1640\n",
      "Epoch 25/100 | Step 8901/20074 | Loss: 0.0097\n",
      "Epoch 25/100 | Step 9001/20074 | Loss: 2.7200\n",
      "Epoch 25/100 | Step 9101/20074 | Loss: 0.0109\n",
      "Epoch 25/100 | Step 9201/20074 | Loss: 0.6030\n",
      "Epoch 25/100 | Step 9301/20074 | Loss: 0.0406\n",
      "Epoch 25/100 | Step 9401/20074 | Loss: 0.0006\n",
      "Epoch 25/100 | Step 9501/20074 | Loss: 0.1851\n",
      "Epoch 25/100 | Step 9601/20074 | Loss: 0.0010\n",
      "Epoch 25/100 | Step 9701/20074 | Loss: 0.0000\n",
      "Epoch 25/100 | Step 9801/20074 | Loss: 2.0143\n",
      "Epoch 25/100 | Step 9901/20074 | Loss: 4.0603\n",
      "Epoch 25/100 | Step 10001/20074 | Loss: 3.9413\n",
      "Epoch 25/100 | Step 10101/20074 | Loss: 0.0000\n",
      "Epoch 25/100 | Step 10201/20074 | Loss: 0.7748\n",
      "Epoch 25/100 | Step 10301/20074 | Loss: 0.0085\n",
      "Epoch 25/100 | Step 10401/20074 | Loss: 0.3117\n",
      "Epoch 25/100 | Step 10501/20074 | Loss: 0.2915\n",
      "Epoch 25/100 | Step 10601/20074 | Loss: 0.0002\n",
      "Epoch 25/100 | Step 10701/20074 | Loss: 0.1621\n",
      "Epoch 25/100 | Step 10801/20074 | Loss: 1.5322\n",
      "Epoch 25/100 | Step 10901/20074 | Loss: 0.3908\n",
      "Epoch 25/100 | Step 11001/20074 | Loss: 0.0342\n",
      "Epoch 25/100 | Step 11101/20074 | Loss: 0.0001\n",
      "Epoch 25/100 | Step 11201/20074 | Loss: 0.3096\n",
      "Epoch 25/100 | Step 11301/20074 | Loss: 0.1301\n",
      "Epoch 25/100 | Step 11401/20074 | Loss: 0.0557\n",
      "Epoch 25/100 | Step 11501/20074 | Loss: 0.0561\n",
      "Epoch 25/100 | Step 11601/20074 | Loss: 0.0168\n",
      "Epoch 25/100 | Step 11701/20074 | Loss: 1.7807\n",
      "Epoch 25/100 | Step 11801/20074 | Loss: 0.1131\n",
      "Epoch 25/100 | Step 11901/20074 | Loss: 0.2718\n",
      "Epoch 25/100 | Step 12001/20074 | Loss: 0.0010\n",
      "Epoch 25/100 | Step 12101/20074 | Loss: 0.0000\n",
      "Epoch 25/100 | Step 12201/20074 | Loss: 6.3913\n",
      "Epoch 25/100 | Step 12301/20074 | Loss: 0.0015\n",
      "Epoch 25/100 | Step 12401/20074 | Loss: 0.2289\n",
      "Epoch 25/100 | Step 12501/20074 | Loss: 0.1345\n",
      "Epoch 25/100 | Step 12601/20074 | Loss: 0.0017\n",
      "Epoch 25/100 | Step 12701/20074 | Loss: 2.2665\n",
      "Epoch 25/100 | Step 12801/20074 | Loss: 1.3306\n",
      "Epoch 25/100 | Step 12901/20074 | Loss: 1.7270\n",
      "Epoch 25/100 | Step 13001/20074 | Loss: 0.7127\n",
      "Epoch 25/100 | Step 13101/20074 | Loss: 0.0045\n",
      "Epoch 25/100 | Step 13201/20074 | Loss: 0.1481\n",
      "Epoch 25/100 | Step 13301/20074 | Loss: 0.0530\n",
      "Epoch 25/100 | Step 13401/20074 | Loss: 0.0003\n",
      "Epoch 25/100 | Step 13501/20074 | Loss: 0.3036\n",
      "Epoch 25/100 | Step 13601/20074 | Loss: 0.0002\n",
      "Epoch 25/100 | Step 13701/20074 | Loss: 0.0047\n",
      "Epoch 25/100 | Step 13801/20074 | Loss: 0.0607\n",
      "Epoch 25/100 | Step 13901/20074 | Loss: 1.0130\n",
      "Epoch 25/100 | Step 14001/20074 | Loss: 0.0075\n",
      "Epoch 25/100 | Step 14101/20074 | Loss: 1.6424\n",
      "Epoch 25/100 | Step 14201/20074 | Loss: 1.7215\n",
      "Epoch 25/100 | Step 14301/20074 | Loss: 0.0008\n",
      "Epoch 25/100 | Step 14401/20074 | Loss: 1.3412\n",
      "Epoch 25/100 | Step 14501/20074 | Loss: 0.0006\n",
      "Epoch 25/100 | Step 14601/20074 | Loss: 0.6235\n",
      "Epoch 25/100 | Step 14701/20074 | Loss: 0.3020\n",
      "Epoch 25/100 | Step 14801/20074 | Loss: 0.0004\n",
      "Epoch 25/100 | Step 14901/20074 | Loss: 0.0060\n",
      "Epoch 25/100 | Step 15001/20074 | Loss: 0.1815\n",
      "Epoch 25/100 | Step 15101/20074 | Loss: 0.4171\n",
      "Epoch 25/100 | Step 15201/20074 | Loss: 0.3541\n",
      "Epoch 25/100 | Step 15301/20074 | Loss: 1.1902\n",
      "Epoch 25/100 | Step 15401/20074 | Loss: 4.0008\n",
      "Epoch 25/100 | Step 15501/20074 | Loss: 0.4236\n",
      "Epoch 25/100 | Step 15601/20074 | Loss: 0.0030\n",
      "Epoch 25/100 | Step 15701/20074 | Loss: 0.0005\n",
      "Epoch 25/100 | Step 15801/20074 | Loss: 0.0043\n",
      "Epoch 25/100 | Step 15901/20074 | Loss: 0.3463\n",
      "Epoch 25/100 | Step 16001/20074 | Loss: 0.3823\n",
      "Epoch 25/100 | Step 16101/20074 | Loss: 0.0007\n",
      "Epoch 25/100 | Step 16201/20074 | Loss: 0.0432\n",
      "Epoch 25/100 | Step 16301/20074 | Loss: 0.0488\n",
      "Epoch 25/100 | Step 16401/20074 | Loss: 0.0104\n",
      "Epoch 25/100 | Step 16501/20074 | Loss: 0.1486\n",
      "Epoch 25/100 | Step 16601/20074 | Loss: 0.1371\n",
      "Epoch 25/100 | Step 16701/20074 | Loss: 0.0671\n",
      "Epoch 25/100 | Step 16801/20074 | Loss: 0.1215\n",
      "Epoch 25/100 | Step 16901/20074 | Loss: 0.0618\n",
      "Epoch 25/100 | Step 17001/20074 | Loss: 0.1376\n",
      "Epoch 25/100 | Step 17101/20074 | Loss: 0.0182\n",
      "Epoch 25/100 | Step 17201/20074 | Loss: 0.1171\n",
      "Epoch 25/100 | Step 17301/20074 | Loss: 0.0730\n",
      "Epoch 25/100 | Step 17401/20074 | Loss: 0.7706\n",
      "Epoch 25/100 | Step 17501/20074 | Loss: 0.6311\n",
      "Epoch 25/100 | Step 17601/20074 | Loss: 0.0032\n",
      "Epoch 25/100 | Step 17701/20074 | Loss: 0.3254\n",
      "Epoch 25/100 | Step 17801/20074 | Loss: 0.4411\n",
      "Epoch 25/100 | Step 17901/20074 | Loss: 1.0533\n",
      "Epoch 25/100 | Step 18001/20074 | Loss: 1.5339\n",
      "Epoch 25/100 | Step 18101/20074 | Loss: 0.0008\n",
      "Epoch 25/100 | Step 18201/20074 | Loss: 0.5175\n",
      "Epoch 25/100 | Step 18301/20074 | Loss: 3.2796\n",
      "Epoch 25/100 | Step 18401/20074 | Loss: 0.2044\n",
      "Epoch 25/100 | Step 18501/20074 | Loss: 0.5097\n",
      "Epoch 25/100 | Step 18601/20074 | Loss: 0.2432\n",
      "Epoch 25/100 | Step 18701/20074 | Loss: 0.2484\n",
      "Epoch 25/100 | Step 18801/20074 | Loss: 0.0041\n",
      "Epoch 25/100 | Step 18901/20074 | Loss: 0.0438\n",
      "Epoch 25/100 | Step 19001/20074 | Loss: 0.2077\n",
      "Epoch 25/100 | Step 19101/20074 | Loss: 0.0073\n",
      "Epoch 25/100 | Step 19201/20074 | Loss: 0.0005\n",
      "Epoch 25/100 | Step 19301/20074 | Loss: 2.4991\n",
      "Epoch 25/100 | Step 19401/20074 | Loss: 1.5071\n",
      "Epoch 25/100 | Step 19501/20074 | Loss: 0.2993\n",
      "Epoch 25/100 | Step 19601/20074 | Loss: 4.0801\n",
      "Epoch 25/100 | Step 19701/20074 | Loss: 0.0001\n",
      "Epoch 25/100 | Step 19801/20074 | Loss: 0.1903\n",
      "Epoch 25/100 | Step 19901/20074 | Loss: 1.0390\n",
      "Epoch 25/100 | Step 20001/20074 | Loss: 0.0007\n",
      "Epoch 26/100 | Step 1/20074 | Loss: 0.0005\n",
      "Epoch 26/100 | Step 101/20074 | Loss: 0.0881\n",
      "Epoch 26/100 | Step 201/20074 | Loss: 0.0021\n",
      "Epoch 26/100 | Step 301/20074 | Loss: 0.0001\n",
      "Epoch 26/100 | Step 401/20074 | Loss: 2.4329\n",
      "Epoch 26/100 | Step 501/20074 | Loss: 0.2800\n",
      "Epoch 26/100 | Step 601/20074 | Loss: 0.0874\n",
      "Epoch 26/100 | Step 701/20074 | Loss: 0.2669\n",
      "Epoch 26/100 | Step 801/20074 | Loss: 0.0054\n",
      "Epoch 26/100 | Step 901/20074 | Loss: 0.1801\n",
      "Epoch 26/100 | Step 1001/20074 | Loss: 0.3488\n",
      "Epoch 26/100 | Step 1101/20074 | Loss: 0.0004\n",
      "Epoch 26/100 | Step 1201/20074 | Loss: 0.5199\n",
      "Epoch 26/100 | Step 1301/20074 | Loss: 0.0857\n",
      "Epoch 26/100 | Step 1401/20074 | Loss: 0.5213\n",
      "Epoch 26/100 | Step 1501/20074 | Loss: 0.0002\n",
      "Epoch 26/100 | Step 1601/20074 | Loss: 0.1925\n",
      "Epoch 26/100 | Step 1701/20074 | Loss: 0.0014\n",
      "Epoch 26/100 | Step 1801/20074 | Loss: 4.4274\n",
      "Epoch 26/100 | Step 1901/20074 | Loss: 1.2937\n",
      "Epoch 26/100 | Step 2001/20074 | Loss: 0.3132\n",
      "Epoch 26/100 | Step 2101/20074 | Loss: 0.0042\n",
      "Epoch 26/100 | Step 2201/20074 | Loss: 1.8375\n",
      "Epoch 26/100 | Step 2301/20074 | Loss: 0.0432\n",
      "Epoch 26/100 | Step 2401/20074 | Loss: 0.0868\n",
      "Epoch 26/100 | Step 2501/20074 | Loss: 2.0538\n",
      "Epoch 26/100 | Step 2601/20074 | Loss: 2.7010\n",
      "Epoch 26/100 | Step 2701/20074 | Loss: 0.6254\n",
      "Epoch 26/100 | Step 2801/20074 | Loss: 0.9136\n",
      "Epoch 26/100 | Step 2901/20074 | Loss: 0.2234\n",
      "Epoch 26/100 | Step 3001/20074 | Loss: 2.2840\n",
      "Epoch 26/100 | Step 3101/20074 | Loss: 0.0001\n",
      "Epoch 26/100 | Step 3201/20074 | Loss: 0.0069\n",
      "Epoch 26/100 | Step 3301/20074 | Loss: 1.2375\n",
      "Epoch 26/100 | Step 3401/20074 | Loss: 0.1506\n",
      "Epoch 26/100 | Step 3501/20074 | Loss: 0.0270\n",
      "Epoch 26/100 | Step 3601/20074 | Loss: 3.2415\n",
      "Epoch 26/100 | Step 3701/20074 | Loss: 0.0076\n",
      "Epoch 26/100 | Step 3801/20074 | Loss: 2.9286\n",
      "Epoch 26/100 | Step 3901/20074 | Loss: 3.6745\n",
      "Epoch 26/100 | Step 4001/20074 | Loss: 3.1203\n",
      "Epoch 26/100 | Step 4101/20074 | Loss: 0.0519\n",
      "Epoch 26/100 | Step 4201/20074 | Loss: 0.0416\n",
      "Epoch 26/100 | Step 4301/20074 | Loss: 0.0270\n",
      "Epoch 26/100 | Step 4401/20074 | Loss: 0.1163\n",
      "Epoch 26/100 | Step 4501/20074 | Loss: 0.2693\n",
      "Epoch 26/100 | Step 4601/20074 | Loss: 0.0440\n",
      "Epoch 26/100 | Step 4701/20074 | Loss: 0.0030\n",
      "Epoch 26/100 | Step 4801/20074 | Loss: 0.0003\n",
      "Epoch 26/100 | Step 4901/20074 | Loss: 2.2348\n",
      "Epoch 26/100 | Step 5001/20074 | Loss: 2.1975\n",
      "Epoch 26/100 | Step 5101/20074 | Loss: 0.4892\n",
      "Epoch 26/100 | Step 5201/20074 | Loss: 1.1398\n",
      "Epoch 26/100 | Step 5301/20074 | Loss: 0.7530\n",
      "Epoch 26/100 | Step 5401/20074 | Loss: 0.0179\n",
      "Epoch 26/100 | Step 5501/20074 | Loss: 0.1516\n",
      "Epoch 26/100 | Step 5601/20074 | Loss: 0.0014\n",
      "Epoch 26/100 | Step 5701/20074 | Loss: 0.0021\n",
      "Epoch 26/100 | Step 5801/20074 | Loss: 1.5080\n",
      "Epoch 26/100 | Step 5901/20074 | Loss: 0.0266\n",
      "Epoch 26/100 | Step 6001/20074 | Loss: 0.1689\n",
      "Epoch 26/100 | Step 6101/20074 | Loss: 0.3183\n",
      "Epoch 26/100 | Step 6201/20074 | Loss: 0.0532\n",
      "Epoch 26/100 | Step 6301/20074 | Loss: 0.0009\n",
      "Epoch 26/100 | Step 6401/20074 | Loss: 0.0339\n",
      "Epoch 26/100 | Step 6501/20074 | Loss: 1.0873\n",
      "Epoch 26/100 | Step 6601/20074 | Loss: 1.3828\n",
      "Epoch 26/100 | Step 6701/20074 | Loss: 0.5156\n",
      "Epoch 26/100 | Step 6801/20074 | Loss: 6.9107\n",
      "Epoch 26/100 | Step 6901/20074 | Loss: 1.0058\n",
      "Epoch 26/100 | Step 7001/20074 | Loss: 0.1505\n",
      "Epoch 26/100 | Step 7101/20074 | Loss: 0.2106\n",
      "Epoch 26/100 | Step 7201/20074 | Loss: 1.1916\n",
      "Epoch 26/100 | Step 7301/20074 | Loss: 0.8684\n",
      "Epoch 26/100 | Step 7401/20074 | Loss: 0.0145\n",
      "Epoch 26/100 | Step 7501/20074 | Loss: 0.2204\n",
      "Epoch 26/100 | Step 7601/20074 | Loss: 0.4898\n",
      "Epoch 26/100 | Step 7701/20074 | Loss: 0.8272\n",
      "Epoch 26/100 | Step 7801/20074 | Loss: 0.0029\n",
      "Epoch 26/100 | Step 7901/20074 | Loss: 0.0532\n",
      "Epoch 26/100 | Step 8001/20074 | Loss: 0.3103\n",
      "Epoch 26/100 | Step 8101/20074 | Loss: 0.1446\n",
      "Epoch 26/100 | Step 8201/20074 | Loss: 1.1134\n",
      "Epoch 26/100 | Step 8301/20074 | Loss: 0.0315\n",
      "Epoch 26/100 | Step 8401/20074 | Loss: 2.4244\n",
      "Epoch 26/100 | Step 8501/20074 | Loss: 3.5177\n",
      "Epoch 26/100 | Step 8601/20074 | Loss: 0.1316\n",
      "Epoch 26/100 | Step 8701/20074 | Loss: 2.0543\n",
      "Epoch 26/100 | Step 8801/20074 | Loss: 0.3877\n",
      "Epoch 26/100 | Step 8901/20074 | Loss: 1.5853\n",
      "Epoch 26/100 | Step 9001/20074 | Loss: 0.0801\n",
      "Epoch 26/100 | Step 9101/20074 | Loss: 0.7675\n",
      "Epoch 26/100 | Step 9201/20074 | Loss: 0.5183\n",
      "Epoch 26/100 | Step 9301/20074 | Loss: 0.0150\n",
      "Epoch 26/100 | Step 9401/20074 | Loss: 0.0559\n",
      "Epoch 26/100 | Step 9501/20074 | Loss: 0.1312\n",
      "Epoch 26/100 | Step 9601/20074 | Loss: 0.9742\n",
      "Epoch 26/100 | Step 9701/20074 | Loss: 0.0004\n",
      "Epoch 26/100 | Step 9801/20074 | Loss: 1.2546\n",
      "Epoch 26/100 | Step 9901/20074 | Loss: 0.9667\n",
      "Epoch 26/100 | Step 10001/20074 | Loss: 0.3384\n",
      "Epoch 26/100 | Step 10101/20074 | Loss: 0.1673\n",
      "Epoch 26/100 | Step 10201/20074 | Loss: 0.3020\n",
      "Epoch 26/100 | Step 10301/20074 | Loss: 0.0234\n",
      "Epoch 26/100 | Step 10401/20074 | Loss: 0.5156\n",
      "Epoch 26/100 | Step 10501/20074 | Loss: 0.2085\n",
      "Epoch 26/100 | Step 10601/20074 | Loss: 1.9250\n",
      "Epoch 26/100 | Step 10701/20074 | Loss: 0.0003\n",
      "Epoch 26/100 | Step 10801/20074 | Loss: 0.0706\n",
      "Epoch 26/100 | Step 10901/20074 | Loss: 0.2571\n",
      "Epoch 26/100 | Step 11001/20074 | Loss: 0.0011\n",
      "Epoch 26/100 | Step 11101/20074 | Loss: 0.0707\n",
      "Epoch 26/100 | Step 11201/20074 | Loss: 0.2938\n",
      "Epoch 26/100 | Step 11301/20074 | Loss: 0.0579\n",
      "Epoch 26/100 | Step 11401/20074 | Loss: 0.6421\n",
      "Epoch 26/100 | Step 11501/20074 | Loss: 0.5581\n",
      "Epoch 26/100 | Step 11601/20074 | Loss: 0.2809\n",
      "Epoch 26/100 | Step 11701/20074 | Loss: 0.1293\n",
      "Epoch 26/100 | Step 11801/20074 | Loss: 1.1116\n",
      "Epoch 26/100 | Step 11901/20074 | Loss: 0.0080\n",
      "Epoch 26/100 | Step 12001/20074 | Loss: 1.2055\n",
      "Epoch 26/100 | Step 12101/20074 | Loss: 0.0187\n",
      "Epoch 26/100 | Step 12201/20074 | Loss: 0.0008\n",
      "Epoch 26/100 | Step 12301/20074 | Loss: 0.0108\n",
      "Epoch 26/100 | Step 12401/20074 | Loss: 2.0459\n",
      "Epoch 26/100 | Step 12501/20074 | Loss: 0.5734\n",
      "Epoch 26/100 | Step 12601/20074 | Loss: 0.0207\n",
      "Epoch 26/100 | Step 12701/20074 | Loss: 3.8376\n",
      "Epoch 26/100 | Step 12801/20074 | Loss: 0.0103\n",
      "Epoch 26/100 | Step 12901/20074 | Loss: 2.7637\n",
      "Epoch 26/100 | Step 13001/20074 | Loss: 1.1642\n",
      "Epoch 26/100 | Step 13101/20074 | Loss: 0.2733\n",
      "Epoch 26/100 | Step 13201/20074 | Loss: 0.1439\n",
      "Epoch 26/100 | Step 13301/20074 | Loss: 0.0012\n",
      "Epoch 26/100 | Step 13401/20074 | Loss: 0.8579\n",
      "Epoch 26/100 | Step 13501/20074 | Loss: 0.9088\n",
      "Epoch 26/100 | Step 13601/20074 | Loss: 0.0003\n",
      "Epoch 26/100 | Step 13701/20074 | Loss: 0.7068\n",
      "Epoch 26/100 | Step 13801/20074 | Loss: 2.2892\n",
      "Epoch 26/100 | Step 13901/20074 | Loss: 0.0051\n",
      "Epoch 26/100 | Step 14001/20074 | Loss: 0.1479\n",
      "Epoch 26/100 | Step 14101/20074 | Loss: 0.0004\n",
      "Epoch 26/100 | Step 14201/20074 | Loss: 1.8236\n",
      "Epoch 26/100 | Step 14301/20074 | Loss: 0.0008\n",
      "Epoch 26/100 | Step 14401/20074 | Loss: 0.3312\n",
      "Epoch 26/100 | Step 14501/20074 | Loss: 0.0601\n",
      "Epoch 26/100 | Step 14601/20074 | Loss: 0.0008\n",
      "Epoch 26/100 | Step 14701/20074 | Loss: 0.0667\n",
      "Epoch 26/100 | Step 14801/20074 | Loss: 0.8794\n",
      "Epoch 26/100 | Step 14901/20074 | Loss: 1.9108\n",
      "Epoch 26/100 | Step 15001/20074 | Loss: 0.0003\n",
      "Epoch 26/100 | Step 15101/20074 | Loss: 0.3551\n",
      "Epoch 26/100 | Step 15201/20074 | Loss: 5.0275\n",
      "Epoch 26/100 | Step 15301/20074 | Loss: 1.5062\n",
      "Epoch 26/100 | Step 15401/20074 | Loss: 0.0007\n",
      "Epoch 26/100 | Step 15501/20074 | Loss: 0.2967\n",
      "Epoch 26/100 | Step 15601/20074 | Loss: 1.6649\n",
      "Epoch 26/100 | Step 15701/20074 | Loss: 0.4012\n",
      "Epoch 26/100 | Step 15801/20074 | Loss: 1.0335\n",
      "Epoch 26/100 | Step 15901/20074 | Loss: 0.0002\n",
      "Epoch 26/100 | Step 16001/20074 | Loss: 0.3795\n",
      "Epoch 26/100 | Step 16101/20074 | Loss: 0.0052\n",
      "Epoch 26/100 | Step 16201/20074 | Loss: 0.0035\n",
      "Epoch 26/100 | Step 16301/20074 | Loss: 0.0012\n",
      "Epoch 26/100 | Step 16401/20074 | Loss: 0.1051\n",
      "Epoch 26/100 | Step 16501/20074 | Loss: 0.0251\n",
      "Epoch 26/100 | Step 16601/20074 | Loss: 4.8288\n",
      "Epoch 26/100 | Step 16701/20074 | Loss: 0.9288\n",
      "Epoch 26/100 | Step 16801/20074 | Loss: 0.0370\n",
      "Epoch 26/100 | Step 16901/20074 | Loss: 0.0110\n",
      "Epoch 26/100 | Step 17001/20074 | Loss: 1.9700\n",
      "Epoch 26/100 | Step 17101/20074 | Loss: 0.0910\n",
      "Epoch 26/100 | Step 17201/20074 | Loss: 0.3229\n",
      "Epoch 26/100 | Step 17301/20074 | Loss: 0.1867\n",
      "Epoch 26/100 | Step 17401/20074 | Loss: 1.0989\n",
      "Epoch 26/100 | Step 17501/20074 | Loss: 3.0061\n",
      "Epoch 26/100 | Step 17601/20074 | Loss: 1.8568\n",
      "Epoch 26/100 | Step 17701/20074 | Loss: 0.3786\n",
      "Epoch 26/100 | Step 17801/20074 | Loss: 0.0000\n",
      "Epoch 26/100 | Step 17901/20074 | Loss: 0.0458\n",
      "Epoch 26/100 | Step 18001/20074 | Loss: 0.0375\n",
      "Epoch 26/100 | Step 18101/20074 | Loss: 1.2024\n",
      "Epoch 26/100 | Step 18201/20074 | Loss: 0.1300\n",
      "Epoch 26/100 | Step 18301/20074 | Loss: 0.2414\n",
      "Epoch 26/100 | Step 18401/20074 | Loss: 0.3465\n",
      "Epoch 26/100 | Step 18501/20074 | Loss: 0.0408\n",
      "Epoch 26/100 | Step 18601/20074 | Loss: 0.1520\n",
      "Epoch 26/100 | Step 18701/20074 | Loss: 0.0997\n",
      "Epoch 26/100 | Step 18801/20074 | Loss: 0.3554\n",
      "Epoch 26/100 | Step 18901/20074 | Loss: 0.0110\n",
      "Epoch 26/100 | Step 19001/20074 | Loss: 0.0009\n",
      "Epoch 26/100 | Step 19101/20074 | Loss: 0.0001\n",
      "Epoch 26/100 | Step 19201/20074 | Loss: 1.7572\n",
      "Epoch 26/100 | Step 19301/20074 | Loss: 0.0028\n",
      "Epoch 26/100 | Step 19401/20074 | Loss: 0.7959\n",
      "Epoch 26/100 | Step 19501/20074 | Loss: 1.9367\n",
      "Epoch 26/100 | Step 19601/20074 | Loss: 0.0006\n",
      "Epoch 26/100 | Step 19701/20074 | Loss: 0.3587\n",
      "Epoch 26/100 | Step 19801/20074 | Loss: 0.0293\n",
      "Epoch 26/100 | Step 19901/20074 | Loss: 0.3628\n",
      "Epoch 26/100 | Step 20001/20074 | Loss: 2.3856\n",
      "Epoch 27/100 | Step 1/20074 | Loss: 0.0001\n",
      "Epoch 27/100 | Step 101/20074 | Loss: 2.5181\n",
      "Epoch 27/100 | Step 201/20074 | Loss: 0.0005\n",
      "Epoch 27/100 | Step 301/20074 | Loss: 0.0006\n",
      "Epoch 27/100 | Step 401/20074 | Loss: 7.7396\n",
      "Epoch 27/100 | Step 501/20074 | Loss: 0.0041\n",
      "Epoch 27/100 | Step 601/20074 | Loss: 1.1267\n",
      "Epoch 27/100 | Step 701/20074 | Loss: 0.0007\n",
      "Epoch 27/100 | Step 801/20074 | Loss: 0.6832\n",
      "Epoch 27/100 | Step 901/20074 | Loss: 0.7176\n",
      "Epoch 27/100 | Step 1001/20074 | Loss: 0.2624\n",
      "Epoch 27/100 | Step 1101/20074 | Loss: 0.0031\n",
      "Epoch 27/100 | Step 1201/20074 | Loss: 3.9788\n",
      "Epoch 27/100 | Step 1301/20074 | Loss: 1.4535\n",
      "Epoch 27/100 | Step 1401/20074 | Loss: 0.0381\n",
      "Epoch 27/100 | Step 1501/20074 | Loss: 0.0159\n",
      "Epoch 27/100 | Step 1601/20074 | Loss: 0.0019\n",
      "Epoch 27/100 | Step 1701/20074 | Loss: 0.0001\n",
      "Epoch 27/100 | Step 1801/20074 | Loss: 0.2439\n",
      "Epoch 27/100 | Step 1901/20074 | Loss: 0.0002\n",
      "Epoch 27/100 | Step 2001/20074 | Loss: 0.2965\n",
      "Epoch 27/100 | Step 2101/20074 | Loss: 1.8386\n",
      "Epoch 27/100 | Step 2201/20074 | Loss: 0.0293\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[31], line 11\u001B[0m\n\u001B[1;32m      9\u001B[0m h \u001B[38;5;241m=\u001B[39m h_0\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]):\n\u001B[0;32m---> 11\u001B[0m     o, h \u001B[38;5;241m=\u001B[39m \u001B[43mrnn_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mh\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     12\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_ftn(o, y)\n\u001B[1;32m     13\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[0;32m~/.pyenv/versions/fivessun/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/workspace/fivessun/library/deeplearning/layer/rnn.py:44\u001B[0m, in \u001B[0;36mBasicRNN.forward\u001B[0;34m(self, x, h)\u001B[0m\n\u001B[1;32m     39\u001B[0m combined \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat((x, h), \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     41\u001B[0m \u001B[38;5;66;03m# It is possible to use one layer by combining i2h and i2o.\u001B[39;00m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;66;03m# h = self.activation_ftn(self.i2h(combined))\u001B[39;00m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;66;03m# y = self.activation_ftn(self.i2o(combined))\u001B[39;00m\n\u001B[0;32m---> 44\u001B[0m h \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mi2h\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcombined\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     45\u001B[0m y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mi2o(combined)\n\u001B[1;32m     46\u001B[0m y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msoftmax(y)  \u001B[38;5;66;03m# probability distribution\u001B[39;00m\n",
      "File \u001B[0;32m~/.pyenv/versions/fivessun/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.pyenv/versions/fivessun/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    avg_loss = 0\n",
    "    for idx, (x, y) in enumerate(dataloader):\n",
    "        h_0 = torch.zeros(1, n_hidden)\n",
    "        # shape of x : (line_length, 1, n_letters)\n",
    "        # shape of y : (1)\n",
    "        # shape of h : (1, n_hidden)\n",
    "        h = h_0\n",
    "        for i in range(x.shape[0]):\n",
    "            # Since the problem is many-to-one, we only use the last output.\n",
    "            o, h = rnn_model(x[i], h)\n",
    "        loss = loss_ftn(o, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs} | Step {idx + 1}/{len(dataloader)} | Loss: {loss.item():.4f}\")\n",
    "        avg_loss += loss\n",
    "\n",
    "    avg_loss /= len(dataloader)\n",
    "    losses.append(avg_loss)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T07:31:46.504635Z",
     "start_time": "2023-08-31T07:28:42.318737Z"
    }
   },
   "id": "aeafa232283da205"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "tensor([9]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([10]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([2]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([1]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([2]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([2]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([17]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([1]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([12]) tensor([0])\n",
      "tensor([4]) tensor([0])\n",
      "tensor([10]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([2]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([1]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([3]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([12]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([7]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([2]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([2]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([3]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([3]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([1]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([1]) tensor([0])\n",
      "tensor([15]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([3]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([10]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([17]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([2]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([3]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([3]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([1]) tensor([0])\n",
      "tensor([1]) tensor([0])\n",
      "tensor([1]) tensor([0])\n",
      "tensor([15]) tensor([0])\n",
      "tensor([1]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([1]) tensor([0])\n",
      "tensor([15]) tensor([0])\n",
      "tensor([1]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([12]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([3]) tensor([0])\n",
      "tensor([3]) tensor([0])\n",
      "tensor([2]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([3]) tensor([0])\n",
      "tensor([3]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([3]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([1]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([1]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([10]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([15]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([1]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([3]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([3]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([2]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([12]) tensor([0])\n",
      "tensor([15]) tensor([0])\n",
      "tensor([3]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([8]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([2]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([10]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([3]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([2]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([17]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([17]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([1]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([12]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([12]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([2]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([15]) tensor([0])\n",
      "tensor([3]) tensor([0])\n",
      "tensor([3]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([3]) tensor([0])\n",
      "tensor([3]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([2]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([12]) tensor([0])\n",
      "tensor([7]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([15]) tensor([0])\n",
      "tensor([1]) tensor([0])\n",
      "tensor([15]) tensor([0])\n",
      "tensor([15]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([15]) tensor([0])\n",
      "tensor([1]) tensor([0])\n",
      "tensor([1]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([1]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([3]) tensor([0])\n",
      "tensor([3]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([17]) tensor([0])\n",
      "tensor([4]) tensor([0])\n",
      "tensor([3]) tensor([0])\n",
      "tensor([3]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([1]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([12]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([15]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([15]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([17]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([10]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([3]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([17]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([12]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([3]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([10]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([2]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([3]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([1]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([1]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([12]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([1]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([9]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([4]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([6]) tensor([0])\n",
      "tensor([0]) tensor([0])\n",
      "tensor([6]) tensor([0])\n"
     ]
    }
   ],
   "source": [
    "for category, lines in category_lines.items():\n",
    "    for line in lines:\n",
    "        category_index = convert_category_to_index(category)\n",
    "        x = convert_line_to_tensor(line)\n",
    "        h_0 = torch.zeros(1, n_hidden)\n",
    "        h = h_0\n",
    "        for i in range(x.shape[0]):\n",
    "            o, h = rnn_model(x[i], h)\n",
    "        \n",
    "        print(torch.argmax(o, dim=1), category_index)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T07:32:09.224412Z",
     "start_time": "2023-08-31T07:32:09.100651Z"
    }
   },
   "id": "907c772ddfdf1c6f"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x4b33f3710>]"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot([loss.detach().numpy() for loss in losses])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T07:35:04.406263Z",
     "start_time": "2023-08-31T07:35:04.317481Z"
    }
   },
   "id": "c1105d4b2331273f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
